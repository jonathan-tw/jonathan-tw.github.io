[{"content":"HW1 - Vector Space Model TFã€IDFè¨ˆç®—æ–¹å¼: ä¸€é–‹å§‹æœƒè®€å–documentçš„è³‡æ–™å‰è™•ç†ï¼Œå…ˆå»åšä¸€äº›normalizedï¼Œè®“lexiconä¸è¦æœ‰é‡è¤‡çš„termï¼Œè€Œç‚ºäº†è®“ç¨‹å¼èƒ½åœ¨1åˆ†é˜å…§è·‘å®Œï¼Œlexiconä½¿ç”¨c++çš„unordered_mapä¸‹å»å­˜ï¼Œunordered_mapä½¿ç”¨hash functionæ‰¾åˆ°key termï¼Œç”¨ä¾†å°‹æ‰¾åŠä¿®æ”¹objectæœ‰è‘—æ¥µé«˜çš„æ•ˆç‡ã€‚æœ‰äº†normalizedå¾Œçš„è³‡æ–™ï¼Œå°±å¯ä»¥è¨ˆç®—æ¯å€‹termåœ¨æ¯å€‹æª”æ¡ˆä¸­çš„TF (local)ï¼Œæ¥è‘—ç®—å‡ºæ¯å€‹termçš„IDF (global)ã€‚TF: è©²index Termåœ¨è©²Docä¸‹å‡ºç¾çš„æ¬¡æ•¸ã€‚IDF: è©²index Term åœ¨æ‰€æœ‰Docä¸‹å‡ºç¾çš„æ–‡ç« æ¬¡æ•¸ã€‚\nDocument Term Weightã€Query Term Weightè¨ˆç®—æ–¹å¼: æœ‰äº†TFã€IDFä¹‹å¾Œï¼Œå°±èƒ½ç®—å‡ºDocument Term WeightåŠQuery Term Weightã€‚é€™è£¡ä½¿ç”¨çš„æ–¹æ³•æ˜¯è€å¸«è¬›ç¾©ä¸­çš„Scheme3ï¼Œæœƒæœ‰è¼ƒä½³çš„scoreã€‚\nå·¦ç‚º:doc term å³ç‚º:query term\nVector Space Modelé‹ä½œåŸç†åŠåƒæ•¸èª¿æ•´: åˆ©ç”¨å…©ç›¸åŒé•·åº¦çš„å‘é‡(doc weight vectorã€query weight vector)æ±‚å‡ºcosine-similarity(å…©å‘é‡ä¹‹å¤¾è§’)ï¼Œæ­¤cosine-similarityå€¼å°±ä»£è¡¨rankingï¼ŒRankingå€¼çš„ç¯„åœ0 â‰¤ ğ‘ ğ‘–ğ‘š ğ‘, ğ‘‘ğ‘— â‰¤ 1ã€‚Rankingå€¼é«˜ä»£è¡¨æ­¤docèˆ‡queryçš„é—œè¯æ€§æ›´é«˜ï¼Œæ’åæœƒæ¯”å…¶ä»–rankingä½çš„docå‰é¢ã€‚è€Œé—œæ–¼Vector Space Modelåƒæ•¸çš„èª¿æ•´ï¼Œæˆ‘åœ¨è€å¸«çš„scheme3ä¸Šé€²è¡Œèª¿æ•´ï¼ŒæŠŠquery weight constantå€¼æ”¹1.5ã€doc weight constantæ”¹2.5ï¼Œæœ‰è‘—è¼ƒä½³çš„çµæœã€‚\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[4190] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 4191.0 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name ); all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); } // while } // else _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] + 2.5 ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); print_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc( Document doc ) { vector\u0026lt;double\u0026gt; temp_query_weights; vector\u0026lt;double\u0026gt; temp_doc_weights = doc.all_term_weight_in_file; int i; for ( i = 0; i \u0026lt; doc.all_term_names_in_file.size(); i++ ) { temp_query_weights.push_back( 0.0 ); } // for for ( i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto iterator = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( iterator == doc.all_term_names_in_file.end() ) { temp_query_weights.push_back( query_file.term_weights[i] ); temp_doc_weights.push_back( 0.0 ); } // if else { int pos = distance( doc.all_term_names_in_file.begin(), iterator ); double relpace_weight = query_file.term_weights[i]; temp_query_weights[pos] = relpace_weight; } // else } // for query_weights = temp_query_weights; doc_weights = temp_doc_weights; return cosine_similarity(); } // rank_doc() double cosine_similarity() { // cout \u0026lt;\u0026lt; query_weights.size() \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; doc_weights.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; double dot = 0.0, denom_query = 0.0, denom_doc = 0.0; for ( int i = 0; i \u0026lt; doc_weights.size(); i++ ) { dot = dot + query_weights[i] * doc_weights[i]; denom_query = denom_query + query_weights[i] * query_weights[i]; denom_doc = denom_doc + doc_weights[i] * doc_weights[i]; } // for() // cout \u0026lt;\u0026lt; dot \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; denom_query \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; denom_doc \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return dot / ( sqrt( denom_query ) * sqrt( denom_doc ) ); } // cosine_similarity() int partition ( int low, int high ) { double pivot = all_docs[high].ranking; // pivot int i = ( low - 1 ); for (int j = low; j \u0026lt; high; j++) { if ( all_docs[j].ranking \u0026gt; pivot ) { i++; // increment index of smaller element Document doc; doc = all_docs[j]; all_docs[j] = all_docs[i]; all_docs[i] = doc; } // if } // for i++; Document doc; doc = all_docs[high]; all_docs[high] = all_docs[i]; all_docs[i] = doc; return i; } // partition() void descending_order_document_ranking( int low, int high) { double pivot = partition( low, high ); descending_order_document_ranking( low, pivot -1 ); descending_order_document_ranking( pivot + 1, high ); } // descending_order_document_ranking() void print_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // print_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW2 - Best Match Model Best Match 25L ä¸€é–‹å§‹ä½¿ç”¨BM25Lé€²è¡Œå¯¦ä½œï¼Œç¶“éå¤šç•ªå˜—è©¦åƒæ•¸ï¼Œç™¼ç¾å…¶å€¼æœ€å¤šåªèƒ½åˆ°0.70å¤šï¼Œå¾Œæ”¹ç”¨BM25èƒ½ç²å¾—è¼ƒé«˜çš„æ•¸å€¼ã€‚è€ŒBM25Læ”¹å–„äº†å…¶long documentsçš„å•é¡Œï¼Œé–“æ¥é¿å…äº†overly-penalizingï¼Œshift parameter ğ›¿ = 0.5èƒ½ç²å¾—è¼ƒä½³çš„çµæœã€‚\nBest Match 25åŸç†åŠæ‡‰ç”¨ Best Match 25ä¸€æ¨£ä½¿ç”¨äº†TF åŠ IDFï¼Œèˆ‡the vector modelä¸åŒçš„æ˜¯BM25å¤šåŠ äº†ä¸€é …document length normalizationçš„principleï¼Œä¸”BM25ç‚ºBM15åŠBM11çš„çµ„åˆï¼Œç•¶ b = 0æ™‚ï¼Œç‚ºBM15ï¼Œb = 1æ™‚ï¼Œç‚ºBM11ï¼Œè—‰ç”±èª¿æ§båƒæ•¸ä¾†ç²å¾—è¼ƒä½³çš„çµæœã€‚\nåšå®Œä¹Ÿç™¼ç¾BM25æœ‰è‘—æ¯”the vector modelæ›´å¥½çš„è¡¨ç¾ï¼Œéœ€è¦çš„é‹ç®—æ™‚é–“è¼ƒå°‘ä¸”ç²¾æº–åº¦æ›´é«˜(ç„¡é ˆåšcosine-similarityæ˜¯ä¸€å¤§å„ªå‹¢)ï¼Œä½†å¯èª¿ç¯€çš„åƒæ•¸éœ€è¦è‡ªè¡Œè¨­å®šï¼Œä¸”doument lengthå¤ªé•·æ™‚æœƒæœ‰ç²¾æº–åº¦è¼ƒå·®çš„è¡¨ç¾ã€‚\nBest Match 25åƒæ•¸ åœ¨ç¨‹å¼ä¸­ç¬¬137~ 139è¡Œä¸­(RankEveryQueryFile)åˆ†åˆ¥è¨­å®šäº†k1ã€k3åŠbå€¼ï¼Œç²å¾—æœ€ä½³åƒæ•¸ç‚ºä»¥ä¸‹å€¼ï¼Œå…¶ä¸­k3æ˜¯éš¨æ„å€¼ï¼Œå› query fileä¸­çš„termï¼Œtf(I,q)çš† = 1\nK1 = 1.635 b = 0.85 k3 = 1000.0\nè—è‰²é …: Document Length Normalization ç¶ è‰²é …: Inverse Document Frequency\nä¸­é–“é …: Inverse Query Frequencyå¯çœç•¥(åœ¨HW2ä¸­)\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[4190] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 4191.0 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; float avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen += doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] + 2.5 ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.635; b = 0.85; k3 = 1000.0; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = ( ( ( k1 + 1.0 ) * doc.all_term_tf_in_file[pos] ) / ( k1 * ( ( 1 - b) + b * ( doc.doc_length / avg_doclen ) ) + doc.all_term_tf_in_file[pos] ) ); doc_temp = doc_temp * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_temp = doc_temp * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW3 - Mean Average Precision (MAP) Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[14954] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 14955.000 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; double avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen = avg_doclen + doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.4; b = 0.5; q = 0.4; k3 = 1000; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3, q; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = doc.all_term_tf_in_file[pos] / ( ( 1 - b ) + b * ( doc.doc_length / avg_doclen ) ); if ( doc_temp \u0026gt; 0 ) { doc_temp = ( ( k1 + 1.0 ) * ( doc_temp + q ) ) / ( k1 + doc_temp + q ); } // if else { doc_temp = 0; } // else doc_score = doc_score + doc_temp; doc_score = doc_score * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_score = doc_score * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW4 - PLSA Model PLSAåƒæ•¸èª¿æ³• Topic_num: 8 / a (ç¬¬ä¸€é …local TFæ©Ÿç‡term = 0.6) all_docä½¿ç”¨vectorå­˜local tf\nb (ç¬¬äºŒé …gobal TFæ©Ÿç‡term = 0.25) / Inter_run: 30 dictionaryä½¿ç”¨mapå­˜wordèˆ‡positionã€global TFå­˜\nPLSAç†è«–å¿ƒå¾— EMå¯æ±‚å¾—éš±å«è®Šé‡T(k)ï¼ŒåŠ å…¥äº†Pd_z[z][m]ã€Pw_z[z][w]è®“å…¶éš¨æ©Ÿåˆå§‹å€¼å’Œç‚º1ã€‚ é€éæ¯å€‹TOPICä¸‹å°æ–¼Pd_z[z][m] (æ¯ä¸€ä»½DOC(m)ä¸‹æ˜¯TOPIC(z)ä¸»é¡Œä¹‹æ©Ÿç‡)èˆ‡ Pw_z[z][w] (æ¯ä¸€ä»½TOPIC(z)ä¸‹term(w)ä¹‹æ©Ÿç‡)ï¼Œåˆ©ç”¨å…©è€…ç¨ç«‹æ©Ÿç‡åè¦†å°Pz_dw[z][m][position]é€²è¡Œæ›´æ–°ã€‚\nPLSAå¯¦ä½œå¿ƒå¾— ç¨‹å¼æµç¨‹ç‚ºscanning documetnsâ¡ Init Parameter ( two for m-stepã€one for e-step) â¡ Run EM for 30 runs â¡ calculate for every query file and log prob\nIn step 1: å­˜å–æ‰€æœ‰docä¸¦è¨˜éŒ„localã€global TFåˆ°å°æ‡‰å®¹å™¨ä¸­\nIn step 2: C++éœ€è¦newå‡ºmemory space for these parameters.\nIn step 3: æ¯runå®Œä¸€æ¬¡EMå®Œï¼Œå°±æœƒå¾—åˆ°æ›´æ–°çš„Pd_z[z][m]ã€Pw_z[z][w] for m-stepï¼Œåˆ©ç”¨é€™å…©å€‹åˆç¹¼çºŒæ›´æ–°Pz_dw[z][m][position] for e-stepï¼Œè¨“ç·´åˆ°æ¥è¿‘æ”¶æ–‚\nIn step 4: èˆ‡query fileä¸­termé€²è¡Œè¨ˆç®—ï¼Œå¾—å‡ºæ¯å€‹docå°æ‡‰queryä¹‹æ©Ÿç‡ä¸¦å–logä»¥å¤§æ’åˆ°å°è¼¸å‡ºã€‚\né¸ç”¨æœ€å°Topic(8)çš„æƒ…æ³åº•ä¸‹è·‘äº†30 runï¼Œéœ€è¦éå¸¸ä¹…çš„æ™‚é–“ï¼Œé›–ç„¶åœ¨mapä¸Šå·²é¸ç”¨unordered_mapæœ€å¿«çš„å®¹å™¨ä¸‹å»å¯¦ä½œï¼Œä½†æ™‚é–“é‚„æ˜¯èŠ±å¾—æ¯”æƒ³åƒä¸­å¤šï¼Œåœ¨çŒœæƒ³çš„åŸå› æ˜¯å› ç‚ºæŸäº›å¯ä»¥ä½¿ç”¨mapä¸‹å»åšæ˜ å°„term indexçš„éƒ¨åˆ†ï¼Œæ¯”èµ·vectorä¸­çš„findé€Ÿåº¦æœƒå¿«ä¸Šä¸å°‘ï¼Œé€™æ¬¡çš„modelæ¯”èµ·ä»¥å¾€éœ€è¦æ›´å¤šçš„æ™‚é–“ä¸‹å»å®Œæˆè©¦é©—ï¼Œä¸‹æ¬¡çš„ä½œæ¥­æ‡‰ææ—©é–‹å§‹æº–å‚™ï¼Œé€™æ¨£æ‰æœ‰æ›´å¤šæ™‚é–“é€²è¡Œæ¸¬è©¦ã€‚\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; # include \u0026lt;cstdlib\u0026gt; # include \u0026lt;stdlib.h\u0026gt; # include \u0026lt;ctime\u0026gt; using namespace std; # define MAX_NUM_DOC 14955.0 # define MAX_NUM_QUERY 100.0 # define MAX_NUM_TOPIC 8.0 # define MAX_NUM_ITER 30.0 # define MAX_LENGTH 7059938.0 # define MAX_NUM_WORDS 111449.0 struct TermProperty { string term_name; double tf; double prob; // probability ( first or third term ) }; // TermProperty struct Document { int docID; string doc_name; double ranking; double doc_length; vector\u0026lt;TermProperty\u0026gt; term_properties; }; // Documentc struct Word2doc { int docID; int position; }; // Word2doc struct Dictionary { TermProperty term_property; vector\u0026lt;Word2doc\u0026gt; word2doc; int word2dic; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_property.term_name == p.term_property.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_property.term_name; return os; } }; // Dictionary struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class PLSA { public: unordered_map\u0026lt;string, Dictionary\u0026gt; dictionary; vector\u0026lt;Document\u0026gt; all_docs; vector\u0026lt;string\u0026gt; all_words; int num_file; int all_docs_len; void ReadAllDocuments() { all_docs_len = 0; string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; double doc_length = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.term_properties = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; document.docID = num_file; all_docs_len += doc_length; all_docs.push_back( document ); num_file++; doc_length = 0.0; k = _findnext( handle, \u0026amp;file_info); } // while } // else _findclose( handle ); } // ReadAllDocuments() void Init() { num_data = MAX_NUM_DOC; num_words = MAX_NUM_WORDS; num_topics = MAX_NUM_TOPIC; srand(time(NULL)); //è¨­ç½®éšæ©Ÿæ•¸ç¨®å­ï¼Œä½¿æ¯æ¬¡ç²å–çš„éšæ©Ÿåºåˆ—ä¸åŒã€‚ Pd_z = new double*[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; Pd_z[z] = new double[num_data-1]; for( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] = (rand()%100000)*0.00001; norm_down += Pd_z[z][m]; } // for for( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] /= norm_down; } // for } // for cout \u0026lt;\u0026lt; \u0026#34;Init step1 done\\n\u0026#34;; Pw_z = new double*[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; Pw_z[z] = new double[num_words-1]; for( int w = 0; w \u0026lt; num_words; w++ ) { Pw_z[z][w] = (rand()%100000)*0.00001; norm_down += Pw_z[z][w]; } // for for( int w = 0; w \u0026lt; num_words; w++ ) { Pw_z[z][w] /= norm_down; } // for } // for cout \u0026lt;\u0026lt; \u0026#34;Init step2 done\\n\u0026#34;; Pz_dw = new double **[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z] = new double *[num_data-1]; for ( int m = 0; m \u0026lt; num_data; m++ ) { int size = ( int ) all_docs[m].term_properties.size(); Pz_dw[z][m] = new double[size-1]; } // for() } // for() cout \u0026lt;\u0026lt; \u0026#34;Init step3 done\\n\u0026#34;; } // Init() void DoPLSA() { // start training for ( int i = 0; i \u0026lt; MAX_NUM_ITER; i++ ) { Estep(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Estep: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; Mstep_1(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Mstep_1: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; Mstep_2(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Mstep_2: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; system( \u0026#34;pause\u0026#34; ); } // for() } // DoPLSA() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { a = 0.6; b = 0.25; output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_prob( file_name ); sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file, file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQueryFile() private: int num_data; // number of data int num_words; // number of words int num_topics; // number of topics double a, b; double **Pd_z = NULL; double **Pw_z = NULL; double ***Pz_dw = NULL; double **Dw_m = NULL; vector\u0026lt;TermProperty\u0026gt; read_a_doc_to_dictionary( string file_name, double \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;TermProperty\u0026gt; term_properties; TermProperty term_property; Word2doc word2doc; Dictionary doc_term; unordered_map\u0026lt;string, int\u0026gt; doc2dic; file.open( path_name, ios::in ); int position = 0; while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_property.term_name = term_name; auto it_dic = dictionary.find( term_name ); if ( it_dic != dictionary.end() ) { doc_term = it_dic -\u0026gt; second; doc_term.term_property.tf++; doc_term.term_property.prob = doc_term.term_property.tf / MAX_LENGTH; if ( doc_term.word2doc[doc_term.word2doc.size()-1].docID != num_file ) { word2doc.docID = num_file; word2doc.position = position; doc_term.word2doc.push_back( word2doc ); term_property.tf = 1; term_properties.push_back(term_property); doc2dic.insert(make_pair(term_name, position)); position++; } // if else { auto it = doc2dic.find( term_name ); int pos = it -\u0026gt; second; term_properties[pos].tf++; } // else it_dic -\u0026gt; second = doc_term; } // if else { term_property.tf = 1; term_property.prob = term_property.tf / MAX_LENGTH; word2doc.docID = num_file; word2doc.position = position; doc_term.term_property = term_property; doc_term.word2doc.push_back( word2doc ); doc_term.word2dic = all_words.size(); all_words.push_back( term_name ); dictionary.insert( make_pair( term_name, doc_term )); position++; term_properties.push_back(term_property); doc2dic.insert(make_pair(term_name, position)); } // else } // while for (int i = 0; i \u0026lt; term_properties.size(); i++ ) { term_properties[i].prob = term_properties[i].tf / doc_length; } // for file.close(); return term_properties; } // read_a_doc_to_dictionary() void Estep() { cout \u0026lt;\u0026lt; all_docs.size() \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; num_data \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; for ( int m = 0; m \u0026lt; num_data; m++ ) { for ( int position = 0; position \u0026lt; all_docs[m].term_properties.size(); position++ ) { double norm_down = 0.0; auto it_dic = dictionary.find( all_docs[m].term_properties[position].term_name ); if ( it_dic == dictionary.end() ) cout \u0026lt;\u0026lt; \u0026#34;cry\\n\u0026#34;; int w = it_dic -\u0026gt; second.word2dic; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double val = Pd_z[z][m] * Pw_z[z][w]; Pz_dw[z][m][position] = val; norm_down += val; } // for() if ( norm_down != 0.0 ) { for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z][m][position] /= norm_down; } // for } // if else { for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z][m][position] = 0.0; } // for } // else } // for() if ( m \u0026gt;= 14950 ) { cout \u0026lt;\u0026lt; all_docs[m].doc_name \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; all_docs[m].term_properties.size() \u0026lt;\u0026lt; \u0026#34; qq1\\n\u0026#34;; } // if } // for } // Estep() void Mstep_1() { // Pd_z[z][m] for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; for ( int m = 0; m \u0026lt; num_data; m++ ) { double sum = 0.0; for ( int position = 0; position \u0026lt; all_docs[m].term_properties.size(); position++ ) { auto it = dictionary.find( all_docs[m].term_properties[position].term_name ); sum += it -\u0026gt; second.term_property.tf * Pz_dw[z][m][position]; } // for Pd_z[z][m] = sum; norm_down += sum; } // for() if ( norm_down != 0 ) { for ( int m = 0; m \u0026lt; num_data; m++) { Pd_z[z][m] /= norm_down; } // for } // if else { for ( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] = 0; } // for } // else } // for() } // Mstep_1() void Mstep_2() { // Pw_z[z][w] for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; for ( int w = 0; w \u0026lt; num_words; w++ ) { double sum = 0.0; string word = all_words[w]; auto it = dictionary.find( word ); vector\u0026lt;Word2doc\u0026gt; word2docs = it -\u0026gt; second.word2doc; for ( int i = 0; i \u0026lt; word2docs.size(); i++ ) { int m = word2docs[i].docID; int position = word2docs[i].position; auto it_2 = dictionary.find( all_docs[m].term_properties[position].term_name ); sum += it_2 -\u0026gt; second.term_property.tf * Pz_dw[z][m][position]; } // for Pw_z[z][w] = sum; norm_down += sum; } // for if ( norm_down != 0 ) { for ( int w = 0; w \u0026lt; num_words; w++) { Pw_z[z][w] /= norm_down; } // for } // if else { for ( int w = 0; w \u0026lt; num_words; w++) { Pw_z[z][w] = 0; } // for } // else } // for } // Mstep_2() void calculate_query_prob( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; file.open( path_name, ios::in ); vector\u0026lt;string\u0026gt; all_query_term; while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; all_query_term.push_back( term_name ); } // while for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { double prob = 0.0; for ( int j = 0; j \u0026lt; all_query_term.size(); j++ ) { prob = prob * calculate_term_prob( all_docs[i], all_query_term[j] ); if ( prob == 0.0 ) break; } // for if ( prob != 0.0 ) prob = log10( prob ); all_docs[i].ranking = prob; } // for } // calculate_query_prob() double calculate_term_prob( Document doc, string query_term ) { double prob = 0.0, temp_prob = 0.0; auto it_dic = dictionary.find( query_term ); if ( it_dic == dictionary.end() ) return prob; int m = doc.docID; int w = it_dic -\u0026gt; second.word2dic; vector\u0026lt;TermProperty\u0026gt; term_properties = doc.term_properties; TermProperty term_property; // first term auto it_doc = find_if( term_properties.begin(), term_properties.end(), [query_term] (const TermProperty\u0026amp; s) { return s.term_name == query_term; } ); if ( it_doc != term_properties.end() ) { term_property = *it_doc; prob += term_property.prob * a; } // if // second term and third term for ( int z = 0; z \u0026lt; num_topics; z++ ) { temp_prob += Pd_z[z][m] * Pw_z[z][w] ; } // for prob += temp_prob * b; prob += ( 1.0 - a - b ) * it_dic -\u0026gt; second.term_property.prob; return prob; } // calculate_term_prob() void output_ranking( ofstream \u0026amp; output_file, string query_file ) { query_file.erase( query_file.end() -4, query_file.end() ); output_file \u0026lt;\u0026lt; query_file \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() }; // PLSA int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; PLSA plsa; plsa.ReadAllDocuments(); system(\u0026#34;pause\u0026#34;); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Initing.\\n\u0026#34;; plsa.Init(); cout \u0026lt;\u0026lt; \u0026#34;End Initing.\\n\u0026#34;; plsa.DoPLSA(); cout \u0026lt;\u0026lt; \u0026#34;Done Training.\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); plsa.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW5 - Query Modeling (Rocchio Algorithm) Query Modeling (Rocchio Algorithm)åƒæ•¸èª¿æ³• First Information Retrieval: BM25 k1= 0.8 b = 0.7 k3 = 100.0\nSecond Information Retrieval: VSM\nRocchio algorithm: a = 2.5 b = 0.4 relevant doc = 10 non-relevant doc = 1 (c = 0.1)\nå¦‚æœFirst Information Retrievalä½¿ç”¨PLSAèƒ½ç²å¾—è¼ƒé«˜çš„åˆ†æ•¸ï¼Œä½†éœ€è¦çš„memoryæ¥µå¤§ã€‚\nQuery Modeling (Rocchio Algorithm)ç†è«–å¿ƒå¾— ç”±æ–¼queryæ‰€è’é›†çš„è³‡æ–™é‡å¤ªå°‘ï¼Œå°è‡´è³‡æ–™æª¢ç´¢å‰æ‰€ç²å¾—çš„è³‡è¨Šå¤ªå°‘ï¼Œè€Œé€éä½¿ç”¨Pseudo-Relevance Feedbackçš„éç¨‹å¾—åˆ°åŠ å¼·å¾Œçš„query termï¼Œå†é€²è¡Œç¬¬äºŒæ¬¡è³‡æ–™æª¢ç´¢ï¼Œèƒ½ç²å¾—è¼ƒå¥½çš„çµæœã€‚Query Modelingåˆ©ç”¨æ­¤æ ¸å¿ƒç²¾ç¥ä¸‹å»å¯¦ä½œï¼Œè€Œæœ¬æ¬¡å¯¦ä½œPseudo-Relevance Feedbackçš„æ–¹æ³•ç‚ºRocchio Algorithmã€‚\nQuery Modeling (Rocchio Algorithm)å¯¦ä½œå¿ƒå¾— ç¨‹å¼æµç¨‹ç‚ºscanning documents â¡ calculate every word TF-IDF â¡Run BM25/PLSA for First IR â¡ using Rocchio for updating new query weight â¡ Run VSM for Second IR\nIn step 1ã€2: å­˜å–æ‰€æœ‰docä¸¦è¨˜éŒ„localã€global TFåˆ°å°æ‡‰å®¹å™¨ä¸­\nIn step 3: è¨ˆç®—BM25ä¸¦æ’åºå‰10ç¯‡æ–‡ç« ç‚ºrelevant docs\nIn step 4: update query vector on relevant docs , normalize new query weight and add original query weight ( a = 2.5, b =0.4 ) if using non-relevant doc ( c =0.1)\nIn step 5: normalize every doc weight and do VSM separately.\nåœ¨å¯¦ä½œéç¨‹ä¸­ç™¼ç¾Relevant Docç¯‡æ•¸çš„è¨­å®šä¹Ÿæœƒæœ‰å¾ˆå¤§çš„å½±éŸ¿, ä¸ä¸€å®šå¤šæ¯”è¼ƒå¥½, å¤ªå°‘ä¹Ÿå¯èƒ½ä¸è¡Œï¼Œ10~15ç¯‡æ˜¯ç²å¾—æ­¤è³‡æ–™é›†è¼ƒç†æƒ³çš„Docæ•¸ã€‚è€Œåƒæ•¸bä¸èƒ½è¨­å¾—å¤ªå¤§å¦å‰‡æœƒè®“new query weightè·Ÿoriginal query weightçš„æ¬Šé‡è®Šå¾—æ¨¡ç³Šï¼Œæ•ˆæœä¸å¥½ï¼Œæœ€å¾Œåœ¨çµ±ä¸€vectoré•·åº¦æ™‚ï¼Œç•¶åˆ†å­å…¶ä¸­ä¸€é …ç‚º0æ™‚å¯ä»¥ä¸ç®—ä¾†å¢åŠ é‹è¡Œé€Ÿåº¦ã€‚\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[14954] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 14955.000 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; double avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen = avg_doclen + doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.4; b = 0.5; q = 0.4; k3 = 1000; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3, q; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = doc.all_term_tf_in_file[pos] / ( ( 1 - b ) + b * ( doc.doc_length / avg_doclen ) ); if ( doc_temp \u0026gt; 0 ) { doc_temp = ( ( k1 + 1.0 ) * ( doc_temp + q ) ) / ( k1 + doc_temp + q ); } // if else { doc_temp = 0; } // else doc_score = doc_score + doc_temp; doc_score = doc_score * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_score = doc_score * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW6 - BERT Model BERT åƒæ•¸èª¿æ³• BERT ç†è«–å¿ƒå¾— BERT(é›™å‘Transformerç·¨ç¢¼è¡¨é”)\nä¸»è¦æ˜¯ä»¥å…©ç¨®é è¨“ç·´çš„æ–¹å¼ä¾†å»ºç«‹èªè¨€æ¨¡å‹ã€‚ è®“modelåˆ†è¾¨å¥â¼¦ä¹‹é–“ä¸Šä¸‹â½‚ä¹‹é—œä¿‚ ç°¡è€ŒçŸ¥BERTé è¨“ç·´æ¨¡å‹éœ€è¦åšå…©å€‹ä¸»è¦ä»»å‹™:\nç¬¬â¼€Task: åˆ¤æ–·å…©å¥è©±æ˜¯å¦çœŸçš„ç›¸é„°(Binary classification)\nç¬¬â¼†Task: é æ¸¬è¢«maskä¹‹å–®è©(Multi-class classification)\n(ä¸‰) BERTå¯¦ä½œå¿ƒå¾—\nä½¿ç”¨åŠ©æ•™æä¾›çš„baseline codeä¸‹å»å¯¦ç¾ï¼Œå…¶ä¸»è¦å‘¼å«çš„åœ°æ–¹å¯ä»¥æŸ¥å®˜æ–¹APIå¯¦ä½œå‡ºä¾†ï¼Œåˆ©ç”¨æ­¤modelå›å‚³lossåŠlogitså€¼ã€‚\nè€ŒTO-DOåœ°æ–¹ä¸­éœ€è¦å›å‚³BM25çš„2D array scoresï¼Œå¯ä»¥åˆ©ç”¨åŠ©æ•™çš„æ–¹æ³•å¯¦ç¾\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 #!/usr/bin/env python # coding: utf-8 # In[ ]: from time import time from datetime import timedelta from copy import deepcopy import random import numpy as np import pandas as pd from ml_metrics import mapk import torch from torch.optim import AdamW from torch.nn.utils.rnn import pad_sequence from torch.utils.data import Dataset, DataLoader from transformers import BertTokenizerFast, BertForMultipleChoice # Random seed SEED = 42 random.seed(SEED) np.random.seed(SEED) torch.manual_seed(SEED) # CUDA device use_cuda_device = 0 torch.cuda.set_device(use_cuda_device) print(\u0026#34;Using CUDA device: %d\u0026#34; % torch.cuda.current_device()) # ## Settings # In[ ]: # Input files document_csv_path = \u0026#39;../input/ntust-ir2020-homework6/documents.csv\u0026#39; training_csv_path = \u0026#39;../input/ntust-ir2020-homework6/train_queries.csv\u0026#39; testing_csv_path = \u0026#39;../input/ntust-ir2020-homework6/test_queries.csv\u0026#39; # Input limitation max_query_length = 64 max_input_length = 512 num_negatives = 3 # num. of negative documents to pair with a positive document # Model finetuning model_name_or_path = \u0026#34;bert-base-uncased\u0026#34; max_epochs = 1 learning_rate = 3e-5 dev_set_ratio = 0.2 # make a ratio of training set as development set for rescoring weight sniffing max_patience = 0 # earlystop if avg. loss on development set doesn\u0026#39;t decrease for num. of epochs batch_size = 2 # num. of inputs = 8 requires ~9200 MB VRAM (num. of inputs = batch_size * (num_negatives + 1)) num_workers = 2 # num. of jobs for pytorch dataloader # Save paths save_model_path = \u0026#34;models/bert_base_uncased\u0026#34; # assign `None` for not saving the model save_submission_path = \u0026#34;bm25_bert_rescoring.csv\u0026#34; K = 1000 # for MAP@K # ## Preparing # In[ ]: # Build and save BERT tokenizer tokenizer = BertTokenizerFast.from_pretrained(model_name_or_path) if save_model_path is not None: save_tokenizer_path = \u0026#34;%s/tokenizer\u0026#34; % (save_model_path) tokenizer.save_pretrained(save_tokenizer_path) # Collect mapping of all document id and text doc_id_to_text = {} doc_df = pd.read_csv(document_csv_path) doc_df.fillna(\u0026#34;\u0026lt;Empty Document\u0026gt;\u0026#34;, inplace=True) id_text_pair = zip(doc_df[\u0026#34;doc_id\u0026#34;], doc_df[\u0026#34;doc_text\u0026#34;]) for i, pair in enumerate(id_text_pair, start=1): doc_id, doc_text = pair doc_id_to_text[doc_id] = doc_text print(\u0026#34;Progress: %d/%d\\r\u0026#34; % (i, len(doc_df)), end=\u0026#39;\u0026#39;) doc_df.tail() # # Training # ## Split a ratio of training set as development set # In[ ]: train_df = pd.read_csv(training_csv_path) dev_df, train_df = np.split(train_df, [int(dev_set_ratio*len(train_df))]) dev_df.reset_index(drop=True, inplace=True) train_df.reset_index(drop=True, inplace=True) print(\u0026#34;train_df shape:\u0026#34;, train_df.shape) print(\u0026#34;dev_df shape:\u0026#34;, dev_df.shape) train_df.tail() # ## Build instances for training/development set # In[ ]: get_ipython().run_cell_magic(\u0026#39;time\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;doc_id_to_token_ids = {}\\n\\n\\ndef preprocess_df(df):\\n \\\u0026#39;\\\u0026#39;\\\u0026#39; Preprocess DataFrame into training instances for BERT. \\\u0026#39;\\\u0026#39;\\\u0026#39;\\n instances = []\\n \\n # Parse CSV\\n for i, row in df.iterrows():\\n query_id, query_text, pos_doc_ids, bm25_top1000, _ = row\\n pos_doc_id_list = pos_doc_ids.split()\\n pos_doc_id_set = set(pos_doc_id_list)\\n bm25_top1000_list = bm25_top1000.split()\\n bm25_top1000_set = set(bm25_top1000_list)\\n\\n # Pair BM25 neg. with pos. samples\\n labeled_pos_neg_list = []\\n for pos_doc_id in pos_doc_id_list:\\n neg_doc_id_set = bm25_top1000_set - pos_doc_id_set\\n neg_doc_ids = random.sample(neg_doc_id_set, num_negatives)\\n pos_position = random.randint(0, num_negatives)\\n pos_neg_doc_ids = neg_doc_ids\\n pos_neg_doc_ids.insert(pos_position, pos_doc_id)\\n labeled_sample = (pos_neg_doc_ids, pos_position)\\n labeled_pos_neg_list.append(labeled_sample)\\n \\n # Make query tokens for BERT\\n query_tokens = tokenizer.tokenize(query_text)\\n if len(query_tokens) \u0026gt; max_query_length: # truncation\\n query_tokens = query_tokens[:max_query_length]\\n query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\\n query_token_ids.insert(0, tokenizer.cls_token_id)\\n query_token_ids.append(tokenizer.sep_token_id)\\n\\n # Make input instances for all query/doc pairs\\n for doc_ids, label in labeled_pos_neg_list:\\n paired_input_ids = []\\n paired_attention_mask = []\\n paired_token_type_ids = []\\n \\n # Merge all pos/neg inputs as a single sample\\n for doc_id in doc_ids:\\n if doc_id in doc_id_to_token_ids:\\n doc_token_ids = doc_id_to_token_ids[doc_id]\\n else:\\n doc_text = doc_id_to_text[doc_id]\\n doc_tokens = tokenizer.tokenize(doc_text)\\n doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\\n doc_id_to_token_ids[doc_id] = doc_token_ids\\n doc_token_ids.append(tokenizer.sep_token_id)\\n\\n # make input sequences for BERT\\n input_ids = query_token_ids + doc_token_ids\\n token_type_ids = [0 for token_id in query_token_ids]\\n token_type_ids.extend(1 for token_id in doc_token_ids)\\n if len(input_ids) \u0026gt; max_input_length: # truncation\\n input_ids = input_ids[:max_input_length]\\n token_type_ids = token_type_ids[:max_input_length]\\n attention_mask = [1 for token_id in input_ids]\\n \\n # convert and collect inputs as tensors\\n input_ids = torch.LongTensor(input_ids)\\n attention_mask = torch.FloatTensor(attention_mask)\\n token_type_ids = torch.LongTensor(token_type_ids)\\n paired_input_ids.append(input_ids)\\n paired_attention_mask.append(attention_mask)\\n paired_token_type_ids.append(token_type_ids)\\n label = torch.LongTensor([label]).squeeze()\\n \\n # Pre-pad tensor pairs for efficiency\\n paired_input_ids = pad_sequence(paired_input_ids, batch_first=True)\\n paired_attention_mask = pad_sequence(paired_attention_mask, batch_first=True)\\n paired_token_type_ids = pad_sequence(paired_token_type_ids, batch_first=True)\\n\\n # collect all inputs as a dictionary\\n instance = {}\\n instance[\\\u0026#39;input_ids\\\u0026#39;] = paired_input_ids.T # transpose for code efficiency\\n instance[\\\u0026#39;attention_mask\\\u0026#39;] = paired_attention_mask.T\\n instance[\\\u0026#39;token_type_ids\\\u0026#39;] = paired_token_type_ids.T\\n instance[\\\u0026#39;label\\\u0026#39;] = label\\n instances.append(instance)\\n\\n print(\u0026#34;Progress: %d/%d\\\\r\u0026#34; % (i+1, len(df)), end=\\\u0026#39;\\\u0026#39;)\\n print()\\n return instances\\n\\ntrain_instances = preprocess_df(train_df)\\ndev_instances = preprocess_df(dev_df)\\n\\nprint(\u0026#34;num. train_instances: %d\u0026#34; % len(train_instances))\\nprint(\u0026#34;num. dev_instances: %d\u0026#34; % len(dev_instances))\\nprint(\u0026#34;input_ids.T shape:\u0026#34;, train_instances[0][\\\u0026#39;input_ids\\\u0026#39;].T.shape)\\ntrain_instances[0][\\\u0026#39;input_ids\\\u0026#39;].T\\n\u0026#39;) # ## Build dataset and dataloader for PyTorch # In[ ]: class TrainingDataset(Dataset): def __init__(self, instances): self.instances = instances def __len__(self): return len(self.instances) def __getitem__(self, i): instance = self.instances[i] input_ids = instance[\u0026#39;input_ids\u0026#39;] attention_mask = instance[\u0026#39;attention_mask\u0026#39;] token_type_ids = instance[\u0026#39;token_type_ids\u0026#39;] label = instance[\u0026#39;label\u0026#39;] return input_ids, attention_mask, token_type_ids, label def get_train_dataloader(instances, batch_size=2, num_workers=4): def collate_fn(batch): input_ids, attention_mask, token_type_ids, labels = zip(*batch) input_ids = pad_sequence(input_ids, batch_first=True).transpose(1,2).contiguous() # re-transpose attention_mask = pad_sequence(attention_mask, batch_first=True).transpose(1,2).contiguous() token_type_ids = pad_sequence(token_type_ids, batch_first=True).transpose(1,2).contiguous() labels = torch.stack(labels) return input_ids, attention_mask, token_type_ids, labels dataset = TrainingDataset(instances) dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, \\ batch_size=batch_size, num_workers=num_workers) return dataloader # Demo dataloader = get_train_dataloader(train_instances) for batch in dataloader: input_ids, attention_mask, token_type_ids, labels = batch break print(input_ids.shape) input_ids # ## Initialize and finetune BERT # In[ ]: model = BertForMultipleChoice.from_pretrained(model_name_or_path) model.cuda() optimizer = AdamW(model.parameters(), lr=learning_rate) optimizer.zero_grad() # ### (TO-DO!) Define validation function for earlystopping # In[ ]: def validate(model, instances): total_loss = 0 model.eval() dataloader = get_train_dataloader(instances, batch_size=batch_size, num_workers=num_workers) for batch in dataloader: batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids, labels = batch \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice) (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs) 2. Sum up the loss of all dev-set samples (Hint: The built-in loss is averaged, so you should multiply it with the batch size) \u0026#39;\u0026#39;\u0026#39; with torch.no_grad(): ### 1. insert_missing_code loss = model( input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels, return_dict=1 ).loss ### 2. insert_missing_code total_loss += loss * batch_size avg_loss = total_loss / len(instances) return avg_loss # ### (TO-DO!) Let\u0026#39;s train this beeg boy ;-) # In[ ]: patience, best_dev_loss = 0, 1e10 best_state_dict = model.state_dict() start_time = time() dataloader = get_train_dataloader(train_instances, batch_size=batch_size, num_workers=num_workers) for epoch in range(1, max_epochs+1): model.train() for i, batch in enumerate(dataloader, start=1): batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids, labels = batch # Backpropogation \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice) (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs) 2. Perform backpropogation on the loss (i.e. compute gradients) 3. Optimize the model. (Hint: These two lines of codes can be found in PyTorch tutorial) \u0026#39;\u0026#39;\u0026#39; ### 1. insert_missing_code loss = model( input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels, return_dict=1 ).loss ### 2. insert_missing_code loss.backward() ### 3. insert_missing_code optimizer.step() optimizer.zero_grad() # Progress bar with timer ;-) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Epoch: %d/%d | Batch: %d/%d | loss=%.5f | %s \\r\u0026#34; \\ % (epoch, max_epochs, i, len(dataloader), loss, elapsed_time), end=\u0026#39;\u0026#39;) # Save parameters of each epoch if save_model_path is not None: save_checkpoint_path = \u0026#34;%s/epoch_%d\u0026#34; % (save_model_path, epoch) model.save_pretrained(save_checkpoint_path) # Get avg. loss on development set print(\u0026#34;Epoch: %d/%d | Validating... \\r\u0026#34; % (epoch, max_epochs), end=\u0026#39;\u0026#39;) dev_loss = validate(model, dev_instances) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Epoch: %d/%d | dev_loss=%.5f | %s \u0026#34; \\ % (epoch, max_epochs, dev_loss, elapsed_time)) # Track best checkpoint and earlystop patience if dev_loss \u0026lt; best_dev_loss: patience = 0 best_dev_loss = dev_loss best_state_dict = deepcopy(model.state_dict()) if save_model_path is not None: model.save_pretrained(save_model_path) else: patience += 1 if patience \u0026gt; max_patience: print(\u0026#39;Earlystop at epoch %d\u0026#39; % epoch) break # Restore parameters with best loss on development set model.load_state_dict(best_state_dict) # # Testing # In[ ]: class TestingDataset(Dataset): def __init__(self, instances): self.instances = instances def __len__(self): return len(self.instances) def __getitem__(self, i): instance = self.instances[i] input_ids = instance[\u0026#39;input_ids\u0026#39;] attention_mask = instance[\u0026#39;attention_mask\u0026#39;] token_type_ids = instance[\u0026#39;token_type_ids\u0026#39;] input_ids = torch.LongTensor(input_ids) attention_mask = torch.FloatTensor(attention_mask) token_type_ids = torch.LongTensor(token_type_ids) return input_ids, attention_mask, token_type_ids, def get_test_dataloader(instances, batch_size=8, num_workers=4): def collate_fn(batch): input_ids, attention_mask, token_type_ids = zip(*batch) input_ids = pad_sequence(input_ids, batch_first=True).unsqueeze(1) # predict as single choice attention_mask = pad_sequence(attention_mask, batch_first=True).unsqueeze(1) token_type_ids = pad_sequence(token_type_ids, batch_first=True).unsqueeze(1) return input_ids, attention_mask, token_type_ids dataset = TestingDataset(instances) dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=False, \\ batch_size=batch_size, num_workers=num_workers) return dataloader # ## (TO-DO!) Define function to predict BERT scores # In[ ]: def predict_query_doc_scores(model, df): model.eval() start_time = time() # Parse CSV query_id_list = df[\u0026#34;query_id\u0026#34;] query_text_list = df[\u0026#34;query_text\u0026#34;] bm25_top1000_list = df[\u0026#34;bm25_top1000\u0026#34;] # Treat {1 query, K documents} as a dataset for prediction query_doc_scores = [] query_doc_ids = [] rows = zip(query_id_list, query_text_list, bm25_top1000_list) for qi, row in enumerate(rows, start=1): query_id, query_text, bm25_top1000 = row bm25_doc_id_list = bm25_top1000.split() query_doc_ids.append(bm25_doc_id_list) ################################################# # Collect all instances of query/doc pairs ################################################# query_instances = [] # Make query tokens for BERT query_tokens = tokenizer.tokenize(query_text) if len(query_tokens) \u0026gt; max_query_length: # truncation query_tokens = query_tokens[:max_query_length] query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens) query_token_ids.insert(0, tokenizer.cls_token_id) query_token_ids.append(tokenizer.sep_token_id) # Make input instances for all query/doc pairs for i, doc_id in enumerate(bm25_doc_id_list, start=1): if doc_id in doc_id_to_token_ids: doc_token_ids = doc_id_to_token_ids[doc_id] else: doc_text = doc_id_to_text[doc_id] doc_tokens = tokenizer.tokenize(doc_text) doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens) doc_id_to_token_ids[doc_id] = doc_token_ids doc_token_ids.append(tokenizer.sep_token_id) # make input sequences for BERT input_ids = query_token_ids + doc_token_ids token_type_ids = [0 for token_id in query_token_ids] token_type_ids.extend(1 for token_id in doc_token_ids) if len(input_ids) \u0026gt; max_input_length: # truncation input_ids = input_ids[:max_input_length] token_type_ids = token_type_ids[:max_input_length] attention_mask = [1 for token_id in input_ids] # convert and collect inputs as tensors input_ids = torch.LongTensor(input_ids) attention_mask = torch.FloatTensor(attention_mask) token_type_ids = torch.LongTensor(token_type_ids) # collect all inputs as a dictionary instance = {} instance[\u0026#39;input_ids\u0026#39;] = input_ids instance[\u0026#39;attention_mask\u0026#39;] = attention_mask instance[\u0026#39;token_type_ids\u0026#39;] = token_type_ids query_instances.append(instance) ################################################################# # Predict relevance scores for all BM25-top-1000 documents ################################################################# doc_scores = np.empty((0,1)) # Predict scores for each document dataloader = get_test_dataloader(query_instances, batch_size=batch_size*(num_negatives+1), num_workers=num_workers) for di, batch in enumerate(dataloader, start=1): batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids = batch \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the logits as relevance scores (using the same function of how you compute built-in loss) (Hint: You need to call a function of model which takes all the 3 tensors in the batch as inputs) 2. The scores are still on GPU. Reallocate them on CPU, and convert into numpy arrays. (Hint: You need to call two functions on the `scores` tensors. You can find them in PyTorch tutorial.) \u0026#39;\u0026#39;\u0026#39; with torch.no_grad(): ### 1. insert_missing_code_to_compute_logits ### scores = model( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True ).logits # merge all scores into a big numpy array ### step 2. insert_missing_function_1()###.###insert_missing_function_2() scores = scores.cpu().numpy() doc_scores = np.vstack((doc_scores, scores)) ## æ–°å¢new row # Progress bar with timer ;-) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Query: %d/%d | Progress: %d/%d | %s \\r\u0026#34; \\ % (qi, len(df), di, len(dataloader), elapsed_time), end=\u0026#39;\u0026#39;) # merge all query/BM25 document pair scores query_doc_scores.append(doc_scores) query_doc_scores = np.hstack(query_doc_scores).T print() return query_doc_scores, query_doc_ids # In[ ]: # ## (TO-DO!) Find best weight of BERT for BM25 rescoring on training set # In[ ]: dev_query_doc_scores, dev_query_doc_ids = predict_query_doc_scores(model, dev_df) print(\u0026#39;---- Grid search weight for \u0026#34;BM25 + weight * BERT\u0026#34; ----\u0026#39;) best_map_score, best_bert_weight = -100, 0.0 bert_scores = dev_query_doc_scores n_query = dev_query_doc_scores.shape[0] # Get MAP@K of BM25 baseline query_pos_doc_ids = dev_df[\u0026#39;pos_doc_ids\u0026#39;].values.tolist() actual = [doc_ids.split() for doc_ids in query_pos_doc_ids] bm25_predicted = [doc_id_list[:K] for doc_id_list in dev_query_doc_ids] map_score = mapk(actual, bm25_predicted, k=K) best_map_score = map_score print(\u0026#34;weight=%.1f: %.5f (BM25 baseline)\u0026#34; % (0, 100*map_score)) # Collect BM25 scores into same format of BERT scores \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Convert the BM25 top-1000 scores into 2d numpy arrays 2. BM25 scores should have the same shape and orders as `dev_query_doc_scores` (i.e. BERT scores) (Hint: If there are 24 dev-set queries, the shape should be (24, 1000) ) \u0026#39;\u0026#39;\u0026#39; ### 2. insert_whatever_you_want_to_meet_the_requirement_in_step2. bm25_scores = [scores.split() for scores in dev_df[\u0026#34;bm25_top1000_scores\u0026#34;]] bm25_scores = [[float(score) for score in scores] for scores in bm25_scores] bm25_scores = np.array(bm25_scores) # Grid search for BM25 + BERT rescoring low_bound, high_bound, scale = 0, 5, 1000 grids = [i / scale for i in range(low_bound * scale+1, high_bound * scale+1)] for weight in grids: \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the weighted scores using `bm25_scores`, `weight`, and `bert_scores` \u0026#39;\u0026#39;\u0026#39; weighted_scores = bm25_scores + weight * bert_scores ### 1. insert_missing_code ### # sort index and map to document ids as output rescore_argsort = np.flip(weighted_scores.argsort(), axis=1) predicted = [] for i in range(n_query): # num. of queries predicted.append([dev_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K]) map_score = mapk(actual, predicted, k=K) # show part of results for human evaluation if weight * 10 % 2 == 0: print(\u0026#34;weight=%.1f: %.5f\u0026#34; % (weight, 100*map_score)) # track weight with best MAP@10 if map_score \u0026gt; best_map_score: best_map_score = map_score best_bert_weight = weight print(\u0026#34;\\nHighest MAP@%d = %.5f found at weight=%.3f\u0026#34; % (K, 100*best_map_score, best_bert_weight)) # ## (TO-DO!) Rescore testing set with BERT for submission # In[ ]: # Predict BERT scores for testing set test_df = pd.read_csv(testing_csv_path) query_id_list = test_df[\u0026#34;query_id\u0026#34;] n_query = len(query_id_list) test_query_doc_scores, test_query_doc_ids = predict_query_doc_scores(model, test_df) bert_scores = test_query_doc_scores # In[ ]: # Rescore query/document score with BM25 + BERT bm25_scores = [scores.split() for scores in test_df[\u0026#34;bm25_top1000_scores\u0026#34;]] # parse into 2d list of string bm25_scores = [[float(score) for score in scores] for scores in bm25_scores] # convert to float bm25_scores = np.array(bm25_scores) \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the weighted scores using `bm25_scores`, `best_bert_weight`, and `bert_scores` \u0026#39;\u0026#39;\u0026#39; ### 1. insesrt_missing_code ### weighted_scores = bm25_scores + best_bert_weight * bert_scores # Rerank document ids with new scores rescore_argsort = np.flip(weighted_scores.argsort(), axis=1) ranked_doc_id_list = [] for i in range(n_query): # num. of queries ranked_doc_id_list.append([test_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K]) ranked_doc_ids = [\u0026#39; \u0026#39;.join(doc_id_list) for doc_id_list in ranked_doc_id_list] # Save reranked results for submission data = {\u0026#39;query_id\u0026#39;: query_id_list, \u0026#39;ranked_doc_ids\u0026#39;: ranked_doc_ids} submission_df = pd.DataFrame(data) submission_df.reset_index(drop=True, inplace=True) submission_df.to_csv(save_submission_path, index=False) print(\u0026#34;Saved submission file as `%s`\u0026#34; % save_submission_path) ","permalink":"https://jonathan-tw.github.io/posts/%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98ntust-information-retrieval/","summary":"HW1 - Vector Space Model TFã€IDFè¨ˆç®—æ–¹å¼: ä¸€é–‹å§‹æœƒè®€å–documentçš„è³‡æ–™å‰è™•ç†ï¼Œå…ˆå»åšä¸€äº›normalizedï¼Œè®“lexiconä¸è¦æœ‰é‡è¤‡çš„termï¼Œè€Œç‚ºäº†è®“ç¨‹å¼èƒ½åœ¨1åˆ†é˜å…§è·‘å®Œï¼Œlexiconä½¿ç”¨c++çš„unordered_mapä¸‹å»å­˜ï¼Œunordered_mapä½¿ç”¨hash func","title":"(èª²ç¨‹ç­†è¨˜)(NTUST) Information Retrieval"},{"content":"Layered Outline What\u0026rsquo;s a protocol?\nprotocols define format, order of messages sent and received among network entities, and actions taken on message transmission, receipt\nTCP/IP 5 Layer Application Layer HTTPã€DHCPã€RPCã€P2Pã€DNS Transport Layer UDPã€TCP Network Layer (IP) IPã€ICMPã€OSPFã€BGP Data-link Layer (MAC) MACã€VLANã€STP Physical Layer ç¶²è·¯æŒ‘ç·š ISO/OSI 7 Layer ï‚§ Presentation: allow applications to interpret meaning of data. example: encryption, compression, machine-specific conventions. ï‚§ Session: synchronization, checkpointing, recovery of data exchange.\nåªè¦æ˜¯åœ¨ç¶²çµ¡ä¸Šè·‘çš„åŒ…ï¼Œéƒ½æ˜¯å®Œæ•´çš„ã€‚å¯ä»¥æœ‰ä¸‹å±¤æ²’ä¸Šå±¤ï¼Œçµ•å°ä¸å¯èƒ½æœ‰ä¸Šå±¤æ²’ä¸‹å±¤ã€‚ IP address: ä¸€å€‹ç¶²å¡åœ¨ç¶²çµ¡ä¸–ç•Œçš„é€šè¨Šåœ°å€ï¼Œç›¸ç•¶æ–¼æˆ‘å€‘ç¾å¯¦ä¸–ç•Œçš„é–€ç‰Œè™Ÿç¢¼ã€‚ MAC address: èº«ä»½è­‰ï¼Œå€åŸŸç¶²å…§é€šä¿¡ç”¨ã€‚\nMacåœ¨data link layerä½¿ç”¨ï¼ŒIPåœ¨network layerä½¿ç”¨ï¼ŒIPä½ç½®åœ¨å€åŸŸç¶²å…§å¯èƒ½ç™¼ç”Ÿæ”¹è®Šï¼Œå”¯ä¸€æ€§çš„MACéœ€è¦ã€‚ macä½ç½®å…¨çƒå”¯ä¸€ï¼Œå›ºåŒ–åœ¨ç¶²å¡è£¡ã€‚OSè­˜åˆ¥å‡ºä¾†çš„macä½ç½®å¯ä»¥æ›´æ”¹ï¼Œä»–æ˜¯ä¸€å€‹stringï¼Œæˆ‘å€‘å¸¸èªªä¿®æ”¹çš„macæŒ‡çš„æ˜¯ä¿®æ”¹é›»è…¦è¨»å†Šè¡¨ä¸­çš„ç´€éŒ„ã€‚ OpenFlow: Flow Table Entries operates between controller, switch.\nApplication Layer HTTP ( Hyper Text Transfer Protocol ) Uses TCP; stateless. non-persistent HTTP â¡ at most one object sent over TCP connection,connection then closed. persistent HTTP â¡ multiple objects can be sent over single TCP connection between client, server. HTTP response time = 2RTT (å»ºç«‹TCPé€£ç·šã€è«‹æ±‚å’Œæ¥æ”¶ç‰©ä»¶)+ file transmission time\nserverå¸Œæœ›èƒ½è­˜åˆ¥ä½¿ç”¨è€…èº«åˆ†æä¾›ä¸åŒçš„å…§å®¹ï¼Œå› HTTPæ˜¯statelessï¼Œæ‰€ä»¥HTTPä½¿ç”¨cookiesã€‚\nçµåˆcookieå’Œuseræ‰€æä¾›çš„å¸³è™Ÿè³‡è¨Šï¼Œwebsiteèƒ½ç²å¾—å¤§é‡ä½¿ç”¨è€…çš„ç›¸é—œè³‡æ–™ï¼Œæœ‰å¯èƒ½è²©è³£é€™äº›è³‡è¨Š(å®‰å…¨ç–‘æ…®)\nWeb cache = proxy server ( server for original requesting clientã€client to origin server )\nproxy servers reduce response time for client request. proxy servers reduce traffic on an institution\u0026rsquo;s access link. Electronic Mail SMTP ( Simple Mail Transfer Protocol ) between mail servers to send email messages.\nTCP /port 25 use persistent connections. DNS ( Domain Name System ) A distributed, hierarchical database. Translate host name to IP address. ( UDP port/53 )\nroot DNS serverã€ TLD DNS server ( .com/.org/.edu/.gov )ã€ authoritative DNS server /au.thor.i.ta.tive/ Local DNS name server: each ISP (residential ISP, company, university) has one = DNS caching\nvideo streaming and content distribution networks (CDNs) DHCP ( Dynamic Host Configuration Protocol ï¼‰ ä¸»è¦æ˜¯ç”¨ï¤­æä¾›åœ¨TCP/ IPç¶²ï¤·ä¸Šçš„ä¸»æ©Ÿå¯ä»¥è‡ªå‹•çš„åˆ†é…åˆ°IPåŠæ‰€éœ€è¦çš„ç›¸é—œè¨­å®šã€‚ DHCPå¯åˆ†æˆï¥¸å€‹éƒ¨åˆ†ï¤­çœ‹ï¼Œä¸€å€‹æ˜¯DHCPä¼ºæœå™¨ç«¯ï¼Œå¦ä¸€å€‹æ˜¯DHCPç”¨æˆ¶ç«¯ã€‚\nDHCP serveræœ‰ä¸€å€‹é›†ä¸­å¼çš„ç®¡ï§¤ç¨‹å¼ï¼Œä¸»è¦è² è²¬è¨­å®šç¶²ï¤·å„ä¸»æ©Ÿä¸Šæ‰€éœ€è¦çš„IPè³‡è¨Šï¼Œä¸¦ æä¾›çµ¦Clientï¥ªå–ä½¿ç”¨ã€‚DHCPç”¨æˆ¶ç«¯æ˜¯è² è²¬å‘DHCPä¼ºæœå™¨ï¥ªå–æ‰€éœ€è¦ä¹‹ç›¸é—œIPè³‡è¨Šï¼Œä¸¦å°‡è³‡ï¦¾è¨»å†Šåˆ°è©²ç³»çµ±ä¸­ã€‚\nSockets Transport Layer what transport service does an app need?\ndata integrity timing throughput security TCP DEF â¡ reliable transportã€flow controlã€connection-oriented doesn\u0026rsquo;t provide â¡ timing, minimum throughput guarantee, security\nTCPçš„å¢å¼·ç‰¹æ€§ Fast Retransmit \u0026amp; Recovery ï¼ˆFRRï¼‰ï¼Œç•¶receiverç™¼ç¾ä¸ŸåŒ…æ™‚ï¼Œç™¼é€é‡è¤‡çš„ACKçµ¦å°æ–¹ï¼Œç•¶senderæ”¶åˆ°3å€‹é‡è¤‡çš„ackæ™‚ï¼Œå°±ç¢ºäººæ­¤packetç‚ºlossï¼Œé¦¬ä¸Šé€²è¡Œé‡å‚³(ä¸éœ€ç­‰å¾…time out)\nç‚ºï¦ºåœ¨ï¥§å¯é çš„ç¶²ï¤·ä»‹é¢ä¸Šå»ºï§·å¯é çš„å‚³é€è³‡ï¦¾æœå‹™ï¼ŒTCPå¿…é ˆè§£æ±ºå¯é æ€§ï¼Œflow controlçš„å•é¡Œï¼Œè€Œä¸”å¿…é ˆèƒ½ç‚ºä¸Šå±¤çš„æ‡‰ç”¨ç¨‹å¼ï¤­æä¾›å¤šå€‹åŸ å£ï¼Œç”¨ï¤­åŒæ™‚ç‚ºå¤šå€‹æ‡‰ç”¨ç¨‹å¼æä¾›è³‡ï¦¾ï¼ŒåŒæ™‚ï¼ŒTCPå¿…é ˆè§£æ±ºï¦šæ¥çš„å•é¡Œï¼Œé€™æ¨£å­çš„è©±ï¼ŒTCPæ‰æ˜¯å€‹å¯é çš„é€šè¨Šå”å®šï¼Œè€Œæœ€å¾Œä¹Ÿè¦å…‹æœé€šè¨Šå®‰å…¨çš„å•é¡Œã€‚ TCPï¦šæ¥æ˜¯å¯é çš„ï¼Œè€Œä¸”ä¿è¨¼ï¦ºå‚³é€å€æ®µçš„é †åºï¼Œä¿è¨¼é †åºçš„èƒ½ï¦Šæ˜¯ç”¨ä¸€å€‹åºè™Ÿï¤­é”æˆçš„ï¼Œå€æ®µå…§ä¹ŸåŒ…æ‹¬ä¸€å€‹åºï¦œè™Ÿï¼Œè¡¨ç¤ºæ¥æ”¶æ–¹æº–å‚™å¥½é€™å€‹åºè™Ÿçš„å€æ®µï¼Œåœ¨TCPå”å®šå‚³é€ä¸€å€‹å€æ®µæ™‚ï¼Œå®ƒæœƒåŒæ™‚æŠŠé€™å€‹å€æ®µæ”¾å…¥é‡æ–°ç™¼é€çš„åºï¦œä¸­ï¼ŒåŒæ™‚å•Ÿå‹•è¨ˆï¥©å™¨ï¼Œå¦‚æœæ”¶åˆ°ï¦ºç¢ºèªè¨Šæ¯ï¼Œå°±æœƒæŠŠé‡é€çš„å€æ®µåºï¦œåˆªé™¤ï¼Œå¦‚æœè¶…éè¨ˆï¥©æ™‚é–“æ™‚ï¼Œå°±æœƒå°‡é€™å€‹å€æ®µé‡é€ã€‚ Sliding Window: data flow controlçš„æŠ€å·§ï¼Œå®ƒè¦æ±‚sender hoståœ¨å‚³é€ä¸€å®šé‡çš„dataä»¥å¾Œï¼Œå¿…é ˆæ¥æ”¶å°æ”¾çš„ç¢ºèªã€‚ Window Sizeæ˜¯ç”±destinationæ±ºå®šï¼Œè€Œä¸”æœƒä¾ç¶²ï¤·ï§Šï¥¾èª¿æ•´ä¸¦éä¸€æˆï¥§è®Š(maybe congestion)ï¼ŒThe destination can send an acknowledgment at any timeã€‚\nTCP socket identified by 4-tuple:\nâ€¢ source IP address â€¢ source port number â€¢ dest IP address â€¢ dest port number\nDemux (ç”±ä¸‹åˆ°ä¸Šå‚³åˆ°socket): receiver uses all four values to direct segment to appropriate socket. each socket identified by its own 4-tuple. (ä¸åŒçš„source ip address or port numberï¼Œå°±æœƒè½‰äº¤çµ¦å…©ä»½ä¸åŒçš„socket)\nTCP congestion control flow control â¡ receiver æ§åˆ¶senderåˆ¥é€å¤ªå¤š (Receive Window)\ncwnd (Congestion Window, bytes) â¡ é™åˆ¶TCP senderè³‡æ–™é€å…¥ç¶²è·¯çš„é€Ÿç‡ï¼Œç‚ºäº†é¿å…congestionï¼Œå€¼æ‡‰è©²éƒ½\u0026lt; cwndã€‚\nrate = cwnd (é‡) / RTT bytes/sec ssthresh = cwnd / 2\nMSS: ( Maximum Segment Size ) â¡ MSSæ˜¯TCPæ•¸æ“šåŒ…æ¬¡èƒ½å¤ å‚³è¼¸çš„æœ€å¤§æ•¸æ“šåˆ†æ®µã€‚å»ºç«‹é€£ç·šä¹‹å‰é›™æ–¹è¬›å¥½ï¼Œå› TCPç‚ºé›™å‘é€£ç·šï¼Œæ‰€ä»¥å…©é‚Šå‚³éä¹‹æœ€å¤§å€¼å¯ä»¥ä¸ä¸€æ¨£\nçˆ²äº†é”åˆ°æœ€ä½³çš„å‚³è¼¸æ•ˆèƒ½ï¼ŒTCPå”è­°åœ¨å»ºç«‹é€£æ¥çš„æ™‚å€™é€šå¸¸è¦å”å•†é›™æ–¹çš„MSSå€¼ï¼Œé€™å€‹å€¼TCPå”è­°åœ¨å¯¦ç¾çš„æ™‚å€™å¾€å¾€ç”¨MTUå€¼ä»£æ›¿ï¼ˆéœ€è¦æ¸›å»IPæ•¸æ“šåŒ…åŒ…é ­çš„å¤§å°20Byteså’ŒTCPæ•¸æ“šæ®µçš„åŒ…é ­20Bytesï¼‰æ‰€ä»¥ä¸€èˆ¬MSSå€¼1460ï¼Œé€šè¨Šé›™æ–¹æœƒæ ¹æ“šé›™æ–¹æä¾›çš„MSSå€¼å¾—æœ€å°å€¼ç¡®å®šçˆ²é€™æ¬¡é€£æ¥çš„æœ€å¤§MSSå€¼ã€‚\nSlow Start â¡ initial.cwnd = 1MSSï¼Œæ¯ç•¶receiverå¾—åˆ°ä¸€æ¬¡ç¢ºèªå¾Œå°±å¢åŠ  1 MSS (å„å°‡cwnd+1ï¼Œdouble cwnd)ï¼Œ æ­¤ä¸€éç¨‹é€ æˆæ¯å›RTTä¸­å‚³é€é€Ÿç‡å€å¢ï¼Œå› æ­¤TCPçš„å‚³é€é€Ÿç‡å¾ä½é€Ÿé–‹å§‹ï¼Œæ¥ä¸‹ä¾†æœƒä»¥æŒ‡æ•¸æ€§åœ°æˆé•·ã€‚\næŒ‡æ•¸æˆé•· cwnd = cwnd + 1 å‡º2å›2 å‡º4å›4 å‡º8å›8ï¼Œå‚³çš„å–®ä½æ˜¯per ackï¼Œæ¯æ¬¡å›ä¾†å¹¾å€‹ackå°±+1å¹¾æ¬¡ã€‚\nSuccess Event\nincrease exponentially (new ack) Slow start to congestion avoidance, cwnd gets to 1/2 of its value before timeout (when cwnd \u0026gt;= ssthresh) Loss Event ( timeout )\ncut cwnd = 1 MSS. ssthresh = cwnd / 2 Congestion Avoidance â¡ ç·šæ€§æˆé•· cwnd = cwnd+1 (cwnd = cwnd + MSS * (MSS / cwnd) )\ncwndå…¨æ”¶å®Œackï¼Œæ•´å€‹cwndæ‰+1ã€‚\nFast Recovery â¡ senderé‡è¤‡æ”¶åˆ°3æ¬¡ç›¸åŒackæ™‚ï¼Œå¯èƒ½æ˜¯packet lossäº†ï¼Œ\nç•¶æ”¶åˆ°ä¸‰å€‹é‡è¤‡çš„ACKRenoå°±æœƒèªçˆ²ä¸¢åŒ…äº†ï¼Œå¹¶èªå®šç¶²çµ¡ä¸­ç™¼ç”Ÿäº†æ“å¡ã€‚RenoæœƒæŠŠç•¶å‰çš„ssthreshçš„å€¼è¨­ç½®çˆ²ç•¶å‰cwndçš„ä¸€åŠï¼Œä½†æ˜¯å¹¶ä¸æœƒå›åˆ°slow startéšæ®µï¼Œè€Œæ˜¯å°†cwndè¨­ç½®çˆ²ï¼ˆæ›´æ–°å¾Œçš„ï¼‰ssthresh+3MSSï¼Œä¹‹å¾Œcwndå‘ˆç·šæ€§å¢é•·ã€‚\nTCP New Reno NewRenoæ˜¯åŸºæ–¼Renoçš„æ”¹é€²ç‰ˆæœ¬ï¼Œä¸»è¦æ˜¯æ”¹é€²äº†Fast Recoveryã€‚\nNewRenoéœ€è¦æ”¶åˆ°è©²çª—å£å†…æ‰€æœ‰æ•¸æ“šåŒ…çš„ç¡®èªå¾Œæ‰æœƒé€€å‡ºå¿«é€Ÿæ¢è¤‡ç‹€æ€ï¼Œå¾è€Œæ›´ä¸€æ­¥æé«˜ååé‡ã€‚\nTCP SACK TCP SACKåœ¨TCP RenoåŸºç¤ä¸Šå¢åŠ äº†ï¼š\né¸æ“‡ç¡®èªï¼ˆSelective Acknowledgementsï¼ŒSACKï¼‰ é¸æ“‡é‡å‚³ï¼ˆSelective Retransmissionï¼‰ SACKå°±æ˜¯æ”¹è®ŠTCPçš„ç¢ºèªæ©Ÿåˆ¶ï¼Œæœ€åˆçš„TCPéš»ç¢ºèªç•¶å‰å·²é€£çºŒæ”¶åˆ°çš„æ•¸æ“šï¼ŒSACKå‰‡æŠŠäº‚åºç­‰ä¿¡æ¯æœƒå…¨éƒ¨å‘Šè¨´å°æ–¹ï¼Œå¾è€Œæ¸›å°‘æ•¸æ“šç™¼é€æ–¹é‡å‚³çš„ç›²ç›®æ€§ã€‚æ¯”å¦‚èªªåºè™Ÿ1ï¼Œ2ï¼Œ3ï¼Œ5ï¼Œ7çš„æ•¸æ“šæ”¶åˆ°äº†ï¼Œé‚£éº¼æ™®é€šçš„ACKéš»æœƒç¢ºèªåºåˆ—è™Ÿ4ï¼Œè€ŒSACKæœƒæŠŠç•¶å‰çš„5ï¼Œ7å·²ç¶“æ”¶åˆ°çš„ä¿¡æ¯åœ¨SACKé¸é …è£é¢å‘ŠçŸ¥å°ç«¯ï¼Œå¾è€Œæé«˜æ€§èƒ½ï¼Œç•¶ä½¿ç”¨SACKçš„æ™‚å€™ï¼ŒNewRenoç®—æ³•å¯ä»¥ä¸ä½¿ç”¨ï¼Œå› çˆ²SACKæœ¬èº«æ”œå¸¶çš„ä¿¡æ¯å°±å¯ä»¥ä½¿å¾—ç™¼é€æ–¹æœ‰è¶³å¤ çš„ä¿¡æ¯ä¾†çŸ¥é“éœ€è¦é‡å‚³å“ªäº›åŒ…ï¼Œè€Œä¸éœ€è¦é‡å‚³å“ªäº›åŒ…ã€‚\nTCP ECN ( Explicit Congestion Notification ) ECN = 11 = routerå·²é‡åˆ°æ“å¡ç‹€æ³ ç•¶routerå°‡ IP paecketçš„ ECN æ¬„ä½è¨­å®šç‚º 11 æ™‚ï¼Œreceiverå°±æœƒæ¥åˆ°è·¯å¾‘ä¸­congestionçš„é€šçŸ¥ã€‚ECN ä½¿ç”¨ TCP headerå‘å‚³é€ç«¯æŒ‡å‡ºç¶²è·¯æ­£é‡åˆ°æ“å¡ç‹€æ³ï¼ŒåŒæ™‚å‘æ¥æ”¶ç«¯æŒ‡å‡ºå‚³é€ç«¯å·²ç¶“å¾æ¥æ”¶ç«¯æ¥åˆ°æ“å¡æŒ‡æ¨™ï¼Œä¸¦ä¸”é™ä½å‚³è¼¸é€Ÿç‡ã€‚\nreceiver sets ECE bit on receiver-to-sender ACK segment to notify sender of congestion\nTCP Vegas Vegaså°‡RTTçš„å¢åŠ ä½œç‚ºcongestion networkçš„ä¿¡è™Ÿï¼ŒRTTå¢åŠ (cwndæ¸›å°)ã€RTTæ¸›å°(cwndå¢åŠ )ã€‚ Vegas é€šéæ¯”è¼ƒå¯¦éš›throughputå’Œexpect throughputä¾†èª¿ç¯€cwndçš„å¤§å°ã€‚\næœŸæœ›ååé‡ï¼šExpected = cwnd / BaseRTT (RTT lowest)ã€‚ å¯¦éš›ååé‡ï¼šActual = cwnd / RTTã€‚ â€“ Diff = ExpectedRate â€“ ActualRate. â€“ if Diff \u0026lt; a . increase CongestionWindow linearly. â€“ else if Diff \u0026gt; b. decrease CongestionWindow linearly. â€“ Else. leave CongestionWindow unchanged.\nç¶²çµ¡ä¸­ Vegas èˆ‡å…¶å®ƒç®—æ³•å…±å­˜çš„æƒ…æ³ä¸‹ï¼ŒåŸºæ–¼ä¸ŸåŒ…çš„æ“å¡æ§åˆ¶ç®—æ³•æœƒå˜—è©¦å¡«æ»¿ç¶²çµ¡ä¸­çš„ç·©æ²–å€ï¼Œå°ç·» Vegas è¨ˆç®—çš„ RTT å¢å¤§ï¼Œé€²è€Œé™ä½æ“å¡çª—å£ï¼Œä½¿å¾—å‚³è¼¸é€Ÿåº¦è¶Šä¾†è¶Šæ…¢ï¼Œå› æ­¤ Vegas æœªèƒ½åœ¨ Internet ä¸Šæ™®éæ¡ç”¨ã€‚\nAIMD: TCP senderæ”¶åˆ°ACKï¼ŒæˆåŠŸcwnd + 1ï¼Œå¤±æ•—cwnd = cwnd / 2 (åŠ æ€§å¢ï¼Œä¹˜æ€§æ¸›) Linear decrease in Vegas does not violate AIMD since it Happens before packets loss.\ntcp vegas / reno: Fast Retransmit â¡ Fast Recovery\nRandowm Early Detection (RED) Routeræ‰€æ¡è¨±çš„ä¸€ç¨®ç­–ç•¥ï¼Œé€équeueå¹³å‡é•·åº¦ä¾†detect congestion (AvgLen)ï¼Œè¨ˆç®—ä¸¢æ£„çš„æ¦‚ç‡ä¾†discard packet.\nRED is good at keeping avg. queue size steady. Thresholds are hard to determine.\nSequence Number TCPä»¥å‘å‰ï¥«ç…§ç¢ºèªï¤­æä¾›å€æ®µæ’åºä½œæ¥­ï¼Œæ¯å€‹è³‡ï¦¾åœ¨å‚³è¼¸å‰ï¨¦æœƒå…ˆåŠ ä»¥ç·¨è™Ÿï¼Œåˆ°ç›®çš„åœ°å·¥ä½œç«™æ™‚ï¼ŒTCPæœƒå°‡é€™äº›å€æ®µé‡æ–°çµ„æˆå®Œæ•´çš„è¨Šæ¯ã€‚ï¥´è³‡ï¦¾ä¸­å°‘ï¦ºæŸå€‹é †åºè™Ÿç¢¼ï¼Œå°±æœƒé‡å‚³è©²å€æ®µã€‚å€æ®µå‚³å‡ºå¾Œï¼Œï¥´åœ¨ä¸€æ®µç‰¹å®šæ™‚é–“å…§æ²’æœ‰æ”¶åˆ°ç¢ºèªï¼Œä¹Ÿæœƒè¦æ±‚é‡å‚³ã€‚\nUDP (best effort) DEF â¡ unreliable data transfer doesn\u0026rsquo;t provide â¡ reliabilityã€flow controlã€congestion controlã€timingã€ throughput guaranteeã€securityã€connection setup(handshaking)\nNetwork Layer Router examines header fields in all IP datagrams passing through it. Two key functions:\nforwarding ( data plane ): move packets from router\u0026rsquo;s input to appropriate router output. (æ€éº¼è½‰é€) routing ( control plane ): determine route taken by packets from source to destination. (è·¯æ€éº¼èµ°) Router Architecture Longest prefix matching: when looking for forwarding table entry for given destination address, use longest address prefix that matches destination address.\nSwitching fabrics: transfer packet from input buffer to appropriate output buffer.\néšŠé ­é˜»å¡ï¼ˆHead-of-line blockingæˆ–ç¸®å¯«ç‚ºHOL blockingï¼‰: å®ƒçš„åŸå› æ˜¯ä¸€åˆ—çš„ç¬¬ä¸€å€‹æ•¸æ“šåŒ…ï¼ˆéšŠé ­ï¼‰å—é˜»è€Œå°ç·»æ•´åˆ—æ•¸æ“šåŒ…å—é˜»ã€‚ä¾‹å¦‚å®ƒæœ‰å¯èƒ½åœ¨ç·©å­˜å¼è¼¸å…¥çš„äº¤æ›æ©Ÿä¸­å‡ºç¾ï¼Œæœ‰å¯èƒ½å› ç‚ºå‚³è¼¸é †åºéŒ¯äº‚è€Œå‡ºç¾ï¼Œäº¦æœ‰å¯èƒ½åœ¨HTTPæµæ°´ç·šä¸­æœ‰å¤šå€‹è«‹æ±‚çš„æƒ…æ³ä¸‹å‡ºç¾ã€‚\nIPv4 DEF â¡ 10.100.122.2è¢«é»åˆ†å‰²ç‚º4å€‹éƒ¨åˆ†(Byte)ï¼Œæ¯å€‹Byte = 8å€‹bitï¼Œæ‰€ä»¥ipv4ç‚º32 bit addressã€‚ ç”± 0é–‹é ­ åˆ° 127çµå°¾çš„IP æ˜¯ A Class ç”± 128é–‹é ­ åˆ° 191çµå°¾çš„IPæ˜¯ B Class ç”± 192é–‹é ­ åˆ° 223çµå°¾çš„å‰‡ç‚º C Class\nPrivate Addresses Net IDã€Host IDèˆ‡Mask DEF â¡ å®šç¾©èˆ‡åˆ‡å‰²ç¶²è·¯ç”¨é€”ï¼Œä¸åŒé¡å‹çš„IPæœƒå°æ‡‰åˆ°æœ‰è‘—ä¸åŒçš„Net_IDã€‚ A Classçš„IP / default mask : å‰é¢1 byteæ•¸å­— = Net IDï¼Œå…¶é¤˜ä¸‰çµ„ = Host ID / 255.0.0.0 B Classçš„IP / default mask: å‰é¢2 byteæ•¸å­— = Net IDï¼Œå¦å…©çµ„ = Host ID / 255.255.0.0 C Classçš„IP / default mask: å‰é¢3 byteæ•¸å­— = Net IDï¼Œå‰©ä¸‹çš„ä¸€çµ„ = Host / 255.255.255.0\nQ: 132.21.0.0, find the class, the block, and the range of the addresses? A: The class is B. The block has a Net_ID of 132.21. The addresses range from 132.21.0.0 to 132.21.255.255.\nSupernet (CIDR)èˆ‡ Subnet Supernet â¡ åˆ©ç”¨ Subnet Mask é‡æ–°å®šç¾©è¼ƒçŸ­çš„Net IDï¼Œå€Ÿç”¨Net IDå¾Œé¢å¹¾å€‹bitã€‚ Subnet â¡ åˆ©ç”¨ Subnet Mask é‡æ–°å®šç¾©è¼ƒé•·çš„Net IDï¼Œå€Ÿç”¨Host IDå‰é¢å¹¾å€‹bitï¼Œå¾Œé¢ç•¶ä½œå­ç¶²è·¯çš„host addressã€‚ **Host_Idä¹‹maskåˆ‡æ³•: çœ‹è¦å¹¾å€‹sub-net(åˆ‡å¹¾å€‹1å‡ºå»)ï¼Œå‰©ä¸‹0ç‚ºè©²sub-netä¸‹å¯ä½¿ç”¨çš„ip addresså€‹æ•¸ã€‚ **å³ç§» Net_Idä¹‹maskåˆ‡æ³•: çœ‹è¦å¹¾å€‹class(åˆ‡å¹¾å€‹0å‡ºå») å·¦ç§» 205.16.37.24/29 : 8 addresses in the block. mask address: 255.255.255.X\ndestination AND mask = supe\trnet/subnet address.\nä»¥network blockè·Ÿnetwork blockä¹‹é–“ç‚ºç‚ºå–®ä½é€²è¡Œåˆ†å‰²ã€‚\nQ: 201.70.64.0, need 6 subnets. Design the subnets A: 8 -3 = 5(host_id_0s) subnetså…±8å€‹. The number of address in each subnet is 32 = ä»£è¡¨1å€‹subnetä¸­(ç¸½å…±8å€‹)ï¼Œå¯å®¹ç´32å€‹ä¸åŒçš„host ip address\nQ: We need to make a supernetwork out of 16 class C blocks. What is the supernet mask? A: 11111111 11111111 11110000 00000000 ( net_idå·¦ç§»4 bit )ã€‚\nDelivery and Routing of IP Packets Routing IP Packets Policy: Direct delivery â¡ Host-specific â¡ Network-specific â¡ Default indirect delivery: destination is in another network. If not present, use direct delivery. Host-specific: the entry in the destination field is host-specific address. If not present, destination field is network-specific address.\nnext-hopç‚ºè©²hoståˆ°é¡Œç›®æŒ‡å®šrouterçš„å‰ä¸€å€‹router input ip address (åˆ°é¡Œç›®æŒ‡å®šrouter)ã€‚\nIPv6 other changes from IPv4:\nchecksum: removed entirely to reduce processing time at each hop options: allowed, but outside of header, indicated by â€œNext Headerâ€ field ICMPv6: new version of ICMP IP Packet IP packet = header(æ§åˆ¶è³‡è¨Š) + payload(è³‡æ–™æœ¬èº«) IP Header(æœ€çŸ­20ã€æœ€å¸¸60): é•·åº¦ç‚º4 Bytesçš„å€æ•¸ IP Payload(æœ€çŸ­8ã€æœ€å¸¸65515)\nHLEN = Header Length(ä½”4 bits)ï¼Œç´€éŒ„æ­¤IP headerçš„é•·åº¦ï¼ŒHLENçš„è¨ˆç®—æ˜¯ä»¥4 Bytesç‚ºåŸºæœ¬å–®ä½ã€‚ï¦µå¦‚ï¼šHLENï¤ä½å€¼ç‚º0101ï¼Œå³ä»£è¡¨IP headerçš„é•·ï¨ç‚º5 Ã—4 = 20 Bytesï¼Œæœ€é•·å¯é”15Ã—4 = 60 Bytes\nIdentification (16 bits) â¡ ï¥´packetåœ¨å‚³è¼¸éç¨‹ä¸­å› ç‚ºMTUçš„é™åˆ¶ï¼Œå°è‡´å‚³è¼¸éç¨‹ä¸­ï¼Œå°‡packetï¨€å‰²æˆå¹¾å€‹Fragmentsé€²è¡Œå‚³é€ï¼Œè€Œå› ç‚ºæ¯å€‹IP packetåˆ°é”ç›®çš„è£ç½®çš„å…ˆå¾Œé †åºå¯èƒ½èˆ‡å‡ºç™¼æ™‚çš„é †åºï¥§åŒï¼Œå› æ­¤æ¥æ”¶ç«¯åœ¨é€²ï¨ˆpacketé‡çµ„æ™‚ï¼Œï¥¥å¿…é ˆä»¥Identificationé€²ï¨ˆåˆ¤æ–·IP packetåŸï¤­çš„é †åºï¼Œä»¥ï¥¥èƒ½å°‡å±¬æ–¼ç›¸åŒè³‡ï¦¾packetçš„fragmentçµ„åˆåœ¨ä¸€èµ·ã€‚\nTotal Length (16 bits) â¡ è¨˜ï¤¿æ•´å€‹packetçš„é•·ï¨ï¼ŒåŒ…å«IP headeråŠIPæ‰€å¸¶è³‡ï¦¾å…§å®¹çš„ç¸½å’Œã€‚\nProtocol: which protocol to use.\nTime To Liveï¼ˆå­˜æ´»æ™‚é–“ï¼‰â¡ ä½”8 Bitsï¼Œè¨˜ï¤¿IPå°åŒ…çš„ã€Œå­˜æ´»æ™‚é–“ã€ï¼Œä»¥é™åˆ¶IPå°åŒ…åœ¨ï¤·ç”±å™¨ä¹‹é–“è½‰é€çš„æ¬¡ï¥©ã€‚ç•¶IPå°åŒ…æ¯ç¶“éä¸€éƒ¨ï¤·ç”±å™¨æ™‚ï¼Œï¤·ç”±å™¨ï¥¥æœƒå°‡Time to Liveï¤ä½å€¼æ¸›1ï¼Œç•¶ï¤·ç”±å™¨æ”¶åˆ°æ­¤ï¤ä½å€¼ç‚º1çš„IPå°åŒ…æ™‚ï¼Œï¥¥ç›´æ¥å°‡ä¹‹ä¸Ÿæ£„ï¼Œï¥§å†è½‰é€ã€‚\nFlagï¼ˆå°åŒ…ï¨€å‰²æ——æ¨™ï¼‰â¡ ä½”3 Bitsï¼Œä¸»è¦å°IPå°åŒ…çš„ï¨€å‰²æä¾›æ§åˆ¶è¨Šæ¯ã€‚ ç¬¬1å€‹bit: æœªå®šç¾©ã€‚ ç¬¬2å€‹bit: D æ˜¯å¦å¯ä»¥åˆ‡å‰² ç¬¬3å€‹bit: M 0 = æœ€å¾Œfragmentationï¼Œ1 = å°šæœ‰å…¶ä»–fragmentationã€‚ If the M bit is 0, it means that there are no more fragments If the M bit is 1, it means that there is at least one more fragment\nIP Packetåˆ‡å‰²èˆ‡é‡çµ„ routerä¸­å¿…é ˆæœ‰IP packetçš„åˆ‡å‰²èˆ‡é‡çµ„æ©Ÿåˆ¶ï¼Œå³æ˜¯å°‡éé•·çš„packetåŠ ä»¥åˆ‡å‰²ï¼Œä»¥ä¾¿èƒ½åœ¨MTUè¼ƒå°çš„ç¶²è·¯ä¸Šå‚³è¼¸ã€‚ï¨€å‰²å¾Œçš„IPå°åŒ…ï¼Œæœƒç”±ç›®çš„è£ç½®é‡çµ„ï¼Œæ¢ï¥¦æˆåŸï¤­IPå°åŒ…çš„æ¨¡æ¨£ã€‚\nIP æœƒå°‡å°åŒ…åˆ‡å‰²æˆå¤šå€‹è¼ƒå°çš„ (å°æ–¼ MTU) fragmentï¼Œä½¿å…¶èƒ½é€édata-linkå‚³è¼¸ ï¼Œ ç›®çš„ç«¯æ¥æ”¶å®Œæ‰€æœ‰fragmentå¾Œï¼Œå†å°‡fragmenté€²è¡Œé‡çµ„ã€‚\nIP Fragmentationä¸­IPv4 èˆ‡ IPv6ï¼Œå…¶å°åŒ…å¤§å°ä¸Šé™ï¼Œåˆ†åˆ¥ç‚º 65535 èˆ‡ 65575 Bytesã€‚ é é è¶…å‡ºäº†data-linkçš„frameå¤§å°ã€‚\nMTU ( Maximum Transmission Unit ) DEF â¡ data-linkä¸­çš„MTUè¦ç¯„äº†frameçš„å¤§å°ä¸Šé™ã€‚ ä¹™å¤ªç¶²è·¯ (Ethernet) ä¸­çš„MTU: 1500 å€‹ä½å…ƒçµ„ã€‚\nICMP ( Internet Control Message Protocol ) pingæ˜¯åŸºæ–¼ICMPå®Œæˆçš„ã€‚å®ƒçš„ç›®çš„å°±æ˜¯è®“æˆ‘å€‘èƒ½å¤ æª¢æ¸¬ç¶²ï¤·çš„ï¦šç·šï§ºæ³ï¼Œä¹Ÿèƒ½ç¢ºä¿ï¦šç·šçš„æº–ç¢ºæ€§ï¼Œï¥§éç”±æ–¼åƒ…æ˜¯æ§åˆ¶è¨Šæ¯çš„å‚³éä¸¦ç„¡è©³ç´°çš„æŒ‡å°ï¼Œæ‰€ä»¥ä¸€èˆ¬è€Œè¨€ï¤­æºç«¯ä¸¦ï¥§è™•ï§¤è©²è¨Šæ¯\nARP ( Address Resolution Protocol ) DEF â¡ å·²çŸ¥IP addressï¼Œæ±‚MAC addressçš„protocolï¼Œåˆ©ç”¨ã€Œå¼ã€çš„æ–¹å¼ã€‚ç™¼é€ä¸€å€‹broadcast packetï¼Œèª°æ˜¯é€™å€‹IPèª°ä¾†å›ç­”ã€‚switchæœƒè¨˜ä½ç™¼ä¾†è«‹æ±‚çš„senderçš„mac addressï¼Œè€Œswitchå­¸ç¿’åˆ°çš„çµæœç¨±ç‚ºforwarding table(æœ‰éæœŸæ™‚é–“)ã€‚\nRouting Protocol RIP ( Routing Information Protocol ) RIP version 2 supports CIDR. RIP uses the services of UDP on well-known port 520. OSPF ( Open Shortest Path First ) ç”±æ–¼OSPFè·¯ç”±å”å®šçš„ç”¢ç”Ÿï¼Œæ˜¯ç‚ºäº†è§£æ±ºRIPè·¯ç”±å”å®šçš„è¨­å‚™æ•¸é‡çš„å•é¡Œï¼Œæ‰€ä»¥OSPFè·¯ç”±å”å®šå°±æ²’æœ‰è¨­å‚™æ•¸é‡ï¼ˆHop Countï¼‰çš„é™åˆ¶ã€‚æœƒä½¿ç”¨LSAçš„æ–¹å¼èˆ‡å…¶ä»–è·¯ç”±å™¨è¨­å‚™äº¤æ›è³‡è¨Šï¼Œè€Œä¸åªæ˜¯Routing Tableçš„æ›´æ–°è³‡è¨Šè€Œå·²ã€‚æ¯ä¸€å°è·¯ç”±å™¨è¨­å‚™æœƒè‡ªè¡Œè¨ˆç®—å±¬æ–¼è‡ªå·±çš„ã€Œæœ€ä½³ç¶²è·¯è·¯å¾‘ã€ï¼Œè€Œé€™æ¨£çš„è³‡è¨Šåœ¨å„å€‹è·¯ç”±å™¨è¨­å‚™ä¹‹é–“æ˜¯ä¸å®Œå…¨ç›¸åŒçš„ã€‚ä¸€å€‹LSAå°åŒ…ä¸­åŒ…å«äº†ä»‹é¢çš„è³‡è¨Šã€æ‰€ä½¿ç”¨çš„ç¶²è·¯è·¯å¾‘è©•åˆ¤æ¨™æº–ï¼ˆMetricï¼‰ä»¥åŠå…¶ä»–ç›¸é—œçš„è³‡è¨Šã€‚ç•¶ç„¶ï¼Œé—œæ–¼è·¯å¾‘çš„é¸æ“‡ï¼ŒOSPFè·¯ç”±å”å®šæ˜¯æ¡ç”¨æœ€çŸ­è·¯å¾‘å„ªå…ˆæ¼”ç®—æ³•ï¼ˆShortest Path First Algorithmï¼‰ã€‚\næ¯ä¸€å°è·¯ç”±å™¨è¨­å‚™æœƒè‡ªè¡Œè¨ˆç®—å±¬æ–¼è‡ªå·±çš„ã€Œæœ€ä½³ç¶²è·¯è·¯å¾‘ã€ï¼Œè€Œé€™æ¨£çš„è³‡è¨Šåœ¨å„å€‹è·¯ç”±å™¨è¨­å‚™ä¹‹é–“æ˜¯ä¸å®Œå…¨ç›¸åŒçš„ã€‚\nIn OSPF, all routers have the same link state database. OSPF packets are encapsulated in IP datagrams. Autonomous System: ç”±ä¸€å€‹å–®ä½ç®¡ç†ï¼Œä¾‹å¦‚ ISPã€è¶…ç´šå¤§å­¸æ ¡ã€å¤§å…¬å¸ç­‰ç­‰ï¼Œä»€éº¼éƒ½å¤§çš„çµ„ç¹”ã€‚æ¯å€‹ AS è£¡é¢çš„è·¯ç”±è¡¨éƒ½è‡ªå·±ç®¡ï¼Œéš»è¦äº‹å…ˆå®£å‘Šè‡ªå·±æŒæœ‰ä»€éº¼ç¶²æ®µå°±è¡Œäº†ã€‚\nBGP ( Border Gateway Protocol ) routing among the ISPs ï¼ŒBGPæœƒè‡ªå‹•æ±ºå®šå„ç¨®ç‹€æ…‹ä¸‹çš„routerï¼Œæ±ºå®šæ±è¥¿å¾å“ªé‚Šèµ°ï¼Œè©²åœ¨å“ªé‚Šåœ(å¤–éƒ¨ç¶²è·¯ä¸‹)ã€‚ ç¶­è­·router tableä¾†å¯¦ç¾ASä¹‹é–“çš„å¯é”æ€§ã€‚\nSimple Network Management Protocol (SNMP ) = ç°¡å–®ç¶²è·¯ç®¡ç†å”å®šä¸€æ–¹é¢ä¸åƒ…å¯ä½¿ç”¨æ–¼ç¶²è·¯è¨­å‚™ä¹‹æ—¥å¸¸ç¶­é‹ä½œæ¥­ï¼Œäº¦å¯æä¾›ç¶²è·¯ç¶­é‹äººå“¡å³æ™‚ç›£æ§è¨­å‚™ç•°å¸¸äº‹ä»¶ç™¼ç”ŸåŠå› æ‡‰è™•ç†ã€‚\nå…¶ä»– HUB(é›†ç·šå™¨) DEF â¡ å®Œå…¨åœ¨physical layerå·¥ä½œï¼Œå®ƒæœƒå°‡è‡ªå·±æ”¶åˆ°çš„æ¯ä¸€å€‹byteï¼Œéƒ½è¤‡è£½åˆ°å…¶ä»–portä¸Šå»ã€‚ ç¼º: ä¸ç®¡æŸå€‹ä»‹é¢æ˜¯å¦éœ€è¦ï¼Œæ‰€æœ‰çš„bitéƒ½æœƒè¢«ç™¼é€å‡ºå»ï¼Œç„¶å¾Œè®“ä¸»æ©Ÿä¾†åˆ¤æ–·æ˜¯ä¸æ˜¯éœ€è¦ã€‚\ndigital subscriber line (DSL): network line and telephone line to central office DSLAM. cable network: å…±äº«é »å¯¬ï¼Œéœ€è¦åˆ†æ•£å¼çš„å¤šé‡å­˜å–å”å®šï¼Œç”¨ä¾†å”èª¿å‚³è¼¸ï¼Œé¿å…ç¢°æ’ã€‚\nå¤§éƒ¨åˆ†packet switchä½¿ç”¨store-and-forward transmission (å„²å­˜è½‰é€å‚³è¼¸)ï¼Œrouterå¿…é ˆå…ˆæ¥æ”¶åˆ°æ•´å€‹å°åŒ…ï¼Œæ‰é–‹å§‹å°‡ç¬¬ä¸€å€‹ä½å…ƒå‚³è¼¸åˆ°å¤–éƒ¨é€£çµã€‚\nPacket Switching vs Circuit Switching: ç¶²éš›ç¶²è·¯ä½¿ç”¨æ­¤ï¼Œè³‡æºä¸æœƒè¢«ä¿ç•™(queueing delay loss)/è³‡æ–™æœƒè¢«é ç´„ä¿ç•™ queueing delay: packet arrival rate to link (temporarily) exceeds output link capacity.\n","permalink":"https://jonathan-tw.github.io/posts/%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98ntust-network-communication-protocols/","summary":"Layered Outline What\u0026rsquo;s a protocol? protocols define format, order of messages sent and received among network entities, and actions taken on message transmission, receipt TCP/IP 5 Layer Application Layer HTTPã€DHCPã€RPCã€P2Pã€DNS Transport Layer UDPã€TCP Network Layer (IP) IPã€ICMPã€OSPFã€BGP Data-link Layer (MAC) MACã€VLANã€STP Physical Layer ç¶²è·¯æŒ‘ç·š ISO/OSI 7 Layer ï‚§ Presentation: allow applications to interpret meaning of data. example: encryption, compression, machine-specific conventions. ï‚§ Session: synchronization, checkpointing, recovery of data exchange. åªè¦æ˜¯åœ¨ç¶²çµ¡ä¸Šè·‘çš„åŒ…ï¼Œéƒ½æ˜¯å®Œæ•´çš„","title":"(èª²ç¨‹ç­†è¨˜)(NTUST) Network Communication Protocols"},{"content":"Improving Performance of QUIC in WiFi Problem Domain WiFi (wireless)\nSolution ç”±æ–¼802.11 frame aggregationæœ‰åŠ©æ–¼æ”¹å–„throughputï¼Œæ‰€ä»¥è®“QUICå¢åŠ traffic burstiness(BQUIC).\nreducing ACK frequency. In GQUIC ACK_DECIMATION mode, è‡³å°‘æ”¶åˆ°100 packetsæ‰å›å‚³ACK. disabling packet pacing. packet pacing aims to reduce sending bursts of packet, it may hinder performance in high-speed networks with low loss rates. Evaluation QUIC: detection algorithmåˆ©ç”¨ACKåŠRTTé€²è¡Œdetectã€‚ä½¿ç”¨packet pacingæ™‚ï¼Œæœƒé™ä½aggregationï¼Œå°è‡´packet delayå¢åŠ è¢«detectåˆ°( minimum delay \u0026gt; threshold ) QUICå¾slow-startææ—©é›¢é–‹ã€‚\nBQUIC: ä¸ä½¿ç”¨packet pacingï¼Œinter packet timeé™ä½ã€‚\nBQUIC increases the throughput between 20% to 30%\nFindings å¾pacingæ©Ÿåˆ¶ä¸Šé€²è¡Œæ”¹è‰¯ï¼Œè§£æ±ºæ–‡ä¸­slow-startè¼ƒæ—©é€€å‡ºçš„å•é¡Œã€‚ å‹•æ…‹æ”¹å–„packet pacing: åœ¨æ‰åŒ…ç‡å°çš„ç’°å¢ƒé€²è¡Œframe aggregationï¼Œåœ¨æ‰åŒ…å¤§çš„ç’°å¢ƒä½¿ç”¨åŸæœ¬QUICã€‚ieda: ä¾æ“šä¸‹å±¤ç¶²è·¯ç’°å¢ƒçš„ä¸åŒï¼Œè€Œæœ‰å½ˆæ€§çš„ä½¿ç”¨ä¸åŒçš„pacingæ©Ÿåˆ¶ã€‚\nThe QUIC Fix for Optimal Video Streaming Problem Domain DASH (media in H.264)\nSolution reliable on I-Frame + unreliable on B/P-Frameï¼ŒB/P-Frameåƒè€ƒI-Frameå°å…¶é€²è¡Œå¢é‡orå·®ç•°çš„encodingï¼Œä¸Ÿäº†I-Frameï¼Œå…¶ä»–B/P-Frameä¹Ÿæ²’ç”¨äº†ã€‚ unreliable data: - QUIC handshake to negotiate support for unreliable streams - replace retransmission of missed data with new data - provide a meta stream within QUIC to tag individual QUIC frames as reliable or unreliable. non-trivial challenge: synchronization of the streams: FEC on unreliable data Evaluation bufRatio = re-buffering time / total video time\nrateBuf = frequency of re-buffering events to / total video frame\naSSIM: æ”¹å–„æ²’æœ‰è€ƒé‡latencyçš„æ¸¬é‡æ–¹å¼ï¼Œæ™šåˆ°çš„frameæœƒdelayï¼Œè€ƒé‡äº†å…¶stallå°frameå½±éŸ¿çš„ç¶œåˆè©•ä¼° The rateBuf values for both ClipStream and ClipStreamFEC are very close to 0%:\nçœŸæ­£è¦reliableçš„packet numberè®Šå¾ˆå°‘ (approx. 1% by count or 12% by size) of the overall video stream reliably\nFindings å¾econder/decoderä¸Šé€²è¡Œ( gpu RGG\u0026lt;\u0026ndash;\u0026gt;YUV )ï¼Œé™ä½è§£ç¢¼CPUå ç”¨ç‡ã€‚\nan out-of-order packet is inserted into the byte-stream within re-order buffer unless the data has already been consumed by the application. If the application tries to consume \u0026ldquo;missing\u0026rdquo; byte-ranges the byte-stream is filled with zeros.\nQ: (Sync) How will the receiver combine the frames from the different streams into the appropriate order in the playback buffer?\nA: add a reliable control stream to signal multiplexing and demultiplexing information for diff sterams, The control stream helps the client to re-assemble the video file (reliably transfer end-of-stream )\nQUICsilver: Optimising QUIC for use with Real-Time Multimedia Traffic (University of Glasgow 2019) Problem Domain RTP (media in H.264)\nSolution æå‡ºplayback deadlineçš„æ¦‚å¿µï¼Œåœ¨retransmitä¹‹å‰ï¼Œç¢ºå®šå…§å®¹çš„æœ‰æ•ˆæ€§(æœ‰æ²’æœ‰ç”¨)ï¼Œæœ‰åŠ©æ–¼æ–¼æ¸›å°‘ç™¼é€ä¸è¦çš„dataã€‚ server predicts that the packet will reach the client in time to be useful for video playback . implementing partial reliability within QUIC can assist with reducing latencies and playback delays\nFindings åœ¨QUICä¸­åŠ å…¥Meda-awareness\n","permalink":"https://jonathan-tw.github.io/posts/2019-ieee-improving-performance-of-quic-in-wifi-2018-conext-the-quic-fix-for-optimal-video-streaming/","summary":"Improving Performance of QUIC in WiFi Problem Domain WiFi (wireless) Solution ç”±æ–¼802.11 frame aggregationæœ‰åŠ©æ–¼æ”¹å–„throughputï¼Œæ‰€ä»¥è®“QUICå¢åŠ traffic burstiness(BQUIC). reducing ACK frequency. In GQUIC ACK_DECIMATION mode, è‡³å°‘æ”¶åˆ°100 packetsæ‰å›å‚³ACK. disabling packet pacing. packet pacing aims to reduce sending bursts of packet, it may hinder performance in high-speed networks with low loss rates. Evaluation QUIC: detection algorithmåˆ©ç”¨ACKåŠRTTé€²è¡Œde","title":"(2019 IEEE) Improving Performance of QUIC in WiFi \u0026 (2018 CoNEXT) The QUIC Fix for Optimal Video Streaming"},{"content":"Summary QUICä¸åŠTCPåœ¨DASHçš„è¡¨ç¾ï¼Œç”±æ–¼å¤§å¤šæ•¸ABR algorighméƒ½ç¶“éå„ªåŒ–èˆ‡TCPé…åˆä½¿ç”¨ï¼Œå› æ­¤æ²’æœ‰å……åˆ†åˆ©ç”¨QUICæä¾›çš„åŠŸèƒ½ã€‚\nProblem statement What is the impact of QUIC on QoE? How should adaptive bitrate streaming be built to leverage the benefits provided by QUIC? Research objectives A Performance Study and future work Methods we studied QoE performance of different DASH quality adaptation algorithms that are either solely based on the playout buffer filling, or on the video segment download rate, or on both. DASH quality adaptation algorithms that are mainly categorized as throughputand/or playout buffer-based techniques. BB2-2: based on buffer. map current buffer occupancy to a quality bitrate.\nthe client tends to download a quality that is higher than the measured rate in order to provide the user with maximum average quality.\nSQUAD: based on rate that is specifically optimized for variations in TCP download rates. BOLA: based on both.\nEvaluation How does the author appraise their methodology?\nTestbed (Parallel Server / Single Server) bottleneck link of 10Mbps / Python-based DASH player / Caddy server Internet (wifi on campus / wifi on residential / wired campus Network ) with 17 bitrates ranging from 100Kbps on Aamazon EC2 How is the experiment set up?\nAverage Quality Bitrate (AQB) = Chosen Rate\nNumber of Quality Switches (#QS)\nSpectrum (H): A lower H indicates a better QoE. (bitrateè®ŠåŒ–çš„é›†ä¸­åº¦é‡)\nRebuffering Ratio:\n$$ \\frac {Total;length - video;length } {video;length } $$\nWe investigated different QoE performance metrics such as the average quality bitrate, a measure of quality variations (denoted spectrum) (å½±ç‰‡è³ªé‡å·®ç•°-é »è­œ) and the average video stalling duration. Testbed BBA-2 seems to benefit from using QUIC to download segments because of the slow-start or initial phase of the algorithm.\nthe client tends to download a quality that is higher than the measured rate in order to provide the user with maximum average quality. åªæœ‰è¼ƒå¤§çš„RTTæ™‚ï¼Œæœƒé€ æˆbufferä¸è¶³ï¼Œé–“æ¥ä½¿å¾—BBA-2 clientåˆ‡æ›åˆ°lower quality.\n(rule-of-thumb)bandwidth-delay product(BDP): B = C * RTT (receive bufferæ”¶åˆ°çš„å¤§å°ï¼Œå·²å‚³é€ä½†å°šæœªç¢ºèªçš„è³‡æ–™)\nInternet Wifi significantly worse quality bitrate: QUIC is likely to be competing with more TCP streams than other QUIC streams, it takes longer than TCP to fetch the same segments Future Work HOL blocking (Multiplexing,Multi-Path)\nPacing\nIf a client requests multiple qualities for a single segment, the server could pace the streams to deliver segments at regular intervals instead of using AIMD. å¦‚æœå–®å€‹ç¶²æ®µæä¾›å¤šç¨®è³ªé‡ï¼Œå®šæœŸæ™‚é–“ä¸‹ï¼Œä»¥streamçš„é€Ÿåº¦ä¸‹å»äº¤ä»˜segmentsè€Œä¸æ˜¯AIMD\nIf a client requests multiple segments over the same connection via multiple streams the server could implement a decreasing pacing rate for segments requested depending on how soon they are required for playback. clientå¦‚æœåœ¨å¤šå€‹streamä¸­è«‹æ±‚å¤šå€‹segmentï¼ŒServerå¯ä»¥æ ¹æ“šã€Œè«‹æ±‚çš„segmentã€éœ€è¦å¤šé•·æ™‚é–“ä¾†é™ä½pacing rate\nAIMD -\u0026gt; TCPæ“å¡æ§åˆ¶çš„æ–¹æ³•\nEliminate Congestion Control Redundancy two congestion on client; in trans/ in DASH algo\nABR clients can disable congestion control in the QUIC transport layer and continue to use their existing congestion control\nQUIC provides the benefits of pacing and assistance for loss recovery in the form of NACKs\nRTP does that.\nConclusion we find through testbed and Internet measurements that QUIC does not provide a boost to current DASH algorithms but instead a degradation in the chosen quality bitrates. Although we observe a lower magnitude of quality variations (è¼ƒä½çš„è³ªé‡è®ŠåŒ–å¹…åº¦), the degradation of streamed quality bitrate with the use of QUIC is detrimental to overall QoE References ABR work/ congestion control / congestion avoidance\n[14]: Tcp hollywood: An unordered, time-lined, tcp for networked multimedia applications (2016)\nTCP Hollywood,a TCP variant, which implements out-of-order delivery and inconsistent retransmissions in order to improve good-put of video streaming applications\n[21]: Media qoe enhancement with quic (2016)\na new congestion control mechanism using QUIC that aggressively varies download rate according to a buffer-based priority level assigned by the ABR streaming client\n[4]: ABR protocol over UDP (2003)\nthe authors employ a form of congestion avoidance where the sending rate at the server is increased by a single packet for every RTT measurement.This design is different from the AIMD congestion control employed by TCP and QUIC since it eliminates the effect of slow start and attempts to provide an accurate estimate of the available bandwidth in the network. Some drawbacks of this approach are the requirement of two UDP sockets for every connection and the use of Berkeley Packet Filters to collect timestamps at the server and client for every video stream, thus, reducing both performance and scalability of the system\n","permalink":"https://jonathan-tw.github.io/posts/2017-nossdav-not-so-quic-a-performance-study-of-dash-over-quic/","summary":"Summary QUICä¸åŠTCPåœ¨DASHçš„è¡¨ç¾ï¼Œç”±æ–¼å¤§å¤šæ•¸ABR algorighméƒ½ç¶“éå„ªåŒ–èˆ‡TCPé…åˆä½¿ç”¨ï¼Œå› æ­¤æ²’æœ‰å……åˆ†åˆ©ç”¨QUICæä¾›çš„åŠŸèƒ½ã€‚ Problem statement What is the impact of QUIC on QoE? How should adaptive bitrate streaming be built to leverage the benefits provided by QUIC? Research objectives A Performance Study and future work Methods we studied QoE performance of different DASH quality adaptation algorithms that are either solely based on the playout buffer filling, or on the video segment download rate, or on both. DASH quality adaptation algorithms that are mainly categorized as","title":"(2017 NOSSDAV) Not so QUIC A Performance Study of DASH over QUIC"},{"content":"Summary åœ¨QUICä¸­ï¼Œé€Ÿåº¦å¿«çš„video streamè¢«æ¯”è¼ƒæ…¢çš„audio streamçµ¦queueä½(sharing a single UDP socket buffer)ï¼Œé€ æˆaudio request response latencyæ‹‰é•·ã€‚å…¶ä¸Šå±¤çš„DASHè¨ˆç®—throughputä¸è€ƒæ…®response latencyï¼Œé€ æˆå¯¦éš›çš„throughputè·Ÿé æœŸçš„DASHä¸åŒã€‚\nProblem statement 2018 (Not so QUIC): focused on buffer-based ABR these recent studies have indicated that buffer-based techniques are aggressive towards video-bitrate maximization whereas suffers in terms of playback smoothness and rebuffering\n2020 (Does QUIC Suit): focused on modern ABR We investigate further to understand the protocol-level behavior of QUIC, which impacts the QoE performance\nABR\u0026rsquo;s important metrics: predicted throughput during video streaming.\nResearch objectives Explore the performance of advanced ABR techniques(MPC,Pensieve)\nMPC-Fast/MPC-Robust -\u0026gt; predictive control Pensieve -\u0026gt; Deep Learning\nMethods (EXPERIMENTAL SETUP) computed 3 QoE metrics\naverage playback bitrate total rebuffering duration playback smoothness a linear representation q(Rn)= Rn,similar to [4], indicating that the playback quality increases linearly with the increase of playback bitrate.\nserver: lsquic,Go-QUIC. client: google chrome (we modify dash.js to add support for advance ABR algorithms like Pensieve and MPC)\nTo emulate realistic traffic behavior:\na Web-based javascript DASH player provided by DASH Industry Foundations (DASHIF) to stream the videos at the client side.\nMahimahi: benchmark traffic shaper(Accurate Record-and-Replay for HTTP)ï¼Œç”¨äºè¨˜éŒ„ä¾†è‡ªåŸºäºHTTPæ‡‰ç”¨ç¨‹åºçš„æµé‡ï¼Œä¸¦é‡å°æ¨¡æ“¬çš„ç¶²è·¯ç’°å¢ƒé€²è¡Œæ¨¡æ“¬ã€‚ Public FCC dataset(æ¨¡æ“¬ç¶²è·¯ä¸­datasetï¼Œcompatible from Mahimahi): a broadband trace from FCC. Evaluation æ¯”è¼ƒQOE Average Video Bitrate: buffer-based ABR mechanisms aggressively use the highest quality levels. Playback Smoothness: A fluctuation in the quality level indicates less smoothness in the video playback, and, therefore, reduces the QoE. Advanced ABR techniques, such as MPC-Fast and Pensieve, provide better playback smoothness with DASH/QUIC, although the supported playback quality is lower compared to DASH/TCP. Rebuffering Time: based on which ABR technique is adopted. æ¯”è¼ƒæ‡‰ç”¨å ´æ™¯ if the response latency is high, this segment may take longer time to reach the client, resulting in a rebuffering\nThe Throughput: the download time = the first and the last bytes received å…©è€…æ™‚é–“å·®. The Response Latency: initiation of the HTTP request and the time when the first byte of the response is received å…©è€…æ™‚é–“å·® ( æ”¶åˆ°çš„ç¬¬ä¸€å€‹ACKçš„æ™‚é–“ - ç™¼èµ·HTTPè«‹æ±‚çš„æ™‚é–“ )\nwhy high response latency observed during the video streaming using QUIC? åœ¨create two parallel HTTP requestsæƒ…æ³ä¸‹ï¼Œgenerate two HTTP ç›¸äº’ä¾è³´çš„streamï¼Œçµæœèˆ‡ä¸Šè¿°å·®ä¸å¤šã€‚ key:socket buffers between HTTP streams.åœ¨QUICä¸­ï¼Œé€Ÿåº¦å¿«çš„streamè¢«æ¯”è¼ƒæ…¢çš„streamçµ¦queueä½äº†ã€‚\nTCP creates two separate sockets for the two HTTP streams, each of the sockets maintains its own socket buffer. (Independent) QUIC multiplexes both the streams and uses a single UDP socket having a single socket buffer, the HTTP responses from both the streams interfere, and higher response rate at one stream affects the queuing delay for the response at the other stream (dependent) åœ¨å‚³çµ±DASHä¸­ï¼Œvideoçš„data rateæ¯”audioæ›´é«˜(video data \u0026gt; audio data)ï¼ŒTCP separate socketä¸­çš„queue depending on their data generation rate.è€Œå°æ–¼QUICä¾†èªªï¼Œæ¯å€‹éœ€è¦æ’­æ”¾çš„segmentéƒ½éœ€è¦clientç™¼ä¸€å€‹http request for videoåŠä¸€å€‹http request for audioï¼Œç”±æ–¼video segemnt requestå…ˆé€ï¼ŒUDP socket bufferè¢«video segementå¡æ»¿ï¼Œaudio segmentå¿…é ˆç­‰video segementåœ¨socket bufferä¸­è¢«é‡‹æ”¾ã€‚\n(a) The audio data has to wait in the queue (the red timeline) before it gets served. (b) The audio streams at QUIC experiences a much higher latency compared to TCP. Audio Stream requestæ™‚é–“è¼ƒä¹… = response latency is higher\nConclusion The QUIC multiplexing of audio and video streams over a single UDP socket results in additional response latency for the audio segments, which are not captured during the calculation of channel throughput. As a consequence, the ABR algorithms take incorrect decisions during selecting the bitrates based on the calculated throughput over a QUIC connection Note â€‹\t#Anything additional that is not included in this outline\nReferences â€‹\t#Relevant articles that might be useful to look at and research on\n","permalink":"https://jonathan-tw.github.io/posts/2020-ieee-does-quic-suit-well-with-modern-adaptive-bitrate-streaming-techniques/","summary":"Summary åœ¨QUICä¸­ï¼Œé€Ÿåº¦å¿«çš„video streamè¢«æ¯”è¼ƒæ…¢çš„audio streamçµ¦queueä½(sharing a single UDP socket buffer)ï¼Œé€ æˆaudio request response latencyæ‹‰é•·ã€‚å…¶ä¸Šå±¤çš„DASHè¨ˆç®—throughputä¸è€ƒæ…®response latencyï¼Œé€ æˆå¯¦éš›çš„throughputè·Ÿ","title":"(2020 IEEE) Does QUIC Suit Well With Modern Adaptive Bitrate Streaming Techniques?"},{"content":"Ref ä»£ç¢¼éš¨æƒ³éŒ„ TSMC \u0026amp; IC Design House(M,R,P)è€ƒå¤ Leetcode Blind 75 CodeTop Leetcode C++ Solutions Behavior questions ç‚ºä»€éº¼æƒ³ä¾†æˆ‘å€‘å…¬å¸ï¼Ÿ ä½ åœ¨xxæœ‰ä»€éº¼ç‰¹åˆ¥çš„è²¢ç»ï¼Ÿ (xx = å¯¦ç¿’å…¬å¸) åœ¨ä½ éå»å·¥ä½œç¶“é©—æˆ–æ±‚å­¸éç¨‹ä¸­ï¼Œä½ èªç‚ºæœ€å¤§çš„å›°é›£/æŒ‘æˆ°æ˜¯?ç‚ºä»€éº¼ï¼Ÿ åœ¨ä½ éå»å·¥ä½œç¶“é©—æˆ–æ±‚å­¸éç¨‹ä¸­ï¼Œä½ èªç‚ºæœ€å¤§çš„æˆå°±/æœæ˜¯?ç‚ºä»€éº¼ï¼Ÿ ä½ äººç”Ÿé‡åˆ°æœ€æŒ«æŠ˜çš„äº‹æƒ…ï¼Ÿä½ æ€éº¼å…‹æœï¼Ÿ èªªèªªä½ çš„å„ªé»å’Œç¼ºé»ï¼Œä¸¦èˆ‰ä¾‹èªªæ˜ã€‚ æœªä¾†çš„è·æ¥­ç”Ÿæ¶¯ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ æœ€çœ‹é‡å·¥ä½œçš„å“ªä¸‰å€‹é»ï¼Ÿ å°æ–¼æˆ‘å€‘å…¬å¸é‚„æœ‰é€™å€‹è·ç¼ºä½ äº†è§£å¤šå°‘ï¼Ÿ ç‚ºä»€éº¼æˆ‘è¦éŒ„å–ä½ ï¼Ÿ å…¬å¸ å»£é” - OpenBMC BU9 Team\nä¸€é¢ D+0: ä¸€å°å¤šé¢è©¦ï¼Œä¸€é–‹å§‹ç‚º25åˆ†é˜Cæ¸¬é©—å«ä¸€é¡Œlink listç™½æ¿é¡Œã€‚æ¥ä¸‹ä¾†è‡ªæˆ‘ä»‹ç´¹åŠè«–æ–‡ä»‹ç´¹ï¼Œç›®å‰openBMCè »å¤šå¤§å¤–å•†éƒ½åœ¨åšï¼Œèƒ½ç·´åˆ°çš„å³æˆ°åŠ›ã€‚ D+5 å£é ­offer getã€‚ é´»æµ·(é´»é‹ç§‘) - é€šè¨Šè»ŸéŸŒé«”ç ”ç™¼å·¥ç¨‹å¸«\nä¸€é¢ D+0: è‡ªæˆ‘ä»‹ç´¹åŠè«–æ–‡ä»‹ç´¹ï¼Œä¸»ç®¡é‡å°è«–æ–‡æå‡ºä¸å°‘å•é¡Œï¼Œä¹Ÿå¾å•é¡Œä¸­ç™¼ç¾ä¸€äº›æœ‰è¶£çš„findingã€‚åƒæ˜¯ä¸åŒtransport protocolå¦‚ä½•é€²è¡Œé€šè¨Šï¼Œeg: server tcp, cliemt quic. D+3 å£é ­offer getã€‚ å°ç© - IT BSID SCPM\né é¢ 3é¡Œhackerrankæ¸¬é©—1.5å° D+0: é›£åº¦EMMï¼Œä¸€é¡Œbfs/dfsè®Šå½¢(1254. Number of Closed Islands)ï¼Œä¸€é¡Œå¼“ç®­å°„æ°£çƒè®Šå½¢(452. Minimum Number of Arrows to Burst Balloons)ã€‚æœ€å¾Œä¸€é¡Œæœ‰5å€‹æ¸¬è³‡TLEï¼Œ2.8é¡Œã€‚ ä¸€é¢ D+0: ä¸»ç®¡é¢è©¦ï¼Œè‡ªæˆ‘ä»‹ç´¹åŠè«–æ–‡ä»‹ç´¹ï¼Œä¸»ç®¡é‡å°éƒ¨é–€çš„å·¥ä½œå…§å®¹é€²è¡Œè©³ç´°çš„ä»‹ç´¹ã€‚ä¸»ç®¡èªªé€™éƒ¨é–€ç®—æ˜¯Béƒ¨å£“åŠ›æ¯”è¼ƒå¤§çš„ï¼Œä½†å­¸å¾—åˆ°æ±è¥¿ï¼Œä¸å¤ªéœ€è¦oncallã€‚ äºŒé¢ D+7: ä¸»ç®¡é¢è©¦ï¼ŒæŠ€è¡“é¢è©¦(javascript, åŸºæœ¬coding)ã€‚é€™éƒ¨åˆ†è¦ºå¾—ç•¶å¤©ç­”å¾—ä¸å¤ªå¥½ã€‚ ä¸‰é¢ D+14: äººè³‡é¢è©¦ åˆ°å ´æ¸¬é©— D+15: äººæ ¼æ¸¬é©— å£é ­offer get D+28: æ ¸è–ª offer get D+42: å› ç‚ºç°½ç´„é‡‘çš„å•é¡Œï¼Œæ‰€ä»¥å¤šèŠ±äº†ä¸€äº›æ™‚é–“èˆ‡HRå•†è¨(å°ç©åªæœ‰é è˜æœ‰ç°½ç´„é‡‘ï¼Œitåˆæœ‰30wï¼Œå¯æƒœ)ã€‚\nå°é”é›» - é›»å‹•è»Šå……é›»æ¨\nä¸€é¢ D+0: èˆ‡ä¸€ä½ä¸»ç®¡å’Œä¸€ä½åŒäº‹é€²è¡Œé¢è©¦ï¼Œå–œæ­¡ä¸»ç®¡æ•´é«”å¾…äººè™•äº‹çš„æ°›åœã€‚ D+7: HRæ ¸è–ªï¼ŒåŸæœ¬ä»¥ç‚ºæœƒè·Ÿå»£é”å·®ä¸å¤šï¼Œçµæœæ¯”æƒ³åƒä¸­é«˜ä¸å°‘ã€‚å°é”é›»æ˜¯ç³»çµ±å ´éœ¸ä¸»æ‡‰è©²æ˜¯çœŸçš„ã€‚ offer get D+7: é›»å‹•è»Šå¸‚å ´ç¾åœ¨å¾ˆç«ï¼Œå„å€‹å» å•†ç£¨æ‹³æ“¦æŒåœ¨è¿½specï¼Œæ‰€ä»¥é«˜å·¥æ™‚é«˜æµå‹•ç‡åœ¨æ‰€é›£å…ã€‚\nLenovo - CSP Firmware Engineer\nä¸€é¢ D+0: èˆ‡å…©ä½åŒäº‹é€²è¡Œé¢è©¦ï¼Œå‰åŠè‡ªæˆ‘ä»‹ç´¹ï¼Œå¾Œé¢è€ƒCï¼Œä¸é›£ã€‚ äºŒé¢ D+5: èˆ‡å…©ä½ä¸»ç®¡é€²è¡Œé¢è©¦ï¼Œä¸€ä½ä¾†è‡ªã€ŒåŒ—äº¬ã€çš„ä¸»ç®¡å°è±¡æ·±åˆ»(å‹å–„åœ‹äºº)ï¼Œä»–æ¯”èµ·å°ç£è¯æƒ³çš„ä¸»ç®¡å•äº†æ›´å¤šã€ŒæŠ€è¡“ã€å•é¡Œï¼Œç‰¹åˆ¥æ˜¯OSã€‚ç”±æ–¼ä¸€äº›æŠ€è¡“åè©åœ¨å…©å²¸ç”¨æ³•ä¸Šä¸å¤ªä¸€æ¨£ï¼Œæ‰€ä»¥è¦å†ä¸‰ç¢ºå®šå•é¡Œã€‚ç•¶å¤©å‚æ™šHRæ ¸è–ªã€‚ offer get D+5: å°ç£è¯æƒ³æ‹›å‹Ÿæ•ˆç‡å¾ˆå¥½ï¼Œé¢è©¦éç¨‹ä¸æ‹–æ³¥å¸¶æ°´ã€‚ä¸»ç®¡èªªå°ç£è¯æƒ³æ”¶è³¼äº†ä¹‹å‰çš„IBMï¼Œæ‰€ä»¥è »å¤šå¾IBMé‚£è·³ä¾†çš„äººï¼Œæ•´é«”å…¬å¸ç®¡ç†é¢¨æ ¼èˆ‡ç¾å•†ç›¸è¿‘ã€‚å› ç‚ºç®—å¤–å•†ï¼Œä¸€ã€äºŒé¢è©¦æ™‚éƒ½æœ‰å«æˆ‘è‹±æ–‡å£èªªï¼Œç•¶æ™‚æ²’æº–å‚™ï¼Œè‡ªå·±è¦ºå¾—è¬›è »çˆ›çš„ï¼Œä½†ä¸»ç®¡èˆ‡åŒäº‹éƒ½åå‘æ­£é¢å›é¥‹ï¼Œä¸æœƒåˆé›£ã€‚\nMOXA(404) - WiFi application Engineer\nä¸€é¢ D+0: æ•´é«”é¢è©¦éç¨‹ä¸­ä¸»ç®¡æ…‹åº¦å‹å¥½ï¼Œè »é¦™çš„ç³»çµ±å» ã€‚é–‹ç™¼ç”¢å“çš„é€±æœŸè¼ƒé•·ï¼Œç›¸å°é–‹ç™¼å“è³ªèˆ‡æ•ˆç‡ä¹Ÿè¼ƒå¥½ã€‚ä¸€å€‹ç¦®æ‹œæœ‰å…©å¤©å¯ä»¥WFHï¼Œã€Œæº–æ™‚ã€ä¸‹ç­ï¼Œå…¬å¸ä¸é¼“å‹µåŠ ç­æ–‡åŒ–ã€‚ äºŒé¢ codingæ¸¬é©— D+4: 5é¡Œ25åˆ†é˜ï¼ŒåŸºæœ¬ä¸Šéƒ½è€ƒCçš„è§€å¿µï¼Œå•è¼¸å…¥è¼¸å‡ºåŠä¸€é¡Œrecursiveã€‚ D+7: æ”¶åˆ°å¯¦é«”é¢è©¦é€šçŸ¥ï¼Œä½†å·²æ”¶åˆ°å…¶ä»–å®¶çš„offerï¼Œæ•…å©‰æ‹’å¾ŒçºŒé¢è©¦ã€‚ å¨æ—­è³‡è¨Š - Infra Team Software Engineer\né é¢ D+0: ä¸€é¡Œç¨‹å¼é¡Œhwï¼Œé›£åº¦easy-mediumä¹‹é–“ã€‚ D+6: æ”¶åˆ°æŠ€è¡“å¯¦é«”é¢è©¦é€šçŸ¥ï¼Œä½†å·²æ”¶åˆ°å…¶ä»–å®¶çš„offerï¼Œæ•…å©‰æ‹’å¾ŒçºŒé¢è©¦ã€‚ ","permalink":"https://jonathan-tw.github.io/posts/2023-%E8%BB%9F%E9%9F%8C%E6%96%B0%E9%AE%AE%E4%BA%BA-%E9%9D%A2%E8%A9%A6%E6%99%82%E7%A8%8B/","summary":"Ref ä»£ç¢¼éš¨æƒ³éŒ„ TSMC \u0026amp; IC Design House(M,R,P)è€ƒå¤ Leetcode Blind 75 CodeTop Leetcode C++ Solutions Behavior questions ç‚ºä»€éº¼æƒ³ä¾†æˆ‘å€‘å…¬å¸ï¼Ÿ ä½ åœ¨xxæœ‰ä»€éº¼ç‰¹åˆ¥çš„è²¢ç»ï¼Ÿ (xx = å¯¦ç¿’å…¬å¸) åœ¨ä½ éå»å·¥ä½œç¶“é©—æˆ–æ±‚å­¸éç¨‹ä¸­ï¼Œä½ èªç‚ºæœ€å¤§çš„å›°é›£/æŒ‘æˆ°æ˜¯?ç‚ºä»€éº¼ï¼Ÿ åœ¨ä½ éå»å·¥ä½œç¶“é©—æˆ–æ±‚å­¸éç¨‹ä¸­ï¼Œä½ èªç‚ºæœ€å¤§çš„æˆå°±/æœæ˜¯?ç‚ºä»€éº¼ï¼Ÿ ä½ äººç”Ÿé‡åˆ°æœ€æŒ«æŠ˜çš„äº‹æƒ…ï¼Ÿä½ æ€","title":"2023 è»ŸéŸŒæ–°é®®äºº é¢è©¦æ™‚ç¨‹"},{"content":"CH1 - åŸºç¤ 1.1 1 2 int x, y z; y = z = 4 // y = 4 and z= 4 1.2 \u0026amp; é‹ç®— =\u0026gt; åšäºŒé€²ä½andé‹ç®— | é‹ç®— =\u0026gt; åšäºŒé€²ä½oré‹ç®— \u0026amp;\u0026amp; å’Œ || çš†ç‚ºåˆ¤æ–·å¼ =\u0026gt; å›å‚³1æˆ–è€…æ˜¯0 1.3 1 x = (y == z) ? 4 : 5; y =4, z = 2, (å•è™Ÿå‰é¢çš„åˆ¤æ–·å¼) false, xå–å¾Œè€…, x = 5 if (true) ? x = 4 if (false) ? x = 5\n1.4 1 ::value = 2; åœ¨Cä¸­ç„¡æ³•ç·¨è­¯é€šéï¼ŒC++å¯ã€‚(æ”¹è®Šå…¨å±€è®Šæ•¸å€¼)\n1.5 i++/++i å€åˆ¥ 1 2 3 int i = 8; printf(\u0026#34;%d\\n\u0026#34;, ++i); // 9 printf(\u0026#34;%d\\n\u0026#34;, i--); // 9, å…ˆå°9, i = 8 æ•ˆç‡æ²’æœ‰å€åˆ¥, è‡ªå®šç¾©++iè¼ƒä½³\n1.7 ä¸ä½¿ç”¨ä¸­é–“è®Šé‡å°‡a,bå€¼é€²è¡Œäº¤æ› 1 2 3 4 5 6 7 8 9 10 11 12 13 void swap2( int a, int b ) { a = a + b; b = a - b; a = a - b; } // ä½¿ç”¨XORå®Œæˆäº¤æ› 1xor1 = 0, 1xor0 = 1. void swap3( int a, int b ) { a = a ^ b; b = a ^ b; a = a ^ b; } 1.8 externçš„ä½¿ç”¨ å°æ–¼externè®Šé‡ä¾†èªªï¼Œåƒ…åƒ…æ˜¯ä¸€å€‹è®Šé‡çš„è²æ˜ï¼Œå…¶ä¸¦ä¸æ˜¯åœ¨å®šç¾©åˆ†é…å…§å­˜ç©ºé–“ã€‚å¦‚æœè©²è®Šé‡å®šç¾©å¤šæ¬¡ï¼Œæœƒæœ‰é€£æ¥éŒ¯èª¤ã€‚å…¶è²æ˜çš„å‡½æ•¸æˆ–è®Šé‡å¯ä»¥åœ¨å…¶ä»–moduleä¸­ä½¿ç”¨ã€‚\n1.9 main()åŸ·è¡Œå®Œé‚„åŸ·è¡Œå…¶ä»–èªå¥å— ä½¿ç”¨atexit(function_name), è¨»å†Šç¨‹å¼çµ‚æ­¢è¦è¢«èª¿ç”¨çš„å‡½æ•¸ã€‚\nCH2 - macro, inline, const, static, sizeof 2.2 ç”¨#defineå®šç¾©æœ€å¤§å’Œæœ€å°å€¼ 1 2 #define MAX(x,y) ( ((x) \u0026gt; (y)) ? (x) : (y) ) #define MIN(x,y) ( ((x) \u0026lt; (y)) ? (x) : (y) ) define: compileæ™‚ä»£å…¥ (æ²’æ•¸æ“šé¡å‹) const: run-timeæ™‚ä»£å…¥ (æœ‰æ•¸æ“šé¡å‹ï¼Œé€²è¡Œå®‰å…¨æª¢æŸ¥)\nmacro èˆ‡ inline ç›¸åŒä¹‹è™•ï¼šæ˜¯æŠŠå‘¼å«çš„ä½å€æ›æˆå‡½å¼ä¸»é«”ï¼Œé€™æ¨£åŸ·è¡Œé€Ÿåº¦è¼ƒå¿«ï¼Œå¤šåŠç”¨æ–¼éœ€è¦å¤šæ¬¡å‘¼å«çš„ç¨‹å¼ã€‚ ç›¸ç•°ä¹‹è™•ï¼šmacro æ˜¯pre-compileï¼Œæœƒåœ¨ç¨‹å¼ç·¨è­¯æˆç‚ºæ©Ÿå™¨èªè¨€ä¹‹å‰å…ˆç·¨è­¯å®Œæˆï¼Œæ˜¯ä¸€ç¨®æ–‡å­—æ›¿æ›çš„æ¦‚å¿µã€‚è€Œinlineå°±åƒä¸€èˆ¬å‡½å¼ä¸€æ¨£ï¼Œæ˜¯compileræ±ºå®šçš„ã€‚\n1 2 3 4 5 6 7 8 // macro example #define SIZE 1024 // å®šç¾©å¸¸æ•¸ #define SQUARE(x) ((x) * (x)) // å®šç¾©å‡½å¼ //inline example inline int square(int x){ return x*x; } macro èˆ‡ inline éƒ½æ˜¯ç”¨ç©ºé–“æ›å–æ™‚é–“\nfunction å‰‡æ˜¯ç”¨æ™‚é–“æ›å–ç©ºé–“\n2.7 constçš„ä½¿ç”¨ #2-12\nconstå®šç¾©å¸¸é‡ (åªè®€ä¸å¯«)ï¼Œå…·æœ‰ä¸å¯è®Šæ€§ã€‚ å°é¡å‹é€²è¡Œå®‰å…¨æª¢æŸ¥ã€‚ ä¸èƒ½å¼•ç”¨(ï¼†)constå€¼ã€‚ å‚³éåƒæ•¸åˆ°å‡½å¼ä¸­ï¼Œä½†åˆä¸å¸Œæœ›é¡å¤–æ–°å¢åƒæ•¸ã€‚ void fun(A a); void fun(A const \u0026amp; a); å¼•ç”¨aä½†åŠ constä¿æŒä¸è®Šæ€§ 1 2 3 4 5 int b = 10; const int * a1 = \u0026amp;b; // ä¸å…è¨±ä¿®æ”¹å€¼(ä¿®é£¾int)ï¼Œä½†æŒ‡å‘ä½ç½®å¯ä»¥ä¿®æ”¹ int const * a1 = \u0026amp;b; int * const a2 = \u0026amp;b; // ä¸å…è¨±ä¿®æ”¹ä½ç½®(ä¿®é£¾æŒ‡æ¨™)ï¼Œä½†å€¼å¯ä»¥ä¿®æ”¹ const int * const a3 = \u0026amp;b; // çš†ä¸èƒ½ä¿®æ”¹ 2.10 staticçš„ä½¿ç”¨ staticåœ¨é€™functionå‘¼å«çµæŸæ™‚ï¼Œå€¼ç¶­æŒä¸è®Š(ä¸æœƒå› ç‚ºfunctionçµæŸè€Œæ¶ˆå¤±ï¼Œå…·æœ‰ç¹¼æ‰¿æ€§)ã€‚ staticåœ¨ç•¶å…¨åŸŸè®Šæ•¸æ™‚ï¼Œå¯ä»¥è¢«æ‰€æœ‰å‡½æ•¸è¨ªå•ï¼Œä½†ä¸èƒ½è¢«è©²æª”æ¡ˆå¤–çš„å…¶ä»–å‡½æ•¸è¨ªå•ï¼Œæ˜¯ä¸€å€‹æœ¬åœ°çš„å…¨å±€è®Šé‡ã€‚ static functionä¹Ÿåªå¯è¢«è©²æª”æ¡ˆå…§çš„å…¶ä»–å‡½æ•¸èª¿ç”¨ï¼Œä¸èƒ½è¢«æ¨¡å¡Šå¤–çš„å…¶ä»–å‡½æ•¸è¨ªå•ã€‚ staticè®Šæ•¸åœ¨classä¸­ä¹Ÿå…·æœ‰å…±äº«æ€§ï¼Œå³å¤šå€‹objå…±äº«ä¸€å€‹staticåƒæ•¸ã€‚ static functionåœ¨classä¸­ä¹Ÿå…·æœ‰å…±äº«æ€§ï¼Œå³ä¾¿æˆ‘å€‘æ²’æœ‰ç”¢ç”Ÿ instance å‡ºä¾†ï¼Œæˆ‘å€‘ä¹Ÿéš¨æ™‚å¯ä»¥å–ç”¨é€™å€‹ functionã€‚ 1 2 3 staticå±€éƒ¨è®Šæ•¸ vs æ™®é€šå±€éƒ¨è®Šæ•¸: staticåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œä¸‹ä¸€æ¬¡ä¾æ“šä¸Šä¸€æ¬¡çµæœå€¼ staticå…¨å±€è®Šæ•¸ vs æ™®é€šå…¨å±€è®Šæ•¸: staticåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé˜²æ­¢åœ¨å…¶ä»–æ–‡ä»¶ä¸­è¢«å¼•ç”¨ static function vs æ™®é€šfunction: static functionåœ¨å…§å­˜ä¸­åªæœ‰ä¸€ä»½ï¼Œæ™®é€šfunctionæ¯å€‹è¢«èª¿ç”¨éƒ½ç¶­æŒä¸€ä»½è¤‡è£½å“ 2.13 ä½¿ç”¨sizeofè¨ˆç®—è®Šæ•¸æ‰€å çš„byteså¤§å° char str[] = \u0026ldquo;hello\u0026rdquo;; char * p= str; int n = 10; sizeof(str) = 5 + 1 (\u0026rsquo;\\0\u0026rsquo;) = 6; sizeof(p) = 4; sizeof(n) = 4; 32ä½winntå¹³å°ä¸‹ï¼ŒæŒ‡é‡å’Œintéƒ½ç‚º4 bytes\nclassä¸­çš„functionä¸å å…§å­˜ï¼Œä½†virtual functionä½”ç”¨ä¸€å€‹pointerçš„å¤§å° = 4 bytesã€‚ intå’Œã€ŒæŒ‡é‡ã€å 4 bytesï¼Œcharå 1 bytesï¼Œdoubleå 8 bytesã€‚ ç¹¼æ‰¿çš„å‡½æ•¸ä¹Ÿè¦å¦å¤–çš„ç©ºé–“ã€‚ staticåœ¨functionä¸­ï¼Œä¸ç®—ç©ºé–“ã€‚ unionçš„å¤§å°å–æ±ºæ–¼ä»–æ‰€æœ‰çš„æˆå“¡ä¸­ä½”ç”¨ç©ºé–“æœ€å¤§ä¸€å€‹æˆå“¡çš„å¤§å°ã€‚åŒé¡å‹å–æœ€é«˜ï¼Œä¸åŒé¡å‹éœ€å°é½Šã€‚ CH3 - pointer 1 2 \u0026amp; = get value address (call by reference) * = å–å‡ºpointerä¹‹value (å–å‡ºaddressæŒ‡å‘çš„value) 3.1 å¼•ç”¨ \u0026amp; 1 2 3 4 5 int a = 10; int b = 20; int \u0026amp;rb = a; // å¼•ç”¨åªèƒ½åœ¨è²æ˜çš„æ™‚å€™è³¦å€¼ rb = b; cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl; // a = 20 å¼•ç”¨vs pointer: å¼•ç”¨åˆå§‹å®Œå¾Œä¸èƒ½è¢«æ”¹è®Šï¼Œpointerå¯ä»¥éš¨æ™‚æŒ‡å‘åˆ¥çš„å°è±¡ã€‚ 3.10 ç”¨pointerè³¦å€¼ 1 2 3 4 5 6 char a[] = \u0026#34;hello world\u0026#34;; char * ptr = a; printf( \u0026#34;%c\\n\u0026#34;, *(ptr+4) ); // o printf( \u0026#34;%c\\n\u0026#34;, ptr[4] ); // o printf( \u0026#34;%c\\n\u0026#34;, a[4] ); // o printf( \u0026#34;%c\\n\u0026#34;, *(a+4) ); // o 3.12 Pointeræ¯”è¼ƒ 1 2 3 4 5 6 7 8 9 10 char str1[] = \u0026#34;abc\u0026#34;; // str1 != str2 è¨˜æ†¶é«”ä½ç½®ä¸ä¸€æ¨£ char str2[] = \u0026#34;abc\u0026#34;; char * str7 = \u0026#34;abc\u0026#34;; // str7 == str8 è¨˜æ†¶é«”ä½ç½®ä¸€æ¨£ char * str8 = \u0026#34;abc\u0026#34;; char * str3 = \u0026#34;AAA\u0026#34;; str3[0] = \u0026#39;B\u0026#39;; // ä¸åˆæ³•ï¼Œstr3æŒ‡å‘å¸¸é‡ï¼Œä¸èƒ½æ“éŒ¯ã€‚pointerä¸èƒ½ç›´æ¥è³¦å€¼ï¼Œè¦åˆå§‹åŒ–ã€‚ int * a[10]; // æŒ‡é‡æ•¸çµ„ï¼Œæ•¸çµ„ä¸­æ¯ä¸€å€‹å…ƒç´ éƒ½æ˜¯æŒ‡é‡ int * a = new int[10]; // ä¸€å€‹æŒ‡é‡ï¼ŒæŒ‡å‘ä¸€å€‹æ•¸çµ„ã€‚ 3.21 æŒ‡é‡æ•¸çµ„æ‰¾éŒ¯ 1 2 3 4 5 6 char * str[] = { \u0026#34;Weclone\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;Fortemedia\u0026#34;, \u0026#34;Nanjing\u0026#34; }; // A,B,C,D char **p = str + 1; // pç§»å‹•æŒ‡å‘B str[0] = (*p++) + 2; // pç§»å‹•æŒ‡å‘Cï¼Œstr[0] = ç©º str[1] = *(p+1); // pä¸ç§»å‹•ï¼Œstr[1] = D str[2] = p[1] + 3; // æŒ‡å‘Då¾€å¾Œ3å€‹å…ƒç´ ï¼ŒJing str[3] = p[0] + ( str[2] - str[1] ); 3.25 å‡½æ•¸æŒ‡é‡ 1 2 3 4 5 6 7 int max( int x, int y ) { return x \u0026gt; y ? x : y; } int (*p) (int,int); int max( int, int ); p = \u0026amp;max; 1 2 3 4 5 6 7 8 9 int main() { int (*op[2]) (int a, int b); op[0] = add1; op[1] = add2; cout \u0026lt;\u0026lt; op[0]( 0, 0 ) \u0026lt;\u0026lt; op[1]( 0, 0 ); } int add1( int a1, int b1 ) { return a1+b1 }; int add2( int a2, int b2 ) { return a2+b2 }; 3.30 æœ‰äº†malloc/freeï¼Œé‚„éœ€è¦new/delete malloc/freeæ˜¯æ¨™æº–åº«å‡½æ•¸ï¼Œç„¡æ³•åŸ·è¡Œconstructorèˆ‡destructor\n1 2 3 4 5 6 char * name = \u0026#34;test\u0026#34;; char * coptyName() { char * newname = new char[strlen(name) + 1]; // å­—ç¬¦ä¸²å·²0ä½œç‚ºçµæŸç¬¦ã€‚ strcpy( newname, name ); return newname; } 3.35 æ£§å…§å­˜ (Stack) ï¼† å †å…§å­˜(Heap)\nstack: å‘¼å«å‡½æ•¸çš„å±€éƒ¨è®Šæ•¸åœ¨æ­¤å„²å­˜ï¼Œå‡½æ•¸çµæŸæ™‚è‡ªå‹•é‡‹æ”¾ã€‚ heap: new/mallocåœ¨æ­¤çš„å‹•æ…‹å…§å­˜åˆ†é…ã€‚ CH4 - String [[8 - String to Integer (atoi)]]\n4-5 strcpyå¯¦ä½œ 1 2 3 4 5 6 7 8 9 10 11 12 13 char * DIY_strcpy( char * strDest, char * strSrc ) { if ( strDest == NULL || strSrc == NULL ) return NULL; char * strDestStart = strDest; while ( *strSrc != \u0026#39;\\0\u0026#39; ) { *strDest = *strSrc; strSrc++; strDest++; } *strDest = \u0026#39;\\0\u0026#39;; return strDestStart; } 4-6 memset vs memcpy vs memcmp memset è¤‡è£½å­—ç¬¦ cï¼ˆä¸€å€‹ç„¡ç¬¦è™Ÿå­—ç¬¦ï¼‰åˆ°åƒæ•¸ str æ‰€æŒ‡å‘çš„å­—ç¬¦ä¸²çš„å‰ n å€‹å­—ç¬¦ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main () { char str[50]; strcpy(str,\u0026#34;This is string.h library function\u0026#34;); puts(str); memset(str,\u0026#39;$\u0026#39;,7); puts(str); return(0); } This is string.h library function qqqqqq string.h library function\nmemcpy vs strcpy memcpyå¯è¤‡è£½å…¶ä»–é¡å‹çš„æ•¸æ“šï¼Œstrcpyå‰‡åªç”¨ä¾†è¤‡è£½å­—ç¬¦ä¸²(\u0026rsquo;\\0\u0026rsquo;)è€ŒçµæŸã€‚\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; void main() { char *s=\u0026#34;pannyloveworld\u0026#34;; char s1[10]; s1[9]=\u0026#39;\\0\u0026#39;; memcpy(s1,s+5,9); // (dest, start_index, é•·åº¦) printf(\u0026#34;%s\u0026#34;,s1); } Output: loveworld\nmemcmp vs strcmp çš†ç‚ºæ¯”è¼ƒå­—ä¸²çš„å¤§å°ï¼Œmemcmpç‚ºæ¯”è¼ƒå…§å­˜å¡Š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main () { char str1[15]; char str2[15]; int ret; memcpy(str1, \u0026#34;abcdef\u0026#34;, 6); memcpy(str2, \u0026#34;ABCDEF\u0026#34;, 6); ret = memcmp(str1, str2, 5); // æ¯”è¼ƒstr1å’Œstr2å‰nå€‹å­—ç¯€AIICç¢¼å€¼çš„å¤§å°ï¼› ret \u0026gt; 0 = str1 \u0026gt; str2, ret \u0026lt; 0 = str1 \u0026lt; str2 // å¤§å¯«asciiå€¼è¼ƒå° if (ret \u0026gt; 0) printf(\u0026#34;str1 is big than str2\u0026#34;); else if (ret \u0026lt; 0) printf(\u0026#34;str2 is big than str1\u0026#34;); else printf(\u0026#34;str1 is equal to str2\u0026#34;); return(0); } Output: str1 is less than str2\n4-11 è¨ˆç®—å­—ä¸²é•·åº¦ 1 2 3 4 5 int strlen2( char * src ) { char * temp = src; while ( *src++ != \u0026#39;\\0\u0026#39; ); return ( src - temp - 1 ); } 4-18 aaabb = aaa3b2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;string.h\u0026gt; void transformation() { char str[200]; scanf( \u0026#34;%s\u0026#34;, str ); char * p = str; char * q = str + 1; int count = 1; int len = strlen(str); char * buf = (char*)malloc(len+1); while ( *q != \u0026#39;\\0\u0026#39; ) { if ( *q == *p ) { q++; p++; count++; } else { sprintf( buf, \u0026#34;%d\u0026#34;, count ); strcat( buf, q ); *q = \u0026#39;\\0\u0026#39;; strcat( str, buf ); q++; p = q; q = p + 1; count = 1; } } sprintf( buf, \u0026#34;%d\u0026#34;, count ); strcat( str, buf ); printf( \u0026#34;%s\\n\u0026#34;, str ); } å­—ä¸²è¨˜å¾—åˆå§‹åŒ–\n1 2 3 char * str3 = NULL; str3 = new char[strlen(str1)+strlen(str2)+1] str3[0] = \u0026#39;\\0\u0026#39;; // è¨˜å¾—newå®Œçš„å­—ä¸²è¦åˆå§‹åŒ–ã€‚ 4-19 å¯¦ä½œstrcat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;string.h\u0026gt; // strcat(åŸæœ¬ï¼Œè¿½åŠ ) char * mystrcat( char * str1, char * str2 ) { char * dest = str1; while ( *str1 != \u0026#39;\\0\u0026#39; ) str1++; while ( *str2 != \u0026#39;\\0\u0026#39; ) { *str1 = *str2; str1++; str2++; } *str1 = \u0026#39;\\0\u0026#39;; return dest; } char * mystrcat2( char str1[], char str2[] ) { char * dest = str1; int i = 0; while ( str1[i] != \u0026#39;\\0\u0026#39; ) i++; int j = 0; while ( str2[j] != \u0026#39;\\0\u0026#39; ) { str1[i] = str2[j]; i++; j++; } str1[i] = \u0026#39;\\0\u0026#39;; return dest; } int main() { char * dest = NULL; char str1[40] = \u0026#34;Hello \u0026#34;; char str2[40] = \u0026#34;World!\u0026#34;; char * str3 = \u0026#34;Hello \u0026#34;; char * str4 = \u0026#34;World!\u0026#34;; dest = (char*)malloc(256); *dest = \u0026#39;\\0\u0026#39;; // dest = mystrcat2( str1, str2 ); dest = mystrcat( mystrcat( dest, str3 ) , str4 ); printf( \u0026#34;dest: %s\\n\u0026#34;, dest ); } CH5 - Bit operation 5-3 set/remove ç‰¹å®šbit bit 3 = 2çš„3æ¬¡æ–¹ = å¾å³æ•¸ä¾†ç¬¬å››ä½ã€‚\n1 2 3 4 5 int main() { int i = 20, j = 28; i = i | ( 1 \u0026lt;\u0026lt; 3 ); // i = 28 (3 = n) j = j \u0026amp; ~( 1 \u0026lt;\u0026lt; 3 ); // j = 20 (3 = n) } 5-4 è¨ˆç®—ä¸€å€‹charæœ‰å¤šå°‘bitè¢«è¨­ç½®1 1 2 3 4 5 6 7 8 9 int main( ) { int a = 63, count = 0; while ( a ) { a = a \u0026amp; a - 1; count++; } printf( \u0026#34;%d\\n\u0026#34;, count ); } 5-8 ç”¨defineè²æ˜ä¸€å€‹å¸¸æ•¸ 1 #define SECONDS_PER_YEAR ( 60 * 60 * 24 \u0026amp; 365) UL(ç„¡è™Ÿå¸¸æ•´æ•¸) ","permalink":"https://jonathan-tw.github.io/posts/c_c++-%E9%9D%A2%E8%A9%A6%E7%A7%98%E7%AC%88/","summary":"CH1 - åŸºç¤ 1.1 1 2 int x, y z; y = z = 4 // y = 4 and z= 4 1.2 \u0026amp; é‹ç®— =\u0026gt; åšäºŒé€²ä½andé‹ç®— | é‹ç®— =\u0026gt; åšäºŒé€²ä½oré‹ç®— \u0026amp;\u0026amp; å’Œ || çš†ç‚ºåˆ¤æ–·å¼ =\u0026gt; å›å‚³1æˆ–è€…æ˜¯0 1.3 1 x = (y == z) ? 4 : 5; y =4, z = 2, (å•è™Ÿå‰é¢çš„åˆ¤æ–·å¼) false, xå–å¾Œè€…, x = 5 if (true) ? x = 4 if (false) ? x = 5 1.4 1 ::value = 2; åœ¨Cä¸­ç„¡æ³•ç·¨è­¯é€šéï¼ŒC++å¯ã€‚(æ”¹è®Šå…¨å±€è®Šæ•¸å€¼","title":"C/C++ é¢è©¦é¡Œ"},{"content":"Lecture 2 - Asynchronous I/O Programming Models\nConcurrency (Asynchronous I/O = cooperative multitasking) Multiple tasks have the ability to run in an overlapping manner Concurrency does not imply parallelism! Multiprocessing CPU-bounded tasks Multithreading I/O bound tasks It uses preemtive multitasking Promise object:\nan async object that be returned by the async function. An object representing completion or failure of an asynchronous operation. Await function:\nmakes program to wait until the promise is resolved or rejected can only be usded inside async function Lecture 3 - Cloud Architectures Cloud Computing Concepts On-demand and self-service (ç•¶éœ€è¦è³‡æºæ™‚æ‰è¢«æä¾›,è‡ªå‹•åŒ–) Resources are provisioned as they are requested and when they are required â€’ No human interaction, automatic No human interaction, automatic Board network access (è³‡æºå¯è¢«ç¶²è·¯å–å¾—) Capabilities are available over the network Resource pooling (è³‡æºæ ¹æ“šéœ€æ±‚å†æä¾›,ç¡¬é«”é…ç½®è¢«mutiple tenantå…±ç”¨) Resourcces are dynamically assigned/re-assigned according to demand Provider\u0026rsquo;s computing resources reused by multiple tenants Computing resources: CPU, memory, storage, network Scalability and elasticity (è³‡æºæ ¹æ“šéœ€æ±‚æ“´å¤§æˆ–ç¸®æ¸›) Infrastructure may grow and shrink according to needs Automatic or manual Measured service (è³‡æºå¯è¢«æ§ç®¡) Resource usage can be monitored, controlled and reported Pay-per-use (ç•¶æ¶ˆè²»è€…ä½¿ç”¨è³‡æºæ™‚æ‰ä»˜è²») Consumers only pay for resources when they use them Multitenancy: Architectural approach where resources are shared between multiple tenants or consumers Infrastructure as Code: Version control, team development, scripting, etc.\nTerraform:\nHigher-level abstraction of the datacenter and associated services Supports many service providers: Google, Microsoft, Oracle, AWS VCN = a private network in a single region in which your instances reside\nLecture 4 - Cloud Native and Microservices Microservices: applications as independenly deployable services\nLoosely coupled: Integrated using well-defined interfaces Technology-agnostic protocols: HTTP, they use REST architecture Independently deployable and easy to replace: A change in small part requires to redeploy only that part Implemented using different technologies: PL, databases Container Dockerfile is a script that creates a new image\nA line in the Dockerfile will create an intermediary layer\ndocker build -t tomvit/httpd:v1 If processing fails at some step, all preceeding steps will be loaded from the cache on the next run.\nNetworking and Linking:\nbridge â€“ container can access host\u0026rsquo;s network (default) host â€“ all host\u0026rsquo;s network interfaces will be available in the container none â€“ container will be placed on its own network and no network interfaces will be configured. Data Volume:\nA directory that bypass the union file system Data volumes can be shared and reused among containers Data volume persists even if the container is deleted It is possible to mount a shared sotrage volume as a data volue by using a volume plugin to mount e.g. NFS Lecture 5 - Browser Networking Network security\nConnection limits Request formatting and response processing TLS negotiation Same-origin policy XMLHttpRequest (XHR) basis for AJAX â†’ Asynchronous JavaScript and XML Security Scripting Attacks CSRF(Cross-site request forgery) åˆ©ç”¨SessionæœªéæœŸçš„ç‰¹æ€§ï¼Œå†’æ²–ä½¿ç”¨è€…èº«ä»½ä¾†åšæƒ¡æ„æ”»æ“Š\nCSRFçš„æ”»æ“Šæµç¨‹æ˜¯åˆ©ç”¨ä½¿ç”¨è€…å‰›ä½¿ç”¨å®ŒæŸæœå‹™å¾Œä¸ä¹…SessionæœªéæœŸçš„æ™‚é–“å…§ï¼Œèª˜å°ä½¿ç”¨è€…é»æ“Šæƒ¡æ„é€£çµä¾†å†’å……ä½¿ç”¨è€…çš„èº«ä»½ç™¼é€éæœ¬æ„çš„è«‹æ±‚ï¼Œä¾‹å¦‚ä½¿ç”¨è€…å‰›ç™»å…¥ç¶²éŠ€æ²’å¤šä¹…å¾Œæ”¶åˆ°ä¸€å€‹é€£çµï¼Œé»é–‹ä¾†å¾Œæ‰ç™¼ç¾æ˜¯ä¸€å€‹è½‰å¸³çš„apiå°‡ä½¿ç”¨è€…çš„æˆ¶é ­çš„éŒ¢è½‰åˆ°æŒ‡å®šæˆ¶é ­å»ã€‚èˆ‰ä¾‹ä¾†èªªï¼Œèª˜å°ä½¿ç”¨è€…æ‹œè¨ªæƒ¡æ„ç¶²ç«™ï¼Œç„¶å¾Œåœ¨ç¶²ç«™å…§çš„img srcå¡æœ‰CSRF æ¼æ´çš„apiï¼Œåªè¦Sessionæ²’éæœŸï¼Œå°±å¯ä»¥æˆåŠŸæ”»æ“Šï¼Œç•¶ç„¶é‚£æ˜¯api æ˜¯GETè«‹æ±‚ç‹€æ³ã€‚ é é˜²æ–¹æ³•:\næª¢æŸ¥Referer: Headerçš„refereræ¬„ä½è¨˜éŒ„è‘—ä¾†æºçš„domainï¼Œåªè¦æ˜¯ä¸åŒdomainå°±æ“‹ï¼Œç°¡å–®ç›´æ¥ï¼Œä½†å¯æƒœçš„æ˜¯æœ‰äº›ç€è¦½å™¨ä¸å¸¶refererã€‚ XSS (Cross-Site-Scripting) å°‡æƒ¡æ„ç¨‹å¼ç¢¼æ¤å…¥ç¶²ç«™å…§ï¼Œè®“ç¶²ç«™å»åŸ·è¡Œæƒ¡æ„ç¨‹å¼ç¢¼ä¾†é”åˆ°ç²å–æ•æ„Ÿè³‡æ–™çš„ç›®çš„\nXSS: å°‡æƒ¡æ„ç¨‹å¼ç¢¼æ¤å…¥ç¶²ç«™å…§ï¼Œè®“ç¶²ç«™å»åŸ·è¡Œæƒ¡æ„ç¨‹å¼ç¢¼ä¾†é”åˆ°ç²å–æ•æ„Ÿè³‡æ–™çš„ç›®çš„ å„²å­˜å‹XSS: XSSæ”»æ“Šå¸¸è¦‹æ–¼ç¶²ç«™æœ‰è®“ä½¿ç”¨è€…è¼¸å…¥çš„éƒ¨åˆ†(å¦‚è¡¨å–®ï¼Œå›è¦†ç•™è¨€ç­‰ç­‰)ï¼Œæ”»æ“Šè€…è¼¸å…¥æƒ¡æ„ç¨‹å¼ç¢¼å¾Œé€å‡ºï¼Œä¾†æ³¨å…¥æƒ¡æ„ç¨‹å¼åˆ°Databaseï¼Œä¾†é”åˆ°æ”»æ“Šçš„ç›®çš„ï¼Œå› ç‚ºæ”»æ“ŠæˆåŠŸå¾Œåªæœ‰åœ¨Databaseçœ‹å¾—å‡ºä¾†ï¼Œä¸”æ¯å€‹ä½¿ç”¨è€…å°æœƒå—åˆ°å½±éŸ¿ï¼Œå› æ­¤æ®ºå‚·åŠ›æœ€å¤§ã€‚\nEx. å‡å¦‚ç¤¾ç¾¤ç¶²ç«™å›è¦†ç•™è¨€æ™‚è¼¸å…¥ ä¸”ç¶²ç«™æ²’æœ‰åšä»»ä½•XSSé˜²ç¦¦å°±é¡¯ç¤ºuserçš„ç•™è¨€ï¼Œå‰‡ç¶²ç«™å°±æœƒé¡¯ç¤ºå‡ºæƒ¡æ„çš„é€šçŸ¥ã€‚\ré é˜²æ–¹æ³•:\næœ€é‡è¦çš„é‚è¼¯æ˜¯ä¸èƒ½ç›¸ä¿¡ä½¿ç”¨è€…çš„è¼¸å…¥ï¼Œåœ¨è¼¸å…¥æ™‚æª¢æŸ¥æˆ–æ˜¯è¼¸å‡ºæ™‚åšæª¢æŸ¥ï¼Œä¾‹å¦‚å°‡ä½¿ç”¨è€…è¼¸å…¥çš„å€¼è½‰æˆç´”æ–‡å­—å†è¼¸å‡ºï¼Œåœ¨å‰ç«¯æ¡†æ¶ä¸‹(ex. Vue.js)ï¼Œå³æœƒå°æ‰€æœ‰çš„dataåšXSSçš„æª¢æŸ¥å°‡å…¶è½‰æ›ç‚ºç´”æ–‡å­—ã€‚ Cross-origin Resource Sharing Protocol (CORS) Allow for cross-site HTTP requests HTTP requests for resources from a different domain than the domain of the resource making the request. JSON and JSONP ajaxè«‹æ±‚å—åŒæºç­–ç•¥å½±éŸ¿ï¼Œä¸å…è¨±é€²è¡Œè·¨åŸŸè«‹æ±‚ï¼Œè€Œscriptæ¨™ç±¤srcå±¬æ€§ä¸­çš„éˆæ¥å»å¯ä»¥è¨ªå•è·¨åŸŸçš„jsè…³æœ¬ï¼Œåˆ©ç”¨é€™å€‹ç‰¹æ€§ï¼Œæœå‹™ç«¯ä¸å†è¿”å›JSONæ ¼å¼çš„æ•¸æ“šï¼Œè€Œæ˜¯è¿”å›ä¸€æ®µèª¿ç”¨æŸå€‹å‡½æ•¸çš„jsä»£ç¢¼ï¼Œåœ¨srcä¸­é€²è¡Œäº†èª¿ç”¨ï¼Œé€™æ¨£å¯¦ç¾äº†è·¨åŸŸã€‚\nService that supports JSONP allows to specify a query string parameter for a wrapper function to load the data in JavaScript code otherwise the data cannot be used in JavaScript they\u0026rsquo;re loaded into the memory but assigned to nothing A kind of workaround for the same origin policy Resource Format GET DELETE http://company.at/customers XML AJAX (1) AJAX (2) http://company.at/suppliers JSON AJAX, JSONP (3) AJAX (4) http://weather.at/innsbruck XML AJAX-CORS (5) AJAX-CORS (6) http://people.at/students JSON AJAX-CORS, JSONP (7) AJAX-CORS (8) http://people.at/{dob}/contact VCARD AJAX-CORS (9) AJAX-CORS (10) res.writeHead(200, {\r'Content-Type': 'Application/json',\r'Access-Control-Allow-Origin': '*'\r});\rLecture 6 - Security é©—è­‰ï¼ˆAuthenticationï¼‰æ˜¯è­‰æ˜èº«åˆ†ï¼ˆidentityï¼‰çš„æ©Ÿåˆ¶ï¼Œä¾‹å¦‚: authenticate(name, passwd)æ–¹æ³•å®šç¾©äº†å¦‚ä½•ä½¿ç”¨nameèˆ‡passwdé€²è¡Œé©—è­‰ã€‚æ­¤å¤–ï¼Œé©—è­‰æ–¹å¼ä¸åƒ…æ˜¯åŸºæ–¼åç¨±åŠå¯†ç¢¼ï¼Œä¹Ÿæœ‰å¯èƒ½åŸºæ–¼æ†‘è­‰ï¼ˆCertificateï¼‰ä¹‹é¡çš„æ©Ÿåˆ¶ã€‚ä¸€æ—¦caterpillaré€šéé©—è­‰ï¼Œå°±å¯ä»¥çœ‹åˆ°è¨Šæ¯ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå¦å¤–æœ‰å€‹æ©Ÿåˆ¶æ±ºå®šè¨Šæ¯è³‡æºå¯å¦æˆæ¬Šè§€çœ‹ï¼Œå°±åƒæˆæ¬Šï¼ˆAuthorizationï¼‰å®šç¾©äº†èº«åˆ†èˆ‡è³‡æºä¹‹é–“çš„å­˜å–æ§åˆ¶è¦å‰‡ï¼Œä¾‹å¦‚ï¼Œif(authorized()) { show(\u0026ldquo;message\u0026rdquo;); }é€™å€‹æµç¨‹ï¼Œå®šç¾©äº†\u0026quot;message\u0026quot;æ˜¯å¦å¯ä»¥é¡¯ç¤ºã€‚\nStandard: HTTP authentication\nHTTP defines two options Basic Access Authentication Digest Access Authentication Basic authentication HTTP Basic Authenticationç‚ºä¸€ç°¡å–®çš„HTTPè«‹æ±‚èªè­‰æ–¹æ³•ï¼Œç”¨ä¾†ä¿è­·serverç«¯çš„è³‡æºã€‚ç•¶clientç«¯å°serverç™¼èµ·è«‹æ±‚çš„åŒæ™‚å¿…é ˆæä¾›å¸³è™Ÿ(user-id)åŠå¯†ç¢¼(password)è®“serverç«¯é©—è­‰ï¼Œåªæœ‰é€šéé©—è­‰æ‰èƒ½å–å¾—è³‡æºã€‚\nClientæä¾›Basic Authenticationè«‹æ±‚çš„å¸³è™Ÿå¯†ç¢¼çš„æ–¹å¼ç‚ºï¼Œ åœ¨HTTP Request HeadersåŠ å…¥key=Authorizationï¼Œvalue=Basic \u0026lt;basic-credentials\u0026gt;ã€‚Basicç‚ºBasic Authenticationè¦ç¯„çš„åç¨±ï¼Œå›ºå®šåŠ åœ¨å‰é ­ã€‚\u0026lt;basic-credentials\u0026gt;ç‚ºBasic Authenticationçš„æ†‘è­‰ï¼Œå…¶ç‚ºä»¥Base64 encodeå°user-id:passwordçš„ç·¨ç¢¼ã€‚ä¾‹å¦‚å¸³è™Ÿç‚ºjohnï¼Œå¯†ç¢¼ç‚ºabcï¼Œå‰‡\u0026lt;basic-credentials\u0026gt;ç‚ºä»¥Base 64 encodeå°john:abcçš„ç·¨ç¢¼ï¼Œä¹Ÿå°±æ˜¯am9objphYmM=ã€‚\nå› æ­¤å°serverè³‡æºç™¼å‡ºè«‹æ±‚æ™‚ï¼Œåœ¨Request HeadersåŠ å…¥ä»¥ä¸‹æ¬„ä½ã€‚\n1 Authentication: Basic am9objphYmM= A client may associate a valid credentials with realms such that it copies authorization information in requests for which server requires authentication (by WWW-Authenticate header) Credentials: credentials are base64 encoded (the format is: username:password) Digest Access Authentication No password between a client and a server but a hash value.\nTLS TLS provives message framing mechanism Every message is signed with Message Authentication Code (MAC) MAC hashes data in a message and combines the resulting hash with a key (negotiated during the TLS handshake) The result is a message authentication code sent with the message TLS and Proxy Servers:\nTLS Offloading: Inbound TLS connection, plain outbound connection â€’ Proxy can inspect messages\nTLS Bridging: Inbound TLS connection, new outbound TLS connection â€’ Proxy can inspect messages\nEnd-to-End TLS (TLS pass-through): TLS connection is passed-through the proxy - Proxy cannot inspect messages\nLoad balancer: Can use TLS offloading or TLS bridging â€’ Can use TLS pass-through with help of Server Name Indication (SNI)\nJSON Web Token (JWT) After user logs in, following requests contain JWT token.\næˆæ¬Š(Authorization)ï¼šé€™æ˜¯å¾ˆå¸¸è¦‹ JWT çš„ä½¿ç”¨æ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨è€…å¾ Client ç«¯ç™»å…¥å¾Œï¼Œè©²ä½¿ç”¨è€…å†æ¬¡å° Server ç«¯ç™¼é€è«‹æ±‚çš„æ™‚å€™ï¼Œæœƒå¤¾å¸¶è‘— JWTï¼Œå…è¨±ä½¿ç”¨è€…å­˜å–è©² token æœ‰æ¬Šé™çš„è³‡æºã€‚å–®ä¸€ç™»éŒ„(Single Sign On)æ˜¯ç•¶ä»Šå»£æ³›ä½¿ç”¨ JWT çš„åŠŸèƒ½ä¹‹ä¸€ï¼Œå› ç‚ºå®ƒçš„æˆæœ¬è¼ƒå°ä¸¦ä¸”å¯ä»¥åœ¨ä¸åŒçš„ç¶²åŸŸ(domain)ä¸­è¼•é¬†ä½¿ç”¨ã€‚ è¨Šæ¯äº¤æ›(Information Exchange)ï¼šJWT å¯ä»¥é€éå…¬é‘°/ç§é‘°ä¾†åšç°½ç« ï¼Œè®“æˆ‘å€‘å¯ä»¥çŸ¥é“æ˜¯èª°ç™¼é€é€™å€‹ JWTï¼Œæ­¤å¤–ï¼Œç”±æ–¼ç°½ç« æ˜¯ä½¿ç”¨ header å’Œ payload è¨ˆç®—çš„ï¼Œå› æ­¤é‚„å¯ä»¥é©—è­‰å…§å®¹æ˜¯å¦é­åˆ°ç¯¡æ”¹ã€‚ Open standard Mechanism to securely transmit information between parties as a JSON object Can be verified and trusted as it is digitally signed Basic concepts\nCompact â†’ has a small size â†’ can be transmitted via a URL, POST, HTTP header. Self-contained â†’ payload contains all required user information. Oauth 2.0 OAuth 1.0 â€“ first standard, security problems, quite complex OAuth 2.0 â€“ new version, not backward compatibile with 1.0 request: client_id + andÂ client_secret, and redirect_link. grant: code + client_id + client_secret + redirect_link + grant_type: \u0026ldquo;authorization_code\u0026rdquo; access token: JWT token jwt.decode(token) to get the data.\nResponse:\nSuccess â€“ 200 OK. Error â€“ 401 Unauthorized when token expires or the client hasn\u0026rsquo;t performed the authorization request. Refreshing a token:\nPOST request to the token endpoint with grant_type=refresh_token and the previously obtained value of refresh_token OpenID Protocol OpenIDçµ•å¦™åœ°è§£æ±ºäº†å¤šå€‹å¸³è™ŸåŒæ­¥å•é¡Œ XRDSï¼šä¸€ç¨®åŸºæ–¼XMLçš„XRIè³‡æºæè¿°ç¬¦ã€‚å®ƒè¢«è¨­è¨ˆç”¨ä¾†æä¾›é—œæ–¼XRIçš„å¯ç”¨çš„ã€æè¿°æ€§ä¿¡æ¯ã€‚åœ¨OpenIDæ‡‰ç”¨å ´åˆä¸­ï¼ŒXRDSç”¨ä¾†æè¿°OpenIDæœå‹™å™¨ï¼Œä¸¦ä¸”ä½¿ç”¨ã€Œpriorityã€åƒæ•¸æ¨™è­˜äº†ç”¨æˆ¶å°æœå‹™å™¨çš„å„ªé¸é †åºã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œhttp://www.livejournal.com/users/frankå…·æœ‰æœ€é«˜çš„å„ªå…ˆæ¬Šï¼ˆæœ€ä½çš„æ•¸å€¼ï¼‰ï¼š\nLecture 7 - Protocols for the Realtime Web server is able to send pieces of response w/o terminating the conn.\nusing transfer-encoding header in HTTP 1.1 (Transfer-Encoding: chunked) Each chunk starts with hexadecimal value for length End of response is marked with the chunk length of 0 using End of File in HTTP 1.0 (server omits content-lenght in the response) Pushing â€“ updates from the server (also called COMET)\nlong polling â€“ server holds the request for some time streaming â€“ server sends updates without closing the socket Server-Sent Events API to handle HTTP streaming in browsers by using DOM events transparent to underlying HTTP streaming mechanism Format response\u0026rsquo;s content-type must be text/event-stream â€’ every line starts with data:, event message terminates with 2 \\n chars. every message may have associated id (is optional) - When a connection is dropped EventSource will automatically reconnect: It may advertise the last seen message ID The client appends Last-Event-ID header in the reconnect request: The stream can be resumed and lost messages can be retransmitted. Cross-document messaging Lecture 8 - HTTP/2 communication is multiplexed within a single TCP connection Multiple requests and responses can be delivered in parallel, deliver lower page load times. Browser Request Prioritization Flow control Header compression Binary Framing Layer PUSH_PROMISE frames\nA singnal that the server intents to push resources to the client The client needs to know which resources the server intends to push to avoid creating duplicate requests for these resources. pushed resources must obey the same-origin policy Ref [1] CORS/CSRF/XSS ä»‹ç´¹èˆ‡é˜²ç¦¦æ–¹æ³• https://medium.com/%E7%A2%BC%E8%BE%B2%E8%83%8C%E5%8C%85%E5%AE%A2/cors-csrf-xss-%E4%BB%8B%E7%B4%B9%E8%88%87%E9%98%B2%E7%A6%A6%E6%96%B9%E6%B3%95-a1f5c55d96a1\n[2] js è·¨åŸŸé—®é¢˜ https://zhuanlan.zhihu.com/p/583595367\n[3] é©—è­‰èˆ‡æˆæ¬Š https://www.ithome.com.tw/voice/134389\n[4] HTTP Basic Authentication https://matthung0807.blogspot.com/2020/04/http-basic-authentication.html\n[5] æ˜¯èª°åœ¨æ•²æ‰“æˆ‘çª—ï¼Ÿä»€éº¼æ˜¯ JWT ï¼Ÿ https://5xruby.tw/posts/what-is-jwt\n","permalink":"https://jonathan-tw.github.io/posts/middleware-architectures-2-review/","summary":"Lecture 2 - Asynchronous I/O Programming Models Concurrency (Asynchronous I/O = cooperative multitasking) Multiple tasks have the ability to run in an overlapping manner Concurrency does not imply parallelism! Multiprocessing CPU-bounded tasks Multithreading I/O bound tasks It uses preemtive multitasking Promise object: an async object that be returned by the async function. An object representing completion or failure of an asynchronous operation. Await function: makes program to wait until the promise is resolved or rejected can only be usded inside async function Lecture 3 - Cloud Architectures Cloud Computing Concepts On-demand and self-service (ç•¶éœ€è¦è³‡æºæ™‚æ‰è¢«æä¾›,è‡ªå‹•åŒ–) Resources are provisioned as they are requested and when they are required â€’ No human interaction, automatic No human interaction, automatic Board network access (è³‡æºå¯è¢«ç¶²è·¯","title":"(èª²ç¨‹ç­†è¨˜)(CTU in Prague) Middleware architectures 2 Review"},{"content":"AJAX / XHR states / CORS / Data access Tasks Create a simple HTML page with an info text field and a single button Implement a JavaScript function which is triggered when the button is clicked The function should fetch relatively large file (e.g. 100-200MB) in the text field show following states: loading - when the open method was called loaded - when the send method was called downloading - while the data is being downloaded finished downloading - when the data has beeen downloaded you can use Promise, async/await Description AJAX overview:\nAsynchronous JavaScript and XML technique for creating better, faster, and more interactive web applications relies on XML, JSON, HTML, CSS and JavaScript AJAX is not a programming language Running this demo by using the jquery module to achieve the ajax request. Following the xhr state in https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/readyState. The browser console would show the current XMLHttpRequest state when starting the server.\nDeal with the problem of CORS To deal with the CORS problem in the ajax request, I upload the large file to the Amazon S3 bucket and set the following json code to allow the local server accessing the remote resources.\nData access A company needs to design an AJAX aplication that will access various resources on the Web by using JavaScript code running in a browser. This application is not public and only internal employees of the company can use it. This application will be available at the address http://company.at. Following table shows a list of URL addresses of resources, their formats and HTTP methods that the application will use to access these resources. Add all possible access technologies to the table for methods GET and DELETE. Note: parameter {dob} means date of birth.\nResource Format GET DELETE http://company.at/customers XML AJAX (1) AJAX (2) http://company.at/suppliers JSON AJAX, JSONP (3) AJAX (4) http://weather.at/innsbruck XML AJAX-CORS (5) AJAX-CORS (6) http://people.at/students JSON AJAX-CORS, JSONP (7) AJAX-CORS (8) http://people.at/{dob}/contact VCARD AJAX-CORS (9) AJAX-CORS (10) (1) (2): Follow the Same Origin Policy (3) (4): JSONP only works on GET method (5) (6) (9) (10): Using CORS to deal with problem of different domain (7) (8): JSONP only works on GET method AJAX-CORS (add http header in server) 1 2 3 4 res.writeHead(200, { \u0026#39;Content-Type\u0026#39;: \u0026#39;Application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }); AJAX-CORS (Using CORS Anywhere which adds CORS headers to the proxied request) Code (index.html) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt; HW2 - AJAX and XHR states \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;Download File\u0026#34; onclick=\u0026#34;DownloadFile()\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; value=\u0026#34;Ready to Download\u0026#34; id=\u0026#34;state\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;progressCounter\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://code.jquery.com/jquery-3.6.0.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var progressElem = $(\u0026#39;#progressCounter\u0026#39;); function DownloadFile() { var url = \u0026#34;https://mytest1.s3.amazonaws.com/file.bz2\u0026#34;; progressElem.text(url); $.ajax({ url: url, cache: false, xhr: function() { var xhr = new XMLHttpRequest(); xhr.onreadystatechange = function() { if ( xhr.readyState == 1 ) { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;loading\u0026#39;); console.log(\u0026#34;Loading - when the open method was called\u0026#34;); } // if else if (xhr.readyState == 2) { if ( xhr.status == 200 ) { xhr.responseType = \u0026#34;blob\u0026#34;; } // if document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;loaded\u0026#39;); console.log(\u0026#34;Loaded - when the send method was called\u0026#34;); } // else if else if ( xhr.readyState == 3 ) { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;downloading\u0026#39;); console.log(\u0026#34;downloading - while the data is being downloaded\u0026#34;); } // else if else { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;finished downloading\u0026#39;); console.log(\u0026#34;finished downloading - when the data has beeen downloade\u0026#34;); } // else }; // xhr.onready xhr.addEventListener(\u0026#34;progress\u0026#34;, function (evt) { console.log(evt.lengthComputable); if (evt.lengthComputable) { var percentComplete = evt.loaded / evt.total; progressElem.html(Math.round(percentComplete * 100) + \u0026#34;%\u0026#34;); } }, false); return xhr; }, success: function (data) { console.log(\u0026#34;nono\u0026#34;); // Convert the Byte Data to BLOB object. var blob = new Blob([data], { type: \u0026#34;application/octetstream\u0026#34; }); var link = window.URL.createObjectURL(blob); var a = $(\u0026#34;\u0026lt;a /\u0026gt;\u0026#34;); a.attr(\u0026#34;download\u0026#34;, \u0026#34;file.bz2\u0026#34;); // attributes,value a.attr(\u0026#34;href\u0026#34;, link); console.log(link); $(\u0026#34;body\u0026#34;).append(a); a[0].click(); $(\u0026#34;body\u0026#34;).remove(a); } }); }; // DownloadFile() \u0026lt;/script\u0026gt; \u0026lt;br\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Node.js server in Docker Tasks Implement a simple HTTP server in Node.js implement a \u0026ldquo;hello NAME\u0026rdquo; service endpoint request: \u0026ldquo;http://localhost:8080/John\u0026rdquo;, response \u0026ldquo;Hello John\u0026rdquo; Define an image Dockerfile, with the following specifications: build on a chosen node.js image the container should listen on port 8888 load the server implementation from a local directory run the server Create a docker image from the Dockerfile Create a container for the image tip: use the -p parameter to map public port to a private port inside the container (e.g. -p 8080:8888) Check if the container is running (docker container ls) Send several requests to the server running within the container you can use ping, curl or telnet for testing Stop the container Description Docker Overview:\npackage an application and its dependencies in a virtual container build, commit and share images based and primarily for Linux allows operating-system-level virtualization run isolated packages, also known, as containers Define an image Dockerfile and create a docker image 1 2 3 4 FROM --platform=linux/amd64 node:10-alpine COPY . /app WORKDIR /app CMD node /app/server.js 1 docker build -t hw3-docker . Create a container and run the curl. -p (Port mapping for the container and the host machine) 1 docker run -p 8080:8888 hw3-docker Docker advanced (Redis) Tasks Task 1: Start a docker container for a redis server: build on a chosen redis image redis run the server Task 2: Start a docker container for a redis client: build on a chosen redis image redis with the client insert some user info where key is the person name, and value is the address Task 3: Implement a simple HTTP server in Node.js implement a \u0026ldquo;http://localhost:8080/person/{person_name}/address\u0026rdquo; API, which returns the address of a person request: GET \u0026ldquo;http://localhost:8080/person/John/address\u0026rdquo;, response \u0026ldquo;Thakurova 9, 160 00, Prague\u0026rdquo; the server should fetch the data from a Redis server. Redis runs in a separate container than the node.js server! (see above). Task 4: Define an image Dockerfile (for the node.js server), with the following specifications: build on a chosen node.js image load the server implementation from a local directory run the server Create a docker image from the Dockerfile Create and run a container Test the server - it shoudl return the address for a person retrieved from the linked redis server container Description Task1 - Run the Redis Server in a container Redis-server stores the data from Redis-client, and can handle the request from the Node.js\n1 sudo docker run -p 6379:6379 --name redis-server -d redis Task 2 - Run the Redis client in a container Redis-client is linked to the Redis-server\n1 sudo docker run -it --link redis-server:redis --name redis-client -d redis Redis-client let user insert the data {kay,value} Task3 - Implement a simple HTTP server and run it on the host, and Node.js fetches the data from the Redis-server\nFailture\r2022/04/03 - I faced the problem in the recent version of Redis, it seems that the problems are MAC docker or Redis version. https://stackoverflow.com/questions/71529419/redis-overiding-remote-host-in-nodejs I implemented the methods of docker-compose or link-container, but still encountered the connection refused problem.\rSuccess\rMust use redis version 3.0.2!!!!!!!!!\n1 npm install redis@3.0.2 Task4 - Docker-compose file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # version of docker-compose version: \u0026#39;3.9\u0026#39; # \u0026#39;services\u0026#39; are equivalent to \u0026#39;containers\u0026#39; services: redis: image: \u0026#39;redis\u0026#39; ports: - \u0026#39;6379:6379\u0026#39; tnode11: image: \u0026#39;hw4-docker\u0026#39; # Specify an array of ports to map ports: - \u0026#39;8080:8888\u0026#39; Code (Redis server) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 const http = require(\u0026#39;http\u0026#39;); const redis = require(\u0026#39;redis\u0026#39;) var url = require(\u0026#34;url\u0026#34;); const server = http.createServer((req,res) =\u0026gt; { res.setHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/plain\u0026#39;); get_api_string = req.url.split(\u0026#39;/\u0026#39;); const client = redis.createClient({ host: \u0026#39;redis\u0026#39;, port: 6379 }) // get request from if ( get_api_string[1] == \u0026#34;person\u0026#34; \u0026amp;\u0026amp; get_api_string[3] == \u0026#34;address\u0026#34; ) { client.get(get_api_string[2], function (error, result) { console.log(\u0026#39;GET result -\u0026gt;\u0026#39; + result); if ( result == null ) res.write(\u0026#34;You must input the data to the redis-server from the redis client\u0026#34;); else res.write(\u0026#34;Hello \u0026#34; + result ); res.end(); }); } else { var echo_name = req.url.substring(1); res.write(\u0026#34;Hello \u0026#34; + echo_name); res.end(); } }); server.listen(8888, \u0026#39;0.0.0.0\u0026#39;, () =\u0026gt; { console.log(\u0026#34;listening for 8080\u0026#34;); }); HTTP/2 Push mechanism Tasks Implement an HTTP/2 push mechanism Implement a simple HTML page consisting of at least 3 additional resources (e.g. css, js, image, ..) Implement a simple http server serving the page via HTTP/2 When the html page is requested, push all those files together with the requested page Use http2 module Description HTTP/2 with the push method HTTP/2 without the push method The above results indicate that HTTP/2 using the push method is faster than the method without the push method. The key reason is the concurrency in the different types of files in one connection (Extend this concept to the QUIC protocol, fixing the HoL blocking problem).\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const sendFile = (stream, fileName) =\u0026gt; { const fd = fs.openSync(fileName, \u0026#34;r\u0026#34;); const stat = fs.fstatSync(fd); const headers = { \u0026#34;content-length\u0026#34;: stat.size, \u0026#34;last-modified\u0026#34;: stat.mtime.toUTCString(), \u0026#34;content-type\u0026#34;: mime.getType(fileName) }; stream.respondWithFD(fd, headers); // key point }; const pushFile = (stream, path, fileName) =\u0026gt; { stream.pushStream({ \u0026#34;:path\u0026#34;: path }, (err, pushStream) =\u0026gt; { if (err) { throw err; } sendFile(pushStream, fileName); }); }; Server-Sent Events Task Design and implement simple SSE server in Node.js and simple client in HTML/JavaScript.\nThe client connects to server and displays all incoming messages. The server sends every 2 seconds one message - e.g. one line of text file. Description Server-sent events are functions extended on top of HTTP. The server can actively send data to the client based on the new MIME type( text/event-stream ). Client: The URL is passed to EventSource() and creates a new EventSource object. onmessage() is triggered by receiving the data.\nServer: new MIME type(text/event-stream).\nCode 1 2 3 4 const message = `retry: ${refreshRate}\\nid:${id}\\ndata: ${data}\\n\\n`; res.write(message); }, refreshRate); retry: Specify the time interval for the browser to re-initiate the connection\nid: ID for each event-stream\ndata: the event data\n\u0026lsquo;\\n\u0026rsquo; gaps the indicator. \u0026lsquo;\\n\\n\u0026rsquo; switches the lines\nOAuth-Browser-Based-App Task Design and implement a simple OAuth - Browser-Based App. Browser-based apps run entirely in the browser after loading the source code from a web page.\nUse a simple server (https) for serving static content (html, js, â€¦â€‹). Configure any OAuth application of your choice. You can use any OAuth solution as authorization and resource server: Google, GitHub, â€¦â€‹ The app in browser connects to the authorization server and allows access to the resources. The app collects an presents in the browser any resource from the resource server using the provided code/token (e.g. list of contacts, files, messages, repositories, â€¦â€‹) Do not use any OAuth library (e.g. GoogleAuth, â€¦â€‹) Description Workflow:\nLog in to the Google cloud console to register a new project. Get the client_id and client_secret and set the redirect_link\nClient calls getGoogleAuthURL() to get the code from https://accounts.google.com/o/oauth2/v2/auth\nScope type: userinfo is the public data. If we want to leverage the common services, just like Google drive or photo\u0026hellip;, we need to add the test users until publishing the project.\nThe server posts the code with Axios to get the token from the https://oauth2.googleapis.com/token\nCall jwt.decode(token) to get the data\nCreate a new project in the Google cloud console\rEnter index.html to log in through ouath2 After logging, Authorization: OK. Print the callback data\rCode (OAuth server implement) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 const HTTP2_PORT = 3000; const http2 = require(\u0026#39;http2\u0026#39;); const mime = require(\u0026#39;mime\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const url = require(\u0026#34;url\u0026#34;); const qs = require(\u0026#39;querystring\u0026#39;); const axios = require(\u0026#39;axios\u0026#39;); const jwt = require(\u0026#39;jsonwebtoken\u0026#39;); const serverOptions = { key: fs.readFileSync(\u0026#39;localhost-privkey.pem\u0026#39;), cert: fs.readFileSync(\u0026#39;localhost-cert.pem\u0026#39;) }; // fill your info var ClientID = \u0026#34;\u0026#34;; var ClientSercet = \u0026#34;\u0026#34;; var RedirectLink = \u0026#34;\u0026#34;; async function getTokenFromGoogle(get_token_url, value) { try { var res = await axios.post(get_token_url, qs.stringify(value), { headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/x-www-form-urlencoded\u0026#39;, }, }); } catch(err) { console.log(err); } return res.data; } // read and send file content in the stream const sendFile = (stream, fileName) =\u0026gt; { const fd = fs.openSync(fileName, \u0026#34;r\u0026#34;); const stat = fs.fstatSync(fd); const headers = { \u0026#34;content-length\u0026#34;: stat.size, \u0026#34;last-modified\u0026#34;: stat.mtime.toUTCString(), \u0026#34;content-type\u0026#34;: mime.getType(fileName) }; stream.respondWithFD(fd, headers); // key point }; // handle requests const http2Handlers = (req, res) =\u0026gt; { // send empty response for favicon.ico if (req.url === \u0026#34;/favicon.ico\u0026#34;) { res.stream.respond({ \u0026#34;:status\u0026#34;: 200 }); res.stream.end(); return; } if (req.url === \u0026#34;/\u0026#34; || req.url === \u0026#34;/index.html\u0026#34; ) { req.url = \u0026#34;/index.html\u0026#34;; const fileName = __dirname + req.url; sendFile(res.stream, fileName); } // if else if (req.url.includes( \u0026#34;/api/oauth/google\u0026#34; ) ) { var parsed = url.parse(req.url); var query = qs.parse(parsed.query); var code = query.code; // get the toake with code var get_token_url = \u0026#34;https://oauth2.googleapis.com/token\u0026#34;; const value = { code, client_id: ClientID, client_secret: ClientSercet, redirect_uri: RedirectLink, grant_type: \u0026#34;authorization_code\u0026#34;, }; getTokenFromGoogle(get_token_url, value).then(function(result) { var googleUser = jwt.decode(result.id_token); res.write( \u0026#39;\u0026lt;h1\u0026gt; Hello \u0026#39; + googleUser.name + \u0026#39;\u0026lt;/h1\u0026gt;\u0026#39;); res.write(\u0026#39;Your email address: \u0026#39; + googleUser.email + \u0026#39;\u0026lt;br\u0026gt;\u0026#39;); res.write(\u0026#39;Locale: \u0026#39; + googleUser.locale + \u0026#39;\u0026lt;br\u0026gt;\u0026#39;); res.end(\u0026#39;\u0026lt;img src=\u0026#39; + googleUser.picture + \u0026#39; referrerpolicy=\u0026#34;no-referrer\u0026#34;/\u0026gt;\u0026#39;) console.log(googleUser); }); } }; http2 .createSecureServer(serverOptions, http2Handlers) .listen(HTTP2_PORT, () =\u0026gt; { console.log(\u0026#34;http2 server started on port\u0026#34;, HTTP2_PORT); }); ","permalink":"https://jonathan-tw.github.io/posts/middleware-architectures-2-hw/","summary":"AJAX / XHR states / CORS / Data access Tasks Create a simple HTML page with an info text field and a single button Implement a JavaScript function which is triggered when the button is clicked The function should fetch relatively large file (e.g. 100-200MB) in the text field show following states: loading - when the open method was called loaded - when the send method was called downloading - while the data is being downloaded finished downloading - when the data has beeen downloaded you can use Promise, async/await Description AJAX overview: Asynchronous JavaScript and XML technique for creating better, faster, and more interactive web applications relies on XML, JSON, HTML, CSS and JavaScript AJAX is not a programming language Running this demo by using the jquery module to achieve the ajax request. Following the xhr state in https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/readyState.","title":"(èª²ç¨‹ç­†è¨˜)(CTU in Prague) Middleware architectures 2 HW"},{"content":" OS:Ubuntu: 16.04 / 18.04\nOpencCV: 4.2\nCuda: 8.0 / 9.0\nNivida Driver: 418.56 (GTX 1050ti) / 435 (GTX 1060)\nCPU: i3-8300 3.70Ghz\nffmpeg: 4.2 / 4.2.4\nnv_codec_headers: 8.2 / 9.0\nInstall Nvidia Driver list available Nvidia Driver\n1 ubuntu-drivers devices add repoitory\n1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update instll Nvidia Driver\n1 sudo apt install nvidia-xxx Install CUDA Check CUDA and Nvidia driver compatibility https://docs.nvidia.com/deploy/cuda-compatibility/index.html\nDownload Cuda runfile which determines by your os https://developer.nvidia.com/cuda-downloads\nplease choose not to install nvidia graphics driver, or the driver will be updated**\ninstall cuda (ex: cuda 8.0 for ubuntnu 16.04 )\n1 sudo sh cuda_8.0.61_375.26_linux.run Add environment variables to ~/.bashrc\n1 2 3 4 sudo nano ~/.bashrc export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc Install NV_codec (optional for gpu ) download nv_codec_headers 8.2: https://github.com/FFmpeg/nv-codec-headers/tree/sdk/8.2\ninstall nv_codec_headers 8.2\n1 sudo make install nv_codec_headers 9.0éœ€è¦cuda 9.0ä»¥ä¸Š\nInstall FFmpeg 4.2.4 install necessary tool\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 sudo apt-get update -qq \u0026amp;\u0026amp; sudo apt-get -y install \\ autoconf \\ automake \\ build-essential \\ cmake \\ git-core \\ libass-dev \\ libfreetype6-dev \\ libgnutls28-dev \\ libsdl2-dev \\ libtool \\ libva-dev \\ libvdpau-dev \\ libvorbis-dev \\ libxcb1-dev \\ libxcb-shm0-dev \\ libxcb-xfixes0-dev \\ pkg-config \\ texinfo \\ wget \\ yasm \\ zlib1g-dev install libx264:\n1 2 3 4 git clone https://code.videolan.org/videolan/x264.git cd x264 sudo ./configure --enable-shared --disable-asm sudo make \u0026amp;\u0026amp; sudo make install installl ffplay\n1 sudo apt-get install libsdl2-dev install libfdk_aac (optional)\nhttps://sourceforge.net/projects/opencore-amr/files/fdk-aac/\n1 2 ./configure make \u0026amp;\u0026amp; sudo make install 1 2 3 vi /etc/ld/so.conf.d/aac.conf /usr/local/lib sudo ldconfig install mp3lame (optional)\n1 sudo apt-get install yasm libmp3lame-dev download ffmpeg:\n1 git clone https://git.ffmpeg.org/ffmpeg.git configure command (if you need cuvid)\n1 sudo ./configure --enable-libmp3lame --enable-libfdk_aac --enable-libx264 --enable-gpl --enable-cuda --enable-cuvid --enable-nvenc --enable-nonfree --enable-pic --enable-rpath --enable-shared --extra-cflags=-I/usr/local/cuda/include --extra-ldflags=-L/usr/local/cuda/lib64 --enable-libnpp --enable-ffplay configure command (if you don\u0026rsquo;t need cuvid and sound but want cuda )\n1 sudo ./configure --enable-libx264 --enable-gpl --enable-cuda --enable-nvenc --enable-nonfree --enable-rpath --enable-pic --enable-shared --extra-cflags=-I/usr/local/cuda/include --extra-ldflags=-L/usr/local/cuda/lib64 --enable-ffplay compile ffmpeg\n1 sudo make \u0026amp;\u0026amp; sudo make install add to envirnment\n1 2 3 4 5 6 7 sudo vi ~/.bashrc export PATH=/usr/local/bin:$PATH source ~/.bashrc sudo nano /etc/ld.so.conf # dertermine on your ffmpeg install location /usr/local/ffmpeg/lib sudo ldconfig for more build options https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu\nTest ffmpeg Hardware acceleration methods:\n1 ffmpeg -hwaccels cpu:\n1 time ffmpeg -rtsp_transport tcp -i rtsp://admin:ai123456@192.168.0.111 -r 30 -t 100 -c:v h264 -b:v 2048k -vf scale=1280:-1 -y -c:v libx264 tcp_cpu_output.mp4 gpu:\n1 time ffmpeg -rtsp_transport tcp -hwaccel cuvid -c:v h264_cuvid -i rtsp://admin:ai123456@192.168.0.111 -r 30 -t 100 -b:v 2048k -vf scale_npp=1280:-1 -y -c:v h264_nvenc tcp_gpu_output.mp4 é»˜èªä¸‹ffmpegæŠ“rtspä½¿ç”¨UDP,é€™æœƒloseå¤§é‡packetï¼Œä½¿ç”¨TCPé¿å…ã€‚\nVideo.mp4 CPU GPU Real Time 16s 0.5s Bit rate 1497kbps 1479kbps rtsp://admin:ai123456@192.168.0.111 (30å¹€) CPU GPU Real Time 1m49s 1m41s Bit rate 1911kbps 2067kbps Install Opencv install required and optional package\n1 sudo add-apt-repository -y \u0026#34;deb http://security.ubuntu.com/ubuntu xenial-security main\u0026#34; 1 sudo apt-get install -y libjpeg8-dev libjasper-dev libpng12-dev 1 sudo apt install build-essential cmake git pkg-config libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev 1 sudo apt install libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394-22-dev download opencv 4.2 and extra library\n1 2 3 4 5 6 7 mkdir opencv_base cd opencv_base git clone https://github.com/opencv/opencv.git --branch=4.2.0 git clone https://github.com/opencv/opencv_contrib.git --branch=4.2.0\tcd opencv_base/opencv/ mkdir build cd build with cuda and opencv_world check your cuda_arch_bin:https://developer.nvidia.com/cuda-gpus\n1 2 3 4 5 6 7 8 9 10 11 sudo cmake -D CMAKE_BUILD_TYPE=Release \\ -D OPENCV_GENERATE_PKGCONFIG=YES \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_JAVA=YES \\ -D WITH_CUDA=ON \\ -D BUILD_opencv_world=ON \\ -D WITH_FFMPEG=ON \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/usr/local/opencv_base/opencv_contrib/modules \\ -D BUILD_opencv_python3=yes \\ -D CUDA_ARCH_BIN=6.1 .. without cuda (Simple) 1 2 3 4 5 6 7 8 9 10 11 sudo cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D OPENCV_GENERATE_PKGCONFIG=YES \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_JAVA=YES \\ -D BUILD_opencv_world=ON \\ -D WITH_FFMPEG=ON \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/usr/local/opencv_base/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON .. Compile cuda \u0026amp;\u0026amp; ffmpeg with Opencv é–‹ç™¼æŒ‡ä»¤ includeåŠlibéƒ½ç‚ºç›¸å°è·¯å¾‘ (if you want to pack code )\nTwo .so files need to be at system location libx264.so\n1 sudo cp lib/libx264.so.160 /lib/x86_64-linux-gnu/ libswresample.so\n1 sudo cp lib/libswresample.so.3 /usr/local/lib/ CUDA compiles to shared file (.so): (NV12toBGR) 1 nvcc -shared cuda_convert.cu -o ../lib/libcuda_convert.so --compiler-options \u0026#39;-fPIC\u0026#39; FFmpeg gpu server compiles to shared file (.so) 1 g++ -shared -fPIC ffmpeg_gpu_server.cpp -o ../lib/libffmpeg_gpu_server.so -I../include/ -L ../lib -Wl,--rpath=\u0026#39;../lib\u0026#39; -lavformat -lavcodec -lavutil -lswscale -lswresample -lcuda_convert -lx264 FFmpeg gpu client compiles 1 g++ ffmpeg_gpu_client.cpp -o ../bin/ffmpeg_gpu_client -I../include/ -Wl,--rpath=\u0026#39;../lib\u0026#39; -L -lffmpeg_gpu_server -lopencv_world -ldl -lm -lz -lX11 -std=gnu++11 ","permalink":"https://jonathan-tw.github.io/posts/ffmpeg_note_1/","summary":"OS:Ubuntu: 16.04 / 18.04 OpencCV: 4.2 Cuda: 8.0 / 9.0 Nivida Driver: 418.56 (GTX 1050ti) / 435 (GTX 1060) CPU: i3-8300 3.70Ghz ffmpeg: 4.2 / 4.2.4 nv_codec_headers: 8.2 / 9.0 Install Nvidia Driver list available Nvidia Driver 1 ubuntu-drivers devices add repoitory 1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update instll Nvidia Driver 1 sudo apt install nvidia-xxx Install CUDA Check CUDA and Nvidia driver compatibility https://docs.nvidia.com/deploy/cuda-compatibility/index.html Download Cuda runfile which determines by your os https://developer.nvidia.com/cuda-downloads please choose not to install nvidia graphics driver, or the driver will be updated** install cuda (ex: cuda 8.0 for ubuntnu 16.04 ) 1 sudo sh cuda_8.0.61_375.26_linux.run Add environment variables to ~/.bashrc 1 2 3 4 sudo nano ~/.bashrc export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc Install NV_codec (optional for gpu ) download nv_codec_headers 8.2: https://github.com/FFmpeg/nv-codec-headers/tree/sdk/8.2 install nv_codec_headers 8.2 1 sudo make install nv_codec_headers 9","title":"FFmpegé–‹ç™¼ç³»åˆ—1 - FFmpegã€OpenCVã€CUDAã€NV_Codec GPUåŠ é€Ÿç’°å¢ƒæ­å»º"}]