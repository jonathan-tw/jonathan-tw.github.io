[{"content":"HW1 - Vector Space Model TF、IDF計算方式: 一開始會讀取document的資料前處理，先去做一些normalized，讓lexicon不要有重複的term，而為了讓程式能在1分鐘內跑完，lexicon使用c++的unordered_map下去存，unordered_map使用hash function找到key term，用來尋找及修改object有著極高的效率。有了normalized後的資料，就可以計算每個term在每個檔案中的TF (local)，接著算出每個term的IDF (global)。TF: 該index Term在該Doc下出現的次數。IDF: 該index Term 在所有Doc下出現的文章次數。\nDocument Term Weight、Query Term Weight計算方式: 有了TF、IDF之後，就能算出Document Term Weight及Query Term Weight。這裡使用的方法是老師講義中的Scheme3，會有較佳的score。\n左為:doc term 右為:query term\nVector Space Model運作原理及參數調整: 利用兩相同長度的向量(doc weight vector、query weight vector)求出cosine-similarity(兩向量之夾角)，此cosine-similarity值就代表ranking，Ranking值的範圍0 ≤ 𝑠𝑖𝑚 𝑞, 𝑑𝑗 ≤ 1。Ranking值高代表此doc與query的關聯性更高，排名會比其他ranking低的doc前面。而關於Vector Space Model參數的調整，我在老師的scheme3上進行調整，把query weight constant值改1.5、doc weight constant改2.5，有著較佳的結果。\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[4190] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 4191.0 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name ); all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); } // while } // else _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] + 2.5 ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); print_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc( Document doc ) { vector\u0026lt;double\u0026gt; temp_query_weights; vector\u0026lt;double\u0026gt; temp_doc_weights = doc.all_term_weight_in_file; int i; for ( i = 0; i \u0026lt; doc.all_term_names_in_file.size(); i++ ) { temp_query_weights.push_back( 0.0 ); } // for for ( i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto iterator = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( iterator == doc.all_term_names_in_file.end() ) { temp_query_weights.push_back( query_file.term_weights[i] ); temp_doc_weights.push_back( 0.0 ); } // if else { int pos = distance( doc.all_term_names_in_file.begin(), iterator ); double relpace_weight = query_file.term_weights[i]; temp_query_weights[pos] = relpace_weight; } // else } // for query_weights = temp_query_weights; doc_weights = temp_doc_weights; return cosine_similarity(); } // rank_doc() double cosine_similarity() { // cout \u0026lt;\u0026lt; query_weights.size() \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; doc_weights.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; double dot = 0.0, denom_query = 0.0, denom_doc = 0.0; for ( int i = 0; i \u0026lt; doc_weights.size(); i++ ) { dot = dot + query_weights[i] * doc_weights[i]; denom_query = denom_query + query_weights[i] * query_weights[i]; denom_doc = denom_doc + doc_weights[i] * doc_weights[i]; } // for() // cout \u0026lt;\u0026lt; dot \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; denom_query \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; denom_doc \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return dot / ( sqrt( denom_query ) * sqrt( denom_doc ) ); } // cosine_similarity() int partition ( int low, int high ) { double pivot = all_docs[high].ranking; // pivot int i = ( low - 1 ); for (int j = low; j \u0026lt; high; j++) { if ( all_docs[j].ranking \u0026gt; pivot ) { i++; // increment index of smaller element Document doc; doc = all_docs[j]; all_docs[j] = all_docs[i]; all_docs[i] = doc; } // if } // for i++; Document doc; doc = all_docs[high]; all_docs[high] = all_docs[i]; all_docs[i] = doc; return i; } // partition() void descending_order_document_ranking( int low, int high) { double pivot = partition( low, high ); descending_order_document_ranking( low, pivot -1 ); descending_order_document_ranking( pivot + 1, high ); } // descending_order_document_ranking() void print_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // print_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW2 - Best Match Model Best Match 25L 一開始使用BM25L進行實作，經過多番嘗試參數，發現其值最多只能到0.70多，後改用BM25能獲得較高的數值。而BM25L改善了其long documents的問題，間接避免了overly-penalizing，shift parameter 𝛿 = 0.5能獲得較佳的結果。\nBest Match 25原理及應用 Best Match 25一樣使用了TF 及 IDF，與the vector model不同的是BM25多加了一項document length normalization的principle，且BM25為BM15及BM11的組合，當 b = 0時，為BM15，b = 1時，為BM11，藉由調控b參數來獲得較佳的結果。\n做完也發現BM25有著比the vector model更好的表現，需要的運算時間較少且精準度更高(無須做cosine-similarity是一大優勢)，但可調節的參數需要自行設定，且doument length太長時會有精準度較差的表現。\nBest Match 25參數 在程式中第137~ 139行中(RankEveryQueryFile)分別設定了k1、k3及b值，獲得最佳參數為以下值，其中k3是隨意值，因query file中的term，tf(I,q)皆 = 1\nK1 = 1.635 b = 0.85 k3 = 1000.0\n藍色項: Document Length Normalization 綠色項: Inverse Document Frequency\n中間項: Inverse Query Frequency可省略(在HW2中)\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[4190] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 4191.0 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; float avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen += doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] + 2.5 ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.635; b = 0.85; k3 = 1000.0; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = ( ( ( k1 + 1.0 ) * doc.all_term_tf_in_file[pos] ) / ( k1 * ( ( 1 - b) + b * ( doc.doc_length / avg_doclen ) ) + doc.all_term_tf_in_file[pos] ) ); doc_temp = doc_temp * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_temp = doc_temp * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW3 - Mean Average Precision (MAP) Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[14954] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 14955.000 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; double avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen = avg_doclen + doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.4; b = 0.5; q = 0.4; k3 = 1000; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3, q; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = doc.all_term_tf_in_file[pos] / ( ( 1 - b ) + b * ( doc.doc_length / avg_doclen ) ); if ( doc_temp \u0026gt; 0 ) { doc_temp = ( ( k1 + 1.0 ) * ( doc_temp + q ) ) / ( k1 + doc_temp + q ); } // if else { doc_temp = 0; } // else doc_score = doc_score + doc_temp; doc_score = doc_score * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_score = doc_score * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW4 - PLSA Model PLSA參數調法 Topic_num: 8 / a (第一項local TF機率term = 0.6) all_doc使用vector存local tf\nb (第二項gobal TF機率term = 0.25) / Inter_run: 30 dictionary使用map存word與position、global TF存\nPLSA理論心得 EM可求得隱含變量T(k)，加入了Pd_z[z][m]、Pw_z[z][w]讓其隨機初始值和為1。 透過每個TOPIC下對於Pd_z[z][m] (每一份DOC(m)下是TOPIC(z)主題之機率)與 Pw_z[z][w] (每一份TOPIC(z)下term(w)之機率)，利用兩者獨立機率反覆對Pz_dw[z][m][position]進行更新。\nPLSA實作心得 程式流程為scanning documetns➡ Init Parameter ( two for m-step、one for e-step) ➡ Run EM for 30 runs ➡ calculate for every query file and log prob\nIn step 1: 存取所有doc並記錄local、global TF到對應容器中\nIn step 2: C++需要new出memory space for these parameters.\nIn step 3: 每run完一次EM完，就會得到更新的Pd_z[z][m]、Pw_z[z][w] for m-step，利用這兩個又繼續更新Pz_dw[z][m][position] for e-step，訓練到接近收斂\nIn step 4: 與query file中term進行計算，得出每個doc對應query之機率並取log以大排到小輸出。\n選用最小Topic(8)的情況底下跑了30 run，需要非常久的時間，雖然在map上已選用unordered_map最快的容器下去實作，但時間還是花得比想像中多，在猜想的原因是因為某些可以使用map下去做映射term index的部分，比起vector中的find速度會快上不少，這次的model比起以往需要更多的時間下去完成試驗，下次的作業應提早開始準備，這樣才有更多時間進行測試。\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; # include \u0026lt;cstdlib\u0026gt; # include \u0026lt;stdlib.h\u0026gt; # include \u0026lt;ctime\u0026gt; using namespace std; # define MAX_NUM_DOC 14955.0 # define MAX_NUM_QUERY 100.0 # define MAX_NUM_TOPIC 8.0 # define MAX_NUM_ITER 30.0 # define MAX_LENGTH 7059938.0 # define MAX_NUM_WORDS 111449.0 struct TermProperty { string term_name; double tf; double prob; // probability ( first or third term ) }; // TermProperty struct Document { int docID; string doc_name; double ranking; double doc_length; vector\u0026lt;TermProperty\u0026gt; term_properties; }; // Documentc struct Word2doc { int docID; int position; }; // Word2doc struct Dictionary { TermProperty term_property; vector\u0026lt;Word2doc\u0026gt; word2doc; int word2dic; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_property.term_name == p.term_property.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_property.term_name; return os; } }; // Dictionary struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class PLSA { public: unordered_map\u0026lt;string, Dictionary\u0026gt; dictionary; vector\u0026lt;Document\u0026gt; all_docs; vector\u0026lt;string\u0026gt; all_words; int num_file; int all_docs_len; void ReadAllDocuments() { all_docs_len = 0; string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; double doc_length = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.term_properties = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; document.docID = num_file; all_docs_len += doc_length; all_docs.push_back( document ); num_file++; doc_length = 0.0; k = _findnext( handle, \u0026amp;file_info); } // while } // else _findclose( handle ); } // ReadAllDocuments() void Init() { num_data = MAX_NUM_DOC; num_words = MAX_NUM_WORDS; num_topics = MAX_NUM_TOPIC; srand(time(NULL)); //設置随機數種子，使每次獲取的随機序列不同。 Pd_z = new double*[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; Pd_z[z] = new double[num_data-1]; for( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] = (rand()%100000)*0.00001; norm_down += Pd_z[z][m]; } // for for( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] /= norm_down; } // for } // for cout \u0026lt;\u0026lt; \u0026#34;Init step1 done\\n\u0026#34;; Pw_z = new double*[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; Pw_z[z] = new double[num_words-1]; for( int w = 0; w \u0026lt; num_words; w++ ) { Pw_z[z][w] = (rand()%100000)*0.00001; norm_down += Pw_z[z][w]; } // for for( int w = 0; w \u0026lt; num_words; w++ ) { Pw_z[z][w] /= norm_down; } // for } // for cout \u0026lt;\u0026lt; \u0026#34;Init step2 done\\n\u0026#34;; Pz_dw = new double **[num_topics-1]; for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z] = new double *[num_data-1]; for ( int m = 0; m \u0026lt; num_data; m++ ) { int size = ( int ) all_docs[m].term_properties.size(); Pz_dw[z][m] = new double[size-1]; } // for() } // for() cout \u0026lt;\u0026lt; \u0026#34;Init step3 done\\n\u0026#34;; } // Init() void DoPLSA() { // start training for ( int i = 0; i \u0026lt; MAX_NUM_ITER; i++ ) { Estep(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Estep: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; Mstep_1(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Mstep_1: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; Mstep_2(); cout \u0026lt;\u0026lt; \u0026#34;iteration: Mstep_2: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; system( \u0026#34;pause\u0026#34; ); } // for() } // DoPLSA() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { a = 0.6; b = 0.25; output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_prob( file_name ); sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file, file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQueryFile() private: int num_data; // number of data int num_words; // number of words int num_topics; // number of topics double a, b; double **Pd_z = NULL; double **Pw_z = NULL; double ***Pz_dw = NULL; double **Dw_m = NULL; vector\u0026lt;TermProperty\u0026gt; read_a_doc_to_dictionary( string file_name, double \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;TermProperty\u0026gt; term_properties; TermProperty term_property; Word2doc word2doc; Dictionary doc_term; unordered_map\u0026lt;string, int\u0026gt; doc2dic; file.open( path_name, ios::in ); int position = 0; while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_property.term_name = term_name; auto it_dic = dictionary.find( term_name ); if ( it_dic != dictionary.end() ) { doc_term = it_dic -\u0026gt; second; doc_term.term_property.tf++; doc_term.term_property.prob = doc_term.term_property.tf / MAX_LENGTH; if ( doc_term.word2doc[doc_term.word2doc.size()-1].docID != num_file ) { word2doc.docID = num_file; word2doc.position = position; doc_term.word2doc.push_back( word2doc ); term_property.tf = 1; term_properties.push_back(term_property); doc2dic.insert(make_pair(term_name, position)); position++; } // if else { auto it = doc2dic.find( term_name ); int pos = it -\u0026gt; second; term_properties[pos].tf++; } // else it_dic -\u0026gt; second = doc_term; } // if else { term_property.tf = 1; term_property.prob = term_property.tf / MAX_LENGTH; word2doc.docID = num_file; word2doc.position = position; doc_term.term_property = term_property; doc_term.word2doc.push_back( word2doc ); doc_term.word2dic = all_words.size(); all_words.push_back( term_name ); dictionary.insert( make_pair( term_name, doc_term )); position++; term_properties.push_back(term_property); doc2dic.insert(make_pair(term_name, position)); } // else } // while for (int i = 0; i \u0026lt; term_properties.size(); i++ ) { term_properties[i].prob = term_properties[i].tf / doc_length; } // for file.close(); return term_properties; } // read_a_doc_to_dictionary() void Estep() { cout \u0026lt;\u0026lt; all_docs.size() \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; num_data \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; for ( int m = 0; m \u0026lt; num_data; m++ ) { for ( int position = 0; position \u0026lt; all_docs[m].term_properties.size(); position++ ) { double norm_down = 0.0; auto it_dic = dictionary.find( all_docs[m].term_properties[position].term_name ); if ( it_dic == dictionary.end() ) cout \u0026lt;\u0026lt; \u0026#34;cry\\n\u0026#34;; int w = it_dic -\u0026gt; second.word2dic; for ( int z = 0; z \u0026lt; num_topics; z++ ) { double val = Pd_z[z][m] * Pw_z[z][w]; Pz_dw[z][m][position] = val; norm_down += val; } // for() if ( norm_down != 0.0 ) { for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z][m][position] /= norm_down; } // for } // if else { for ( int z = 0; z \u0026lt; num_topics; z++ ) { Pz_dw[z][m][position] = 0.0; } // for } // else } // for() if ( m \u0026gt;= 14950 ) { cout \u0026lt;\u0026lt; all_docs[m].doc_name \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; all_docs[m].term_properties.size() \u0026lt;\u0026lt; \u0026#34; qq1\\n\u0026#34;; } // if } // for } // Estep() void Mstep_1() { // Pd_z[z][m] for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; for ( int m = 0; m \u0026lt; num_data; m++ ) { double sum = 0.0; for ( int position = 0; position \u0026lt; all_docs[m].term_properties.size(); position++ ) { auto it = dictionary.find( all_docs[m].term_properties[position].term_name ); sum += it -\u0026gt; second.term_property.tf * Pz_dw[z][m][position]; } // for Pd_z[z][m] = sum; norm_down += sum; } // for() if ( norm_down != 0 ) { for ( int m = 0; m \u0026lt; num_data; m++) { Pd_z[z][m] /= norm_down; } // for } // if else { for ( int m = 0; m \u0026lt; num_data; m++ ) { Pd_z[z][m] = 0; } // for } // else } // for() } // Mstep_1() void Mstep_2() { // Pw_z[z][w] for ( int z = 0; z \u0026lt; num_topics; z++ ) { double norm_down = 0.0; for ( int w = 0; w \u0026lt; num_words; w++ ) { double sum = 0.0; string word = all_words[w]; auto it = dictionary.find( word ); vector\u0026lt;Word2doc\u0026gt; word2docs = it -\u0026gt; second.word2doc; for ( int i = 0; i \u0026lt; word2docs.size(); i++ ) { int m = word2docs[i].docID; int position = word2docs[i].position; auto it_2 = dictionary.find( all_docs[m].term_properties[position].term_name ); sum += it_2 -\u0026gt; second.term_property.tf * Pz_dw[z][m][position]; } // for Pw_z[z][w] = sum; norm_down += sum; } // for if ( norm_down != 0 ) { for ( int w = 0; w \u0026lt; num_words; w++) { Pw_z[z][w] /= norm_down; } // for } // if else { for ( int w = 0; w \u0026lt; num_words; w++) { Pw_z[z][w] = 0; } // for } // else } // for } // Mstep_2() void calculate_query_prob( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; file.open( path_name, ios::in ); vector\u0026lt;string\u0026gt; all_query_term; while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; all_query_term.push_back( term_name ); } // while for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { double prob = 0.0; for ( int j = 0; j \u0026lt; all_query_term.size(); j++ ) { prob = prob * calculate_term_prob( all_docs[i], all_query_term[j] ); if ( prob == 0.0 ) break; } // for if ( prob != 0.0 ) prob = log10( prob ); all_docs[i].ranking = prob; } // for } // calculate_query_prob() double calculate_term_prob( Document doc, string query_term ) { double prob = 0.0, temp_prob = 0.0; auto it_dic = dictionary.find( query_term ); if ( it_dic == dictionary.end() ) return prob; int m = doc.docID; int w = it_dic -\u0026gt; second.word2dic; vector\u0026lt;TermProperty\u0026gt; term_properties = doc.term_properties; TermProperty term_property; // first term auto it_doc = find_if( term_properties.begin(), term_properties.end(), [query_term] (const TermProperty\u0026amp; s) { return s.term_name == query_term; } ); if ( it_doc != term_properties.end() ) { term_property = *it_doc; prob += term_property.prob * a; } // if // second term and third term for ( int z = 0; z \u0026lt; num_topics; z++ ) { temp_prob += Pd_z[z][m] * Pw_z[z][w] ; } // for prob += temp_prob * b; prob += ( 1.0 - a - b ) * it_dic -\u0026gt; second.term_property.prob; return prob; } // calculate_term_prob() void output_ranking( ofstream \u0026amp; output_file, string query_file ) { query_file.erase( query_file.end() -4, query_file.end() ); output_file \u0026lt;\u0026lt; query_file \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() }; // PLSA int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; PLSA plsa; plsa.ReadAllDocuments(); system(\u0026#34;pause\u0026#34;); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Initing.\\n\u0026#34;; plsa.Init(); cout \u0026lt;\u0026lt; \u0026#34;End Initing.\\n\u0026#34;; plsa.DoPLSA(); cout \u0026lt;\u0026lt; \u0026#34;Done Training.\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); plsa.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW5 - Query Modeling (Rocchio Algorithm) Query Modeling (Rocchio Algorithm)參數調法 First Information Retrieval: BM25 k1= 0.8 b = 0.7 k3 = 100.0\nSecond Information Retrieval: VSM\nRocchio algorithm: a = 2.5 b = 0.4 relevant doc = 10 non-relevant doc = 1 (c = 0.1)\n如果First Information Retrieval使用PLSA能獲得較高的分數，但需要的memory極大。\nQuery Modeling (Rocchio Algorithm)理論心得 由於query所蒐集的資料量太少，導致資料檢索前所獲得的資訊太少，而透過使用Pseudo-Relevance Feedback的過程得到加強後的query term，再進行第二次資料檢索，能獲得較好的結果。Query Modeling利用此核心精神下去實作，而本次實作Pseudo-Relevance Feedback的方法為Rocchio Algorithm。\nQuery Modeling (Rocchio Algorithm)實作心得 程式流程為scanning documents ➡ calculate every word TF-IDF ➡Run BM25/PLSA for First IR ➡ using Rocchio for updating new query weight ➡ Run VSM for Second IR\nIn step 1、2: 存取所有doc並記錄local、global TF到對應容器中\nIn step 3: 計算BM25並排序前10篇文章為relevant docs\nIn step 4: update query vector on relevant docs , normalize new query weight and add original query weight ( a = 2.5, b =0.4 ) if using non-relevant doc ( c =0.1)\nIn step 5: normalize every doc weight and do VSM separately.\n在實作過程中發現Relevant Doc篇數的設定也會有很大的影響, 不一定多比較好, 太少也可能不行，10~15篇是獲得此資料集較理想的Doc數。而參數b不能設得太大否則會讓new query weight跟original query weight的權重變得模糊，效果不好，最後在統一vector長度時，當分子其中一項為0時可以不算來增加運行速度。\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 # include \u0026lt;fstream\u0026gt; # include \u0026lt;string\u0026gt; # include \u0026lt;vector\u0026gt; # include \u0026lt;iostream\u0026gt; # include \u0026lt;algorithm\u0026gt; # include \u0026lt;io.h\u0026gt; # include \u0026lt;cmath\u0026gt; # include \u0026lt;unordered_map\u0026gt; using namespace std; struct TermProperty { int tf[14954] = { 0 }; int num_of_article_including; double idf; double weight; }; // TermProperty # define MAX_NUM_DOCS 14955.000 struct Dictionary { string term_name; string term_in_last_file_name; TermProperty term_property; bool operator==(const Dictionary\u0026amp; p) { return this-\u0026gt;term_name == p.term_name; } // bool inline friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, Dictionary\u0026amp; p) { os \u0026lt;\u0026lt; p.term_name; return os; } }; // Dictionary struct Document { string doc_name; float doc_length; vector\u0026lt;string\u0026gt; all_term_names_in_file; vector\u0026lt;double\u0026gt; all_term_tf_in_file; vector\u0026lt;double\u0026gt; all_term_ni_in_file; vector\u0026lt;double\u0026gt; all_term_weight_in_file; double ranking; }; // Document struct Query { string file_name; vector\u0026lt;int\u0026gt; tf; vector\u0026lt;string\u0026gt; term_names; vector\u0026lt;double\u0026gt; term_weights; }; // Query struct compare_key { inline bool operator() (const Document\u0026amp; struct1, const Document\u0026amp; struct2) { if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026gt; struct2.doc_name ) { return 1; } // if else if ( struct1.ranking == struct2.ranking \u0026amp;\u0026amp; struct1.doc_name \u0026lt; struct2.doc_name ) { return 0; } // else if return ( struct1.ranking \u0026gt; struct2.ranking ); } // }; class Doc_Scanner { public: unordered_map\u0026lt;string, Dictionary\u0026gt; doc_terms_map; vector\u0026lt;Document\u0026gt; all_docs; double avg_doclen; int num_file; vector\u0026lt;Query\u0026gt; all_queries; void ReadAllDocuments() { string doc_files_path = \u0026#34;./docs/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( doc_files_path.c_str(), \u0026amp;file_info ); int k = handle; float doc_length = 0.0; avg_doclen = 0.0; if ( handle == -1 ) { cout \u0026lt;\u0026lt; \u0026#34;Read Docs Error. Please check your docs path.\\n\u0026#34;; } // if else { num_file = 0; while ( k != -1 ) { string file_name = file_info.name; Document document; document.doc_name = file_name; document.ranking = -1; document.all_term_names_in_file = read_a_doc_to_dictionary( file_name, doc_length ); document.doc_length = doc_length; avg_doclen = avg_doclen + doc_length; all_docs.push_back( document ); num_file++; k = _findnext( handle, \u0026amp;file_info); doc_length = 0.0; } // while } // else avg_doclen = avg_doclen / MAX_NUM_DOCS; _findclose( handle ); } // ReadAllDocuments() void CalculateDocsTermWeight() { for ( int i = 0; i \u0026lt; all_docs.size(); i++ ) { for ( int j = 0; j \u0026lt; all_docs[i].all_term_names_in_file.size(); j++ ) { auto iterator = doc_terms_map.find( all_docs[i].all_term_names_in_file[j] ); Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.idf = MAX_NUM_DOCS / doc_term.term_property.num_of_article_including; doc_term.term_property.weight = ( doc_term.term_property.tf[i] ) * log10( doc_term.term_property.idf ); all_docs[i].all_term_tf_in_file.push_back( doc_term.term_property.tf[i] ); all_docs[i].all_term_ni_in_file.push_back( doc_term.term_property.num_of_article_including ); all_docs[i].all_term_weight_in_file.push_back( doc_term.term_property.weight ); iterator -\u0026gt; second = doc_term; } // for } // for } // CalculateDocsTermWeight() void CalculateEveryQueryFile() { string query_files_path = \u0026#34;./queries/*.txt\u0026#34;; struct _finddata_t file_info; int handle = _findfirst( query_files_path.c_str(), \u0026amp;file_info ); int k = handle; while ( k != -1 ) { string file_name = file_info.name; calculate_query_term_weight( file_name ); k = _findnext( handle, \u0026amp;file_info ); } // while } // RankEveryQuery() void RankEveryQueryFile( ofstream \u0026amp; output_file ) { output_file \u0026lt;\u0026lt; \u0026#34;Query,RetrievedDocuments\\n\u0026#34;; k1 = 1.4; b = 0.5; q = 0.4; k3 = 1000; for( int i = 0; i \u0026lt; all_queries.size(); i++ ) { query_file = all_queries[i]; for( int j = 0; j \u0026lt; all_docs.size(); j++ ) { all_docs[j].ranking = rank_doc_BM25( all_docs[j] ); } // for sort( all_docs.begin(), all_docs.end(), compare_key() ); output_ranking( output_file ); } // for } // RankEveryQueryFile() private: Query query_file; vector\u0026lt;double\u0026gt; query_weights; vector\u0026lt;double\u0026gt; doc_weights; double k1, b, k3, q; void calculate_query_term_weight( string query_file_name ) { fstream file; string term_name; string path_name = \u0026#34;./queries/\u0026#34; + query_file_name; Query query; double weight; query.file_name = query_file_name; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; weight = 2.5 * log10( doc_term.term_property.idf ); query.term_weights.push_back( weight ); query.term_names.push_back( term_name ); } // if } // while file.close(); all_queries.push_back( query ); } // calculate_query_term_weight() double rank_doc_BM25( Document doc ) { double doc_score = 0.0, doc_temp = 0.0; for( int i = 0; i \u0026lt; query_file.term_names.size(); i++ ) { auto it = find( doc.all_term_names_in_file.begin(), doc.all_term_names_in_file.end(), query_file.term_names[i] ); if ( it != doc.all_term_names_in_file.end() ) { int pos = it - doc.all_term_names_in_file.begin(); doc_temp = doc.all_term_tf_in_file[pos] / ( ( 1 - b ) + b * ( doc.doc_length / avg_doclen ) ); if ( doc_temp \u0026gt; 0 ) { doc_temp = ( ( k1 + 1.0 ) * ( doc_temp + q ) ) / ( k1 + doc_temp + q ); } // if else { doc_temp = 0; } // else doc_score = doc_score + doc_temp; doc_score = doc_score * ( ( k3 + 1.0 ) * 1.0 / ( k3 + 1.0 ) ); auto iterator = doc_terms_map.find( query_file.term_names[i] ); Dictionary doc_term = iterator -\u0026gt; second; doc_score = doc_score * log10( ( MAX_NUM_DOCS - doc_term.term_property.num_of_article_including + 0.5 ) / ( doc_term.term_property.num_of_article_including + 0.5 ) ); } // if doc_score = doc_score + doc_temp; doc_temp = 0.0; } // for() return doc_score; } // rank_doc_BM25() void output_ranking( ofstream \u0026amp; output_file ) { string query_name = query_file.file_name; query_name.erase( query_name.end() -4, query_name.end() ); output_file \u0026lt;\u0026lt; query_name \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; for( int i = 0; i \u0026lt; all_docs.size(); i++ ) { string term_name = all_docs[i].doc_name; term_name.erase( term_name.end() -4, term_name.end() ); output_file \u0026lt;\u0026lt; term_name \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } // for output_file \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // output_ranking() vector\u0026lt;string\u0026gt; read_a_doc_to_dictionary( string file_name, float \u0026amp; doc_length ) { fstream file; string term_name; string path_name = \u0026#34;./docs/\u0026#34; + file_name; vector\u0026lt;string\u0026gt; term_names; file.open( path_name, ios::in ); while ( file.peek() != EOF ) { file \u0026gt;\u0026gt; term_name; doc_length++; term_name = add_index_term_to_dictionary( term_name, file_name ); if ( term_name != \u0026#34;\u0026#34; ) term_names.push_back( term_name ); } // while file.close(); return term_names; } // read_a_doc_to_dictionary() string add_index_term_to_dictionary( string term_name, string file_name ) { auto iterator = doc_terms_map.find( term_name ); if ( iterator != doc_terms_map.end() ) { Dictionary doc_term = iterator -\u0026gt; second; doc_term.term_property.tf[num_file]++; if ( doc_term.term_in_last_file_name != file_name ) { doc_term.term_property.num_of_article_including++; doc_term.term_in_last_file_name = file_name; } // if else { term_name = \u0026#34;\u0026#34;; } // else iterator -\u0026gt; second = doc_term; } // if else { TermProperty term_property; term_property.tf[num_file] = 1; term_property.num_of_article_including = 1; Dictionary doc_term; doc_term.term_name = term_name; doc_term.term_in_last_file_name = file_name; doc_term.term_property = term_property; doc_terms_map.insert( make_pair( term_name, doc_term )); } // else return term_name; } // add_index_term_to_dictionary() }; // Doc_Scanner int main() { cout \u0026lt;\u0026lt; \u0026#34;Start scanning documents.\\n\u0026#34;; Doc_Scanner doc_scanner; doc_scanner.ReadAllDocuments(); doc_scanner.CalculateDocsTermWeight(); cout \u0026lt;\u0026lt; \u0026#34;Scanning documents done.\\n\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Start Calulate Query Weight\\n\u0026#34;; // PrintVector( doc_terms ); doc_scanner.CalculateEveryQueryFile(); cout \u0026lt;\u0026lt; \u0026#34;End Calulate Query Weight\\n\u0026#34;; ofstream output_file( \u0026#34;output_csv.csv\u0026#34; ); doc_scanner.RankEveryQueryFile( output_file ); cout \u0026lt;\u0026lt; \u0026#34;End Program\\n\u0026#34;; } // main() HW6 - BERT Model BERT 參數調法 BERT 理論心得 BERT(雙向Transformer編碼表達)\n主要是以兩種預訓練的方式來建立語言模型。 讓model分辨句⼦之間上下⽂之關係 簡而知BERT預訓練模型需要做兩個主要任務:\n第⼀Task: 判斷兩句話是否真的相鄰(Binary classification)\n第⼆Task: 預測被mask之單詞(Multi-class classification)\n(三) BERT實作心得\n使用助教提供的baseline code下去實現，其主要呼叫的地方可以查官方API實作出來，利用此model回傳loss及logits值。\n而TO-DO地方中需要回傳BM25的2D array scores，可以利用助教的方法實現\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 #!/usr/bin/env python # coding: utf-8 # In[ ]: from time import time from datetime import timedelta from copy import deepcopy import random import numpy as np import pandas as pd from ml_metrics import mapk import torch from torch.optim import AdamW from torch.nn.utils.rnn import pad_sequence from torch.utils.data import Dataset, DataLoader from transformers import BertTokenizerFast, BertForMultipleChoice # Random seed SEED = 42 random.seed(SEED) np.random.seed(SEED) torch.manual_seed(SEED) # CUDA device use_cuda_device = 0 torch.cuda.set_device(use_cuda_device) print(\u0026#34;Using CUDA device: %d\u0026#34; % torch.cuda.current_device()) # ## Settings # In[ ]: # Input files document_csv_path = \u0026#39;../input/ntust-ir2020-homework6/documents.csv\u0026#39; training_csv_path = \u0026#39;../input/ntust-ir2020-homework6/train_queries.csv\u0026#39; testing_csv_path = \u0026#39;../input/ntust-ir2020-homework6/test_queries.csv\u0026#39; # Input limitation max_query_length = 64 max_input_length = 512 num_negatives = 3 # num. of negative documents to pair with a positive document # Model finetuning model_name_or_path = \u0026#34;bert-base-uncased\u0026#34; max_epochs = 1 learning_rate = 3e-5 dev_set_ratio = 0.2 # make a ratio of training set as development set for rescoring weight sniffing max_patience = 0 # earlystop if avg. loss on development set doesn\u0026#39;t decrease for num. of epochs batch_size = 2 # num. of inputs = 8 requires ~9200 MB VRAM (num. of inputs = batch_size * (num_negatives + 1)) num_workers = 2 # num. of jobs for pytorch dataloader # Save paths save_model_path = \u0026#34;models/bert_base_uncased\u0026#34; # assign `None` for not saving the model save_submission_path = \u0026#34;bm25_bert_rescoring.csv\u0026#34; K = 1000 # for MAP@K # ## Preparing # In[ ]: # Build and save BERT tokenizer tokenizer = BertTokenizerFast.from_pretrained(model_name_or_path) if save_model_path is not None: save_tokenizer_path = \u0026#34;%s/tokenizer\u0026#34; % (save_model_path) tokenizer.save_pretrained(save_tokenizer_path) # Collect mapping of all document id and text doc_id_to_text = {} doc_df = pd.read_csv(document_csv_path) doc_df.fillna(\u0026#34;\u0026lt;Empty Document\u0026gt;\u0026#34;, inplace=True) id_text_pair = zip(doc_df[\u0026#34;doc_id\u0026#34;], doc_df[\u0026#34;doc_text\u0026#34;]) for i, pair in enumerate(id_text_pair, start=1): doc_id, doc_text = pair doc_id_to_text[doc_id] = doc_text print(\u0026#34;Progress: %d/%d\\r\u0026#34; % (i, len(doc_df)), end=\u0026#39;\u0026#39;) doc_df.tail() # # Training # ## Split a ratio of training set as development set # In[ ]: train_df = pd.read_csv(training_csv_path) dev_df, train_df = np.split(train_df, [int(dev_set_ratio*len(train_df))]) dev_df.reset_index(drop=True, inplace=True) train_df.reset_index(drop=True, inplace=True) print(\u0026#34;train_df shape:\u0026#34;, train_df.shape) print(\u0026#34;dev_df shape:\u0026#34;, dev_df.shape) train_df.tail() # ## Build instances for training/development set # In[ ]: get_ipython().run_cell_magic(\u0026#39;time\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;doc_id_to_token_ids = {}\\n\\n\\ndef preprocess_df(df):\\n \\\u0026#39;\\\u0026#39;\\\u0026#39; Preprocess DataFrame into training instances for BERT. \\\u0026#39;\\\u0026#39;\\\u0026#39;\\n instances = []\\n \\n # Parse CSV\\n for i, row in df.iterrows():\\n query_id, query_text, pos_doc_ids, bm25_top1000, _ = row\\n pos_doc_id_list = pos_doc_ids.split()\\n pos_doc_id_set = set(pos_doc_id_list)\\n bm25_top1000_list = bm25_top1000.split()\\n bm25_top1000_set = set(bm25_top1000_list)\\n\\n # Pair BM25 neg. with pos. samples\\n labeled_pos_neg_list = []\\n for pos_doc_id in pos_doc_id_list:\\n neg_doc_id_set = bm25_top1000_set - pos_doc_id_set\\n neg_doc_ids = random.sample(neg_doc_id_set, num_negatives)\\n pos_position = random.randint(0, num_negatives)\\n pos_neg_doc_ids = neg_doc_ids\\n pos_neg_doc_ids.insert(pos_position, pos_doc_id)\\n labeled_sample = (pos_neg_doc_ids, pos_position)\\n labeled_pos_neg_list.append(labeled_sample)\\n \\n # Make query tokens for BERT\\n query_tokens = tokenizer.tokenize(query_text)\\n if len(query_tokens) \u0026gt; max_query_length: # truncation\\n query_tokens = query_tokens[:max_query_length]\\n query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\\n query_token_ids.insert(0, tokenizer.cls_token_id)\\n query_token_ids.append(tokenizer.sep_token_id)\\n\\n # Make input instances for all query/doc pairs\\n for doc_ids, label in labeled_pos_neg_list:\\n paired_input_ids = []\\n paired_attention_mask = []\\n paired_token_type_ids = []\\n \\n # Merge all pos/neg inputs as a single sample\\n for doc_id in doc_ids:\\n if doc_id in doc_id_to_token_ids:\\n doc_token_ids = doc_id_to_token_ids[doc_id]\\n else:\\n doc_text = doc_id_to_text[doc_id]\\n doc_tokens = tokenizer.tokenize(doc_text)\\n doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\\n doc_id_to_token_ids[doc_id] = doc_token_ids\\n doc_token_ids.append(tokenizer.sep_token_id)\\n\\n # make input sequences for BERT\\n input_ids = query_token_ids + doc_token_ids\\n token_type_ids = [0 for token_id in query_token_ids]\\n token_type_ids.extend(1 for token_id in doc_token_ids)\\n if len(input_ids) \u0026gt; max_input_length: # truncation\\n input_ids = input_ids[:max_input_length]\\n token_type_ids = token_type_ids[:max_input_length]\\n attention_mask = [1 for token_id in input_ids]\\n \\n # convert and collect inputs as tensors\\n input_ids = torch.LongTensor(input_ids)\\n attention_mask = torch.FloatTensor(attention_mask)\\n token_type_ids = torch.LongTensor(token_type_ids)\\n paired_input_ids.append(input_ids)\\n paired_attention_mask.append(attention_mask)\\n paired_token_type_ids.append(token_type_ids)\\n label = torch.LongTensor([label]).squeeze()\\n \\n # Pre-pad tensor pairs for efficiency\\n paired_input_ids = pad_sequence(paired_input_ids, batch_first=True)\\n paired_attention_mask = pad_sequence(paired_attention_mask, batch_first=True)\\n paired_token_type_ids = pad_sequence(paired_token_type_ids, batch_first=True)\\n\\n # collect all inputs as a dictionary\\n instance = {}\\n instance[\\\u0026#39;input_ids\\\u0026#39;] = paired_input_ids.T # transpose for code efficiency\\n instance[\\\u0026#39;attention_mask\\\u0026#39;] = paired_attention_mask.T\\n instance[\\\u0026#39;token_type_ids\\\u0026#39;] = paired_token_type_ids.T\\n instance[\\\u0026#39;label\\\u0026#39;] = label\\n instances.append(instance)\\n\\n print(\u0026#34;Progress: %d/%d\\\\r\u0026#34; % (i+1, len(df)), end=\\\u0026#39;\\\u0026#39;)\\n print()\\n return instances\\n\\ntrain_instances = preprocess_df(train_df)\\ndev_instances = preprocess_df(dev_df)\\n\\nprint(\u0026#34;num. train_instances: %d\u0026#34; % len(train_instances))\\nprint(\u0026#34;num. dev_instances: %d\u0026#34; % len(dev_instances))\\nprint(\u0026#34;input_ids.T shape:\u0026#34;, train_instances[0][\\\u0026#39;input_ids\\\u0026#39;].T.shape)\\ntrain_instances[0][\\\u0026#39;input_ids\\\u0026#39;].T\\n\u0026#39;) # ## Build dataset and dataloader for PyTorch # In[ ]: class TrainingDataset(Dataset): def __init__(self, instances): self.instances = instances def __len__(self): return len(self.instances) def __getitem__(self, i): instance = self.instances[i] input_ids = instance[\u0026#39;input_ids\u0026#39;] attention_mask = instance[\u0026#39;attention_mask\u0026#39;] token_type_ids = instance[\u0026#39;token_type_ids\u0026#39;] label = instance[\u0026#39;label\u0026#39;] return input_ids, attention_mask, token_type_ids, label def get_train_dataloader(instances, batch_size=2, num_workers=4): def collate_fn(batch): input_ids, attention_mask, token_type_ids, labels = zip(*batch) input_ids = pad_sequence(input_ids, batch_first=True).transpose(1,2).contiguous() # re-transpose attention_mask = pad_sequence(attention_mask, batch_first=True).transpose(1,2).contiguous() token_type_ids = pad_sequence(token_type_ids, batch_first=True).transpose(1,2).contiguous() labels = torch.stack(labels) return input_ids, attention_mask, token_type_ids, labels dataset = TrainingDataset(instances) dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, \\ batch_size=batch_size, num_workers=num_workers) return dataloader # Demo dataloader = get_train_dataloader(train_instances) for batch in dataloader: input_ids, attention_mask, token_type_ids, labels = batch break print(input_ids.shape) input_ids # ## Initialize and finetune BERT # In[ ]: model = BertForMultipleChoice.from_pretrained(model_name_or_path) model.cuda() optimizer = AdamW(model.parameters(), lr=learning_rate) optimizer.zero_grad() # ### (TO-DO!) Define validation function for earlystopping # In[ ]: def validate(model, instances): total_loss = 0 model.eval() dataloader = get_train_dataloader(instances, batch_size=batch_size, num_workers=num_workers) for batch in dataloader: batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids, labels = batch \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice) (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs) 2. Sum up the loss of all dev-set samples (Hint: The built-in loss is averaged, so you should multiply it with the batch size) \u0026#39;\u0026#39;\u0026#39; with torch.no_grad(): ### 1. insert_missing_code loss = model( input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels, return_dict=1 ).loss ### 2. insert_missing_code total_loss += loss * batch_size avg_loss = total_loss / len(instances) return avg_loss # ### (TO-DO!) Let\u0026#39;s train this beeg boy ;-) # In[ ]: patience, best_dev_loss = 0, 1e10 best_state_dict = model.state_dict() start_time = time() dataloader = get_train_dataloader(train_instances, batch_size=batch_size, num_workers=num_workers) for epoch in range(1, max_epochs+1): model.train() for i, batch in enumerate(dataloader, start=1): batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids, labels = batch # Backpropogation \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice) (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs) 2. Perform backpropogation on the loss (i.e. compute gradients) 3. Optimize the model. (Hint: These two lines of codes can be found in PyTorch tutorial) \u0026#39;\u0026#39;\u0026#39; ### 1. insert_missing_code loss = model( input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels, return_dict=1 ).loss ### 2. insert_missing_code loss.backward() ### 3. insert_missing_code optimizer.step() optimizer.zero_grad() # Progress bar with timer ;-) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Epoch: %d/%d | Batch: %d/%d | loss=%.5f | %s \\r\u0026#34; \\ % (epoch, max_epochs, i, len(dataloader), loss, elapsed_time), end=\u0026#39;\u0026#39;) # Save parameters of each epoch if save_model_path is not None: save_checkpoint_path = \u0026#34;%s/epoch_%d\u0026#34; % (save_model_path, epoch) model.save_pretrained(save_checkpoint_path) # Get avg. loss on development set print(\u0026#34;Epoch: %d/%d | Validating... \\r\u0026#34; % (epoch, max_epochs), end=\u0026#39;\u0026#39;) dev_loss = validate(model, dev_instances) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Epoch: %d/%d | dev_loss=%.5f | %s \u0026#34; \\ % (epoch, max_epochs, dev_loss, elapsed_time)) # Track best checkpoint and earlystop patience if dev_loss \u0026lt; best_dev_loss: patience = 0 best_dev_loss = dev_loss best_state_dict = deepcopy(model.state_dict()) if save_model_path is not None: model.save_pretrained(save_model_path) else: patience += 1 if patience \u0026gt; max_patience: print(\u0026#39;Earlystop at epoch %d\u0026#39; % epoch) break # Restore parameters with best loss on development set model.load_state_dict(best_state_dict) # # Testing # In[ ]: class TestingDataset(Dataset): def __init__(self, instances): self.instances = instances def __len__(self): return len(self.instances) def __getitem__(self, i): instance = self.instances[i] input_ids = instance[\u0026#39;input_ids\u0026#39;] attention_mask = instance[\u0026#39;attention_mask\u0026#39;] token_type_ids = instance[\u0026#39;token_type_ids\u0026#39;] input_ids = torch.LongTensor(input_ids) attention_mask = torch.FloatTensor(attention_mask) token_type_ids = torch.LongTensor(token_type_ids) return input_ids, attention_mask, token_type_ids, def get_test_dataloader(instances, batch_size=8, num_workers=4): def collate_fn(batch): input_ids, attention_mask, token_type_ids = zip(*batch) input_ids = pad_sequence(input_ids, batch_first=True).unsqueeze(1) # predict as single choice attention_mask = pad_sequence(attention_mask, batch_first=True).unsqueeze(1) token_type_ids = pad_sequence(token_type_ids, batch_first=True).unsqueeze(1) return input_ids, attention_mask, token_type_ids dataset = TestingDataset(instances) dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=False, \\ batch_size=batch_size, num_workers=num_workers) return dataloader # ## (TO-DO!) Define function to predict BERT scores # In[ ]: def predict_query_doc_scores(model, df): model.eval() start_time = time() # Parse CSV query_id_list = df[\u0026#34;query_id\u0026#34;] query_text_list = df[\u0026#34;query_text\u0026#34;] bm25_top1000_list = df[\u0026#34;bm25_top1000\u0026#34;] # Treat {1 query, K documents} as a dataset for prediction query_doc_scores = [] query_doc_ids = [] rows = zip(query_id_list, query_text_list, bm25_top1000_list) for qi, row in enumerate(rows, start=1): query_id, query_text, bm25_top1000 = row bm25_doc_id_list = bm25_top1000.split() query_doc_ids.append(bm25_doc_id_list) ################################################# # Collect all instances of query/doc pairs ################################################# query_instances = [] # Make query tokens for BERT query_tokens = tokenizer.tokenize(query_text) if len(query_tokens) \u0026gt; max_query_length: # truncation query_tokens = query_tokens[:max_query_length] query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens) query_token_ids.insert(0, tokenizer.cls_token_id) query_token_ids.append(tokenizer.sep_token_id) # Make input instances for all query/doc pairs for i, doc_id in enumerate(bm25_doc_id_list, start=1): if doc_id in doc_id_to_token_ids: doc_token_ids = doc_id_to_token_ids[doc_id] else: doc_text = doc_id_to_text[doc_id] doc_tokens = tokenizer.tokenize(doc_text) doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens) doc_id_to_token_ids[doc_id] = doc_token_ids doc_token_ids.append(tokenizer.sep_token_id) # make input sequences for BERT input_ids = query_token_ids + doc_token_ids token_type_ids = [0 for token_id in query_token_ids] token_type_ids.extend(1 for token_id in doc_token_ids) if len(input_ids) \u0026gt; max_input_length: # truncation input_ids = input_ids[:max_input_length] token_type_ids = token_type_ids[:max_input_length] attention_mask = [1 for token_id in input_ids] # convert and collect inputs as tensors input_ids = torch.LongTensor(input_ids) attention_mask = torch.FloatTensor(attention_mask) token_type_ids = torch.LongTensor(token_type_ids) # collect all inputs as a dictionary instance = {} instance[\u0026#39;input_ids\u0026#39;] = input_ids instance[\u0026#39;attention_mask\u0026#39;] = attention_mask instance[\u0026#39;token_type_ids\u0026#39;] = token_type_ids query_instances.append(instance) ################################################################# # Predict relevance scores for all BM25-top-1000 documents ################################################################# doc_scores = np.empty((0,1)) # Predict scores for each document dataloader = get_test_dataloader(query_instances, batch_size=batch_size*(num_negatives+1), num_workers=num_workers) for di, batch in enumerate(dataloader, start=1): batch = (tensor.cuda() for tensor in batch) input_ids, attention_mask, token_type_ids = batch \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the logits as relevance scores (using the same function of how you compute built-in loss) (Hint: You need to call a function of model which takes all the 3 tensors in the batch as inputs) 2. The scores are still on GPU. Reallocate them on CPU, and convert into numpy arrays. (Hint: You need to call two functions on the `scores` tensors. You can find them in PyTorch tutorial.) \u0026#39;\u0026#39;\u0026#39; with torch.no_grad(): ### 1. insert_missing_code_to_compute_logits ### scores = model( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True ).logits # merge all scores into a big numpy array ### step 2. insert_missing_function_1()###.###insert_missing_function_2() scores = scores.cpu().numpy() doc_scores = np.vstack((doc_scores, scores)) ## 新增new row # Progress bar with timer ;-) elapsed_time = time() - start_time elapsed_time = timedelta(seconds=int(elapsed_time)) print(\u0026#34;Query: %d/%d | Progress: %d/%d | %s \\r\u0026#34; \\ % (qi, len(df), di, len(dataloader), elapsed_time), end=\u0026#39;\u0026#39;) # merge all query/BM25 document pair scores query_doc_scores.append(doc_scores) query_doc_scores = np.hstack(query_doc_scores).T print() return query_doc_scores, query_doc_ids # In[ ]: # ## (TO-DO!) Find best weight of BERT for BM25 rescoring on training set # In[ ]: dev_query_doc_scores, dev_query_doc_ids = predict_query_doc_scores(model, dev_df) print(\u0026#39;---- Grid search weight for \u0026#34;BM25 + weight * BERT\u0026#34; ----\u0026#39;) best_map_score, best_bert_weight = -100, 0.0 bert_scores = dev_query_doc_scores n_query = dev_query_doc_scores.shape[0] # Get MAP@K of BM25 baseline query_pos_doc_ids = dev_df[\u0026#39;pos_doc_ids\u0026#39;].values.tolist() actual = [doc_ids.split() for doc_ids in query_pos_doc_ids] bm25_predicted = [doc_id_list[:K] for doc_id_list in dev_query_doc_ids] map_score = mapk(actual, bm25_predicted, k=K) best_map_score = map_score print(\u0026#34;weight=%.1f: %.5f (BM25 baseline)\u0026#34; % (0, 100*map_score)) # Collect BM25 scores into same format of BERT scores \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Convert the BM25 top-1000 scores into 2d numpy arrays 2. BM25 scores should have the same shape and orders as `dev_query_doc_scores` (i.e. BERT scores) (Hint: If there are 24 dev-set queries, the shape should be (24, 1000) ) \u0026#39;\u0026#39;\u0026#39; ### 2. insert_whatever_you_want_to_meet_the_requirement_in_step2. bm25_scores = [scores.split() for scores in dev_df[\u0026#34;bm25_top1000_scores\u0026#34;]] bm25_scores = [[float(score) for score in scores] for scores in bm25_scores] bm25_scores = np.array(bm25_scores) # Grid search for BM25 + BERT rescoring low_bound, high_bound, scale = 0, 5, 1000 grids = [i / scale for i in range(low_bound * scale+1, high_bound * scale+1)] for weight in grids: \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the weighted scores using `bm25_scores`, `weight`, and `bert_scores` \u0026#39;\u0026#39;\u0026#39; weighted_scores = bm25_scores + weight * bert_scores ### 1. insert_missing_code ### # sort index and map to document ids as output rescore_argsort = np.flip(weighted_scores.argsort(), axis=1) predicted = [] for i in range(n_query): # num. of queries predicted.append([dev_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K]) map_score = mapk(actual, predicted, k=K) # show part of results for human evaluation if weight * 10 % 2 == 0: print(\u0026#34;weight=%.1f: %.5f\u0026#34; % (weight, 100*map_score)) # track weight with best MAP@10 if map_score \u0026gt; best_map_score: best_map_score = map_score best_bert_weight = weight print(\u0026#34;\\nHighest MAP@%d = %.5f found at weight=%.3f\u0026#34; % (K, 100*best_map_score, best_bert_weight)) # ## (TO-DO!) Rescore testing set with BERT for submission # In[ ]: # Predict BERT scores for testing set test_df = pd.read_csv(testing_csv_path) query_id_list = test_df[\u0026#34;query_id\u0026#34;] n_query = len(query_id_list) test_query_doc_scores, test_query_doc_ids = predict_query_doc_scores(model, test_df) bert_scores = test_query_doc_scores # In[ ]: # Rescore query/document score with BM25 + BERT bm25_scores = [scores.split() for scores in test_df[\u0026#34;bm25_top1000_scores\u0026#34;]] # parse into 2d list of string bm25_scores = [[float(score) for score in scores] for scores in bm25_scores] # convert to float bm25_scores = np.array(bm25_scores) \u0026#39;\u0026#39;\u0026#39; TO-DO: 1. Compute the weighted scores using `bm25_scores`, `best_bert_weight`, and `bert_scores` \u0026#39;\u0026#39;\u0026#39; ### 1. insesrt_missing_code ### weighted_scores = bm25_scores + best_bert_weight * bert_scores # Rerank document ids with new scores rescore_argsort = np.flip(weighted_scores.argsort(), axis=1) ranked_doc_id_list = [] for i in range(n_query): # num. of queries ranked_doc_id_list.append([test_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K]) ranked_doc_ids = [\u0026#39; \u0026#39;.join(doc_id_list) for doc_id_list in ranked_doc_id_list] # Save reranked results for submission data = {\u0026#39;query_id\u0026#39;: query_id_list, \u0026#39;ranked_doc_ids\u0026#39;: ranked_doc_ids} submission_df = pd.DataFrame(data) submission_df.reset_index(drop=True, inplace=True) submission_df.to_csv(save_submission_path, index=False) print(\u0026#34;Saved submission file as `%s`\u0026#34; % save_submission_path) ","permalink":"https://jonathan-tw.github.io/posts/%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98ntust-information-retrieval/","summary":"HW1 - Vector Space Model TF、IDF計算方式: 一開始會讀取document的資料前處理，先去做一些normalized，讓lexicon不要有重複的term，而為了讓程式能在1分鐘內跑完，lexicon使用c++的unordered_map下去存，unordered_map使用hash func","title":"(課程筆記)(NTUST) Information Retrieval"},{"content":"Layered Outline What\u0026rsquo;s a protocol?\nprotocols define format, order of messages sent and received among network entities, and actions taken on message transmission, receipt\nTCP/IP 5 Layer Application Layer HTTP、DHCP、RPC、P2P、DNS Transport Layer UDP、TCP Network Layer (IP) IP、ICMP、OSPF、BGP Data-link Layer (MAC) MAC、VLAN、STP Physical Layer 網路挑線 ISO/OSI 7 Layer  Presentation: allow applications to interpret meaning of data. example: encryption, compression, machine-specific conventions.  Session: synchronization, checkpointing, recovery of data exchange.\n只要是在網絡上跑的包，都是完整的。可以有下層沒上層，絕對不可能有上層沒下層。 IP address: 一個網卡在網絡世界的通訊地址，相當於我們現實世界的門牌號碼。 MAC address: 身份證，區域網內通信用。\nMac在data link layer使用，IP在network layer使用，IP位置在區域網內可能發生改變，唯一性的MAC需要。 mac位置全球唯一，固化在網卡裡。OS識別出來的mac位置可以更改，他是一個string，我們常說修改的mac指的是修改電腦註冊表中的紀錄。 OpenFlow: Flow Table Entries operates between controller, switch.\nApplication Layer HTTP ( Hyper Text Transfer Protocol ) Uses TCP; stateless. non-persistent HTTP ➡ at most one object sent over TCP connection,connection then closed. persistent HTTP ➡ multiple objects can be sent over single TCP connection between client, server. HTTP response time = 2RTT (建立TCP連線、請求和接收物件)+ file transmission time\nserver希望能識別使用者身分提供不同的內容，因HTTP是stateless，所以HTTP使用cookies。\n結合cookie和user所提供的帳號資訊，website能獲得大量使用者的相關資料，有可能販賣這些資訊(安全疑慮)\nWeb cache = proxy server ( server for original requesting client、client to origin server )\nproxy servers reduce response time for client request. proxy servers reduce traffic on an institution\u0026rsquo;s access link. Electronic Mail SMTP ( Simple Mail Transfer Protocol ) between mail servers to send email messages.\nTCP /port 25 use persistent connections. DNS ( Domain Name System ) A distributed, hierarchical database. Translate host name to IP address. ( UDP port/53 )\nroot DNS server、 TLD DNS server ( .com/.org/.edu/.gov )、 authoritative DNS server /au.thor.i.ta.tive/ Local DNS name server: each ISP (residential ISP, company, university) has one = DNS caching\nvideo streaming and content distribution networks (CDNs) DHCP ( Dynamic Host Configuration Protocol ） 主要是用來提供在TCP/ IP網路上的主機可以自動的分配到IP及所需要的相關設定。 DHCP可分成兩個部分來看，一個是DHCP伺服器端，另一個是DHCP用戶端。\nDHCP server有一個集中式的管理程式，主要負責設定網路各主機上所需要的IP資訊，並 提供給Client索取使用。DHCP用戶端是負責向DHCP伺服器索取所需要之相關IP資訊，並將資料註冊到該系統中。\nSockets Transport Layer what transport service does an app need?\ndata integrity timing throughput security TCP DEF ➡ reliable transport、flow control、connection-oriented doesn\u0026rsquo;t provide ➡ timing, minimum throughput guarantee, security\nTCP的增強特性 Fast Retransmit \u0026amp; Recovery （FRR），當receiver發現丟包時，發送重複的ACK給對方，當sender收到3個重複的ack時，就確人此packet為loss，馬上進行重傳(不需等待time out)\n為了在不可靠的網路介面上建立可靠的傳送資料服務，TCP必須解決可靠性，flow control的問題，而且必須能為上層的應用程式來提供多個埠口，用來同時為多個應用程式提供資料，同時，TCP必須解決連接的問題，這樣子的話，TCP才是個可靠的通訊協定，而最後也要克服通訊安全的問題。 TCP連接是可靠的，而且保証了傳送區段的順序，保証順序的能力是用一個序號來達成的，區段內也包括一個序列號，表示接收方準備好這個序號的區段，在TCP協定傳送一個區段時，它會同時把這個區段放入重新發送的序列中，同時啟動計數器，如果收到了確認訊息，就會把重送的區段序列刪除，如果超過計數時間時，就會將這個區段重送。 Sliding Window: data flow control的技巧，它要求sender host在傳送一定量的data以後，必須接收對放的確認。 Window Size是由destination決定，而且會依網路流量調整並非一成不變(maybe congestion)，The destination can send an acknowledgment at any time。\nTCP socket identified by 4-tuple:\n• source IP address • source port number • dest IP address • dest port number\nDemux (由下到上傳到socket): receiver uses all four values to direct segment to appropriate socket. each socket identified by its own 4-tuple. (不同的source ip address or port number，就會轉交給兩份不同的socket)\nTCP congestion control flow control ➡ receiver 控制sender別送太多 (Receive Window)\ncwnd (Congestion Window, bytes) ➡ 限制TCP sender資料送入網路的速率，為了避免congestion，值應該都\u0026lt; cwnd。\nrate = cwnd (量) / RTT bytes/sec ssthresh = cwnd / 2\nMSS: ( Maximum Segment Size ) ➡ MSS是TCP數據包次能夠傳輸的最大數據分段。建立連線之前雙方講好，因TCP為雙向連線，所以兩邊傳遞之最大值可以不一樣\n爲了達到最佳的傳輸效能，TCP協議在建立連接的時候通常要協商雙方的MSS值，這個值TCP協議在實現的時候往往用MTU值代替（需要減去IP數據包包頭的大小20Bytes和TCP數據段的包頭20Bytes）所以一般MSS值1460，通訊雙方會根據雙方提供的MSS值得最小值确定爲這次連接的最大MSS值。\nSlow Start ➡ initial.cwnd = 1MSS，每當receiver得到一次確認後就增加 1 MSS (各將cwnd+1，double cwnd)， 此一過程造成每回RTT中傳送速率倍增，因此TCP的傳送速率從低速開始，接下來會以指數性地成長。\n指數成長 cwnd = cwnd + 1 出2回2 出4回4 出8回8，傳的單位是per ack，每次回來幾個ack就+1幾次。\nSuccess Event\nincrease exponentially (new ack) Slow start to congestion avoidance, cwnd gets to 1/2 of its value before timeout (when cwnd \u0026gt;= ssthresh) Loss Event ( timeout )\ncut cwnd = 1 MSS. ssthresh = cwnd / 2 Congestion Avoidance ➡ 線性成長 cwnd = cwnd+1 (cwnd = cwnd + MSS * (MSS / cwnd) )\ncwnd全收完ack，整個cwnd才+1。\nFast Recovery ➡ sender重複收到3次相同ack時，可能是packet loss了，\n當收到三個重複的ACKReno就會認爲丢包了，并認定網絡中發生了擁塞。Reno會把當前的ssthresh的值設置爲當前cwnd的一半，但是并不會回到slow start階段，而是将cwnd設置爲（更新後的）ssthresh+3MSS，之後cwnd呈線性增長。\nTCP New Reno NewReno是基於Reno的改進版本，主要是改進了Fast Recovery。\nNewReno需要收到該窗口内所有數據包的确認後才會退出快速恢複狀态，從而更一步提高吞吐量。\nTCP SACK TCP SACK在TCP Reno基礎上增加了：\n選擇确認（Selective Acknowledgements，SACK） 選擇重傳（Selective Retransmission） SACK就是改變TCP的確認機制，最初的TCP隻確認當前已連續收到的數據，SACK則把亂序等信息會全部告訴對方，從而減少數據發送方重傳的盲目性。比如說序號1，2，3，5，7的數據收到了，那麼普通的ACK隻會確認序列號4，而SACK會把當前的5，7已經收到的信息在SACK選項裏面告知對端，從而提高性能，當使用SACK的時候，NewReno算法可以不使用，因爲SACK本身攜帶的信息就可以使得發送方有足夠的信息來知道需要重傳哪些包，而不需要重傳哪些包。\nTCP ECN ( Explicit Congestion Notification ) ECN = 11 = router已遇到擁塞狀況 當router將 IP paecket的 ECN 欄位設定為 11 時，receiver就會接到路徑中congestion的通知。ECN 使用 TCP header向傳送端指出網路正遇到擁塞狀況，同時向接收端指出傳送端已經從接收端接到擁塞指標，並且降低傳輸速率。\nreceiver sets ECE bit on receiver-to-sender ACK segment to notify sender of congestion\nTCP Vegas Vegas將RTT的增加作為congestion network的信號，RTT增加(cwnd減小)、RTT減小(cwnd增加)。 Vegas 通過比較實際throughput和expect throughput來調節cwnd的大小。\n期望吞吐量：Expected = cwnd / BaseRTT (RTT lowest)。 實際吞吐量：Actual = cwnd / RTT。 – Diff = ExpectedRate – ActualRate. – if Diff \u0026lt; a . increase CongestionWindow linearly. – else if Diff \u0026gt; b. decrease CongestionWindow linearly. – Else. leave CongestionWindow unchanged.\n網絡中 Vegas 與其它算法共存的情況下，基於丟包的擁塞控制算法會嘗試填滿網絡中的緩沖區，導緻 Vegas 計算的 RTT 增大，進而降低擁塞窗口，使得傳輸速度越來越慢，因此 Vegas 未能在 Internet 上普遍採用。\nAIMD: TCP sender收到ACK，成功cwnd + 1，失敗cwnd = cwnd / 2 (加性增，乘性減) Linear decrease in Vegas does not violate AIMD since it Happens before packets loss.\ntcp vegas / reno: Fast Retransmit ➡ Fast Recovery\nRandowm Early Detection (RED) Router所採許的一種策略，透過queue平均長度來detect congestion (AvgLen)，計算丢棄的概率來discard packet.\nRED is good at keeping avg. queue size steady. Thresholds are hard to determine.\nSequence Number TCP以向前參照確認來提供區段排序作業，每個資料在傳輸前都會先加以編號，到目的地工作站時，TCP會將這些區段重新組成完整的訊息。若資料中少了某個順序號碼，就會重傳該區段。區段傳出後，若在一段特定時間內沒有收到確認，也會要求重傳。\nUDP (best effort) DEF ➡ unreliable data transfer doesn\u0026rsquo;t provide ➡ reliability、flow control、congestion control、timing、 throughput guarantee、security、connection setup(handshaking)\nNetwork Layer Router examines header fields in all IP datagrams passing through it. Two key functions:\nforwarding ( data plane ): move packets from router\u0026rsquo;s input to appropriate router output. (怎麼轉送) routing ( control plane ): determine route taken by packets from source to destination. (路怎麼走) Router Architecture Longest prefix matching: when looking for forwarding table entry for given destination address, use longest address prefix that matches destination address.\nSwitching fabrics: transfer packet from input buffer to appropriate output buffer.\n隊頭阻塞（Head-of-line blocking或縮寫為HOL blocking）: 它的原因是一列的第一個數據包（隊頭）受阻而導緻整列數據包受阻。例如它有可能在緩存式輸入的交換機中出現，有可能因為傳輸順序錯亂而出現，亦有可能在HTTP流水線中有多個請求的情況下出現。\nIPv4 DEF ➡ 10.100.122.2被點分割為4個部分(Byte)，每個Byte = 8個bit，所以ipv4為32 bit address。 由 0開頭 到 127結尾的IP 是 A Class 由 128開頭 到 191結尾的IP是 B Class 由 192開頭 到 223結尾的則為 C Class\nPrivate Addresses Net ID、Host ID與Mask DEF ➡ 定義與切割網路用途，不同類型的IP會對應到有著不同的Net_ID。 A Class的IP / default mask : 前面1 byte數字 = Net ID，其餘三組 = Host ID / 255.0.0.0 B Class的IP / default mask: 前面2 byte數字 = Net ID，另兩組 = Host ID / 255.255.0.0 C Class的IP / default mask: 前面3 byte數字 = Net ID，剩下的一組 = Host / 255.255.255.0\nQ: 132.21.0.0, find the class, the block, and the range of the addresses? A: The class is B. The block has a Net_ID of 132.21. The addresses range from 132.21.0.0 to 132.21.255.255.\nSupernet (CIDR)與 Subnet Supernet ➡ 利用 Subnet Mask 重新定義較短的Net ID，借用Net ID後面幾個bit。 Subnet ➡ 利用 Subnet Mask 重新定義較長的Net ID，借用Host ID前面幾個bit，後面當作子網路的host address。 **Host_Id之mask切法: 看要幾個sub-net(切幾個1出去)，剩下0為該sub-net下可使用的ip address個數。 **右移 Net_Id之mask切法: 看要幾個class(切幾個0出去) 左移 205.16.37.24/29 : 8 addresses in the block. mask address: 255.255.255.X\ndestination AND mask = supe\trnet/subnet address.\n以network block跟network block之間為為單位進行分割。\nQ: 201.70.64.0, need 6 subnets. Design the subnets A: 8 -3 = 5(host_id_0s) subnets共8個. The number of address in each subnet is 32 = 代表1個subnet中(總共8個)，可容納32個不同的host ip address\nQ: We need to make a supernetwork out of 16 class C blocks. What is the supernet mask? A: 11111111 11111111 11110000 00000000 ( net_id左移4 bit )。\nDelivery and Routing of IP Packets Routing IP Packets Policy: Direct delivery ➡ Host-specific ➡ Network-specific ➡ Default indirect delivery: destination is in another network. If not present, use direct delivery. Host-specific: the entry in the destination field is host-specific address. If not present, destination field is network-specific address.\nnext-hop為該host到題目指定router的前一個router input ip address (到題目指定router)。\nIPv6 other changes from IPv4:\nchecksum: removed entirely to reduce processing time at each hop options: allowed, but outside of header, indicated by “Next Header” field ICMPv6: new version of ICMP IP Packet IP packet = header(控制資訊) + payload(資料本身) IP Header(最短20、最常60): 長度為4 Bytes的倍數 IP Payload(最短8、最常65515)\nHLEN = Header Length(佔4 bits)，紀錄此IP header的長度，HLEN的計算是以4 Bytes為基本單位。例如：HLEN欄位值為0101，即代表IP header的長度為5 ×4 = 20 Bytes，最長可達15×4 = 60 Bytes\nIdentification (16 bits) ➡ 若packet在傳輸過程中因為MTU的限制，導致傳輸過程中，將packet切割成幾個Fragments進行傳送，而因為每個IP packet到達目的裝置的先後順序可能與出發時的順序不同，因此接收端在進行packet重組時，便必須以Identification進行判斷IP packet原來的順序，以便能將屬於相同資料packet的fragment組合在一起。\nTotal Length (16 bits) ➡ 記錄整個packet的長度，包含IP header及IP所帶資料內容的總和。\nProtocol: which protocol to use.\nTime To Live（存活時間）➡ 佔8 Bits，記錄IP封包的「存活時間」，以限制IP封包在路由器之間轉送的次數。當IP封包每經過一部路由器時，路由器便會將Time to Live欄位值減1，當路由器收到此欄位值為1的IP封包時，便直接將之丟棄，不再轉送。\nFlag（封包切割旗標）➡ 佔3 Bits，主要對IP封包的切割提供控制訊息。 第1個bit: 未定義。 第2個bit: D 是否可以切割 第3個bit: M 0 = 最後fragmentation，1 = 尚有其他fragmentation。 If the M bit is 0, it means that there are no more fragments If the M bit is 1, it means that there is at least one more fragment\nIP Packet切割與重組 router中必須有IP packet的切割與重組機制，即是將過長的packet加以切割，以便能在MTU較小的網路上傳輸。切割後的IP封包，會由目的裝置重組，恢復成原來IP封包的模樣。\nIP 會將封包切割成多個較小的 (小於 MTU) fragment，使其能透過data-link傳輸 ， 目的端接收完所有fragment後，再將fragment進行重組。\nIP Fragmentation中IPv4 與 IPv6，其封包大小上限，分別為 65535 與 65575 Bytes。 遠遠超出了data-link的frame大小。\nMTU ( Maximum Transmission Unit ) DEF ➡ data-link中的MTU規範了frame的大小上限。 乙太網路 (Ethernet) 中的MTU: 1500 個位元組。\nICMP ( Internet Control Message Protocol ) ping是基於ICMP完成的。它的目的就是讓我們能夠檢測網路的連線狀況，也能確保連線的準確性，不過由於僅是控制訊息的傳遞並無詳細的指導，所以一般而言來源端並不處理該訊息\nARP ( Address Resolution Protocol ) DEF ➡ 已知IP address，求MAC address的protocol，利用「吼」的方式。發送一個broadcast packet，誰是這個IP誰來回答。switch會記住發來請求的sender的mac address，而switch學習到的結果稱為forwarding table(有過期時間)。\nRouting Protocol RIP ( Routing Information Protocol ) RIP version 2 supports CIDR. RIP uses the services of UDP on well-known port 520. OSPF ( Open Shortest Path First ) 由於OSPF路由協定的產生，是為了解決RIP路由協定的設備數量的問題，所以OSPF路由協定就沒有設備數量（Hop Count）的限制。會使用LSA的方式與其他路由器設備交換資訊，而不只是Routing Table的更新資訊而已。每一台路由器設備會自行計算屬於自己的「最佳網路路徑」，而這樣的資訊在各個路由器設備之間是不完全相同的。一個LSA封包中包含了介面的資訊、所使用的網路路徑評判標準（Metric）以及其他相關的資訊。當然，關於路徑的選擇，OSPF路由協定是採用最短路徑優先演算法（Shortest Path First Algorithm）。\n每一台路由器設備會自行計算屬於自己的「最佳網路路徑」，而這樣的資訊在各個路由器設備之間是不完全相同的。\nIn OSPF, all routers have the same link state database. OSPF packets are encapsulated in IP datagrams. Autonomous System: 由一個單位管理，例如 ISP、超級大學校、大公司等等，什麼都大的組織。每個 AS 裡面的路由表都自己管，隻要事先宣告自己持有什麼網段就行了。\nBGP ( Border Gateway Protocol ) routing among the ISPs ，BGP會自動決定各種狀態下的router，決定東西從哪邊走，該在哪邊停(外部網路下)。 維護router table來實現AS之間的可達性。\nSimple Network Management Protocol (SNMP ) = 簡單網路管理協定一方面不僅可使用於網路設備之日常維運作業，亦可提供網路維運人員即時監控設備異常事件發生及因應處理。\n其他 HUB(集線器) DEF ➡ 完全在physical layer工作，它會將自己收到的每一個byte，都複製到其他port上去。 缺: 不管某個介面是否需要，所有的bit都會被發送出去，然後讓主機來判斷是不是需要。\ndigital subscriber line (DSL): network line and telephone line to central office DSLAM. cable network: 共享頻寬，需要分散式的多重存取協定，用來協調傳輸，避免碰撞。\n大部分packet switch使用store-and-forward transmission (儲存轉送傳輸)，router必須先接收到整個封包，才開始將第一個位元傳輸到外部連結。\nPacket Switching vs Circuit Switching: 網際網路使用此，資源不會被保留(queueing delay loss)/資料會被預約保留 queueing delay: packet arrival rate to link (temporarily) exceeds output link capacity.\n","permalink":"https://jonathan-tw.github.io/posts/%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98ntust-network-communication-protocols/","summary":"Layered Outline What\u0026rsquo;s a protocol? protocols define format, order of messages sent and received among network entities, and actions taken on message transmission, receipt TCP/IP 5 Layer Application Layer HTTP、DHCP、RPC、P2P、DNS Transport Layer UDP、TCP Network Layer (IP) IP、ICMP、OSPF、BGP Data-link Layer (MAC) MAC、VLAN、STP Physical Layer 網路挑線 ISO/OSI 7 Layer  Presentation: allow applications to interpret meaning of data. example: encryption, compression, machine-specific conventions.  Session: synchronization, checkpointing, recovery of data exchange. 只要是在網絡上跑的包，都是完整的","title":"(課程筆記)(NTUST) Network Communication Protocols"},{"content":"Improving Performance of QUIC in WiFi Problem Domain WiFi (wireless)\nSolution 由於802.11 frame aggregation有助於改善throughput，所以讓QUIC增加traffic burstiness(BQUIC).\nreducing ACK frequency. In GQUIC ACK_DECIMATION mode, 至少收到100 packets才回傳ACK. disabling packet pacing. packet pacing aims to reduce sending bursts of packet, it may hinder performance in high-speed networks with low loss rates. Evaluation QUIC: detection algorithm利用ACK及RTT進行detect。使用packet pacing時，會降低aggregation，導致packet delay增加被detect到( minimum delay \u0026gt; threshold ) QUIC從slow-start提早離開。\nBQUIC: 不使用packet pacing，inter packet time降低。\nBQUIC increases the throughput between 20% to 30%\nFindings 從pacing機制上進行改良，解決文中slow-start較早退出的問題。 動態改善packet pacing: 在掉包率小的環境進行frame aggregation，在掉包大的環境使用原本QUIC。ieda: 依據下層網路環境的不同，而有彈性的使用不同的pacing機制。\nThe QUIC Fix for Optimal Video Streaming Problem Domain DASH (media in H.264)\nSolution reliable on I-Frame + unreliable on B/P-Frame，B/P-Frame參考I-Frame對其進行增量or差異的encoding，丟了I-Frame，其他B/P-Frame也沒用了。 unreliable data: - QUIC handshake to negotiate support for unreliable streams - replace retransmission of missed data with new data - provide a meta stream within QUIC to tag individual QUIC frames as reliable or unreliable. non-trivial challenge: synchronization of the streams: FEC on unreliable data Evaluation bufRatio = re-buffering time / total video time\nrateBuf = frequency of re-buffering events to / total video frame\naSSIM: 改善沒有考量latency的測量方式，晚到的frame會delay，考量了其stall對frame影響的綜合評估 The rateBuf values for both ClipStream and ClipStreamFEC are very close to 0%:\n真正要reliable的packet number變很少 (approx. 1% by count or 12% by size) of the overall video stream reliably\nFindings 從econder/decoder上進行( gpu RGG\u0026lt;\u0026ndash;\u0026gt;YUV )，降低解碼CPU占用率。\nan out-of-order packet is inserted into the byte-stream within re-order buffer unless the data has already been consumed by the application. If the application tries to consume \u0026ldquo;missing\u0026rdquo; byte-ranges the byte-stream is filled with zeros.\nQ: (Sync) How will the receiver combine the frames from the different streams into the appropriate order in the playback buffer?\nA: add a reliable control stream to signal multiplexing and demultiplexing information for diff sterams, The control stream helps the client to re-assemble the video file (reliably transfer end-of-stream )\nQUICsilver: Optimising QUIC for use with Real-Time Multimedia Traffic (University of Glasgow 2019) Problem Domain RTP (media in H.264)\nSolution 提出playback deadline的概念，在retransmit之前，確定內容的有效性(有沒有用)，有助於於減少發送不要的data。 server predicts that the packet will reach the client in time to be useful for video playback . implementing partial reliability within QUIC can assist with reducing latencies and playback delays\nFindings 在QUIC中加入Meda-awareness\n","permalink":"https://jonathan-tw.github.io/posts/2019-ieee-improving-performance-of-quic-in-wifi-2018-conext-the-quic-fix-for-optimal-video-streaming/","summary":"Improving Performance of QUIC in WiFi Problem Domain WiFi (wireless) Solution 由於802.11 frame aggregation有助於改善throughput，所以讓QUIC增加traffic burstiness(BQUIC). reducing ACK frequency. In GQUIC ACK_DECIMATION mode, 至少收到100 packets才回傳ACK. disabling packet pacing. packet pacing aims to reduce sending bursts of packet, it may hinder performance in high-speed networks with low loss rates. Evaluation QUIC: detection algorithm利用ACK及RTT進行de","title":"(2019 IEEE) Improving Performance of QUIC in WiFi \u0026 (2018 CoNEXT) The QUIC Fix for Optimal Video Streaming"},{"content":"Summary QUIC不及TCP在DASH的表現，由於大多數ABR algorighm都經過優化與TCP配合使用，因此沒有充分利用QUIC提供的功能。\nProblem statement What is the impact of QUIC on QoE? How should adaptive bitrate streaming be built to leverage the benefits provided by QUIC? Research objectives A Performance Study and future work Methods we studied QoE performance of different DASH quality adaptation algorithms that are either solely based on the playout buffer filling, or on the video segment download rate, or on both. DASH quality adaptation algorithms that are mainly categorized as throughputand/or playout buffer-based techniques. BB2-2: based on buffer. map current buffer occupancy to a quality bitrate.\nthe client tends to download a quality that is higher than the measured rate in order to provide the user with maximum average quality.\nSQUAD: based on rate that is specifically optimized for variations in TCP download rates. BOLA: based on both.\nEvaluation How does the author appraise their methodology?\nTestbed (Parallel Server / Single Server) bottleneck link of 10Mbps / Python-based DASH player / Caddy server Internet (wifi on campus / wifi on residential / wired campus Network ) with 17 bitrates ranging from 100Kbps on Aamazon EC2 How is the experiment set up?\nAverage Quality Bitrate (AQB) = Chosen Rate\nNumber of Quality Switches (#QS)\nSpectrum (H): A lower H indicates a better QoE. (bitrate變化的集中度量)\nRebuffering Ratio:\n$$ \\frac {Total;length - video;length } {video;length } $$\nWe investigated different QoE performance metrics such as the average quality bitrate, a measure of quality variations (denoted spectrum) (影片質量差異-頻譜) and the average video stalling duration. Testbed BBA-2 seems to benefit from using QUIC to download segments because of the slow-start or initial phase of the algorithm.\nthe client tends to download a quality that is higher than the measured rate in order to provide the user with maximum average quality. 只有較大的RTT時，會造成buffer不足，間接使得BBA-2 client切換到lower quality.\n(rule-of-thumb)bandwidth-delay product(BDP): B = C * RTT (receive buffer收到的大小，已傳送但尚未確認的資料)\nInternet Wifi significantly worse quality bitrate: QUIC is likely to be competing with more TCP streams than other QUIC streams, it takes longer than TCP to fetch the same segments Future Work HOL blocking (Multiplexing,Multi-Path)\nPacing\nIf a client requests multiple qualities for a single segment, the server could pace the streams to deliver segments at regular intervals instead of using AIMD. 如果單個網段提供多種質量，定期時間下，以stream的速度下去交付segments而不是AIMD\nIf a client requests multiple segments over the same connection via multiple streams the server could implement a decreasing pacing rate for segments requested depending on how soon they are required for playback. client如果在多個stream中請求多個segment，Server可以根據「請求的segment」需要多長時間來降低pacing rate\nAIMD -\u0026gt; TCP擁塞控制的方法\nEliminate Congestion Control Redundancy two congestion on client; in trans/ in DASH algo\nABR clients can disable congestion control in the QUIC transport layer and continue to use their existing congestion control\nQUIC provides the benefits of pacing and assistance for loss recovery in the form of NACKs\nRTP does that.\nConclusion we find through testbed and Internet measurements that QUIC does not provide a boost to current DASH algorithms but instead a degradation in the chosen quality bitrates. Although we observe a lower magnitude of quality variations (較低的質量變化幅度), the degradation of streamed quality bitrate with the use of QUIC is detrimental to overall QoE References ABR work/ congestion control / congestion avoidance\n[14]: Tcp hollywood: An unordered, time-lined, tcp for networked multimedia applications (2016)\nTCP Hollywood,a TCP variant, which implements out-of-order delivery and inconsistent retransmissions in order to improve good-put of video streaming applications\n[21]: Media qoe enhancement with quic (2016)\na new congestion control mechanism using QUIC that aggressively varies download rate according to a buffer-based priority level assigned by the ABR streaming client\n[4]: ABR protocol over UDP (2003)\nthe authors employ a form of congestion avoidance where the sending rate at the server is increased by a single packet for every RTT measurement.This design is different from the AIMD congestion control employed by TCP and QUIC since it eliminates the effect of slow start and attempts to provide an accurate estimate of the available bandwidth in the network. Some drawbacks of this approach are the requirement of two UDP sockets for every connection and the use of Berkeley Packet Filters to collect timestamps at the server and client for every video stream, thus, reducing both performance and scalability of the system\n","permalink":"https://jonathan-tw.github.io/posts/2017-nossdav-not-so-quic-a-performance-study-of-dash-over-quic/","summary":"Summary QUIC不及TCP在DASH的表現，由於大多數ABR algorighm都經過優化與TCP配合使用，因此沒有充分利用QUIC提供的功能。 Problem statement What is the impact of QUIC on QoE? How should adaptive bitrate streaming be built to leverage the benefits provided by QUIC? Research objectives A Performance Study and future work Methods we studied QoE performance of different DASH quality adaptation algorithms that are either solely based on the playout buffer filling, or on the video segment download rate, or on both. DASH quality adaptation algorithms that are mainly categorized as","title":"(2017 NOSSDAV) Not so QUIC A Performance Study of DASH over QUIC"},{"content":"Summary 在QUIC中，速度快的video stream被比較慢的audio stream給queue住(sharing a single UDP socket buffer)，造成audio request response latency拉長。其上層的DASH計算throughput不考慮response latency，造成實際的throughput跟預期的DASH不同。\nProblem statement 2018 (Not so QUIC): focused on buffer-based ABR these recent studies have indicated that buffer-based techniques are aggressive towards video-bitrate maximization whereas suffers in terms of playback smoothness and rebuffering\n2020 (Does QUIC Suit): focused on modern ABR We investigate further to understand the protocol-level behavior of QUIC, which impacts the QoE performance\nABR\u0026rsquo;s important metrics: predicted throughput during video streaming.\nResearch objectives Explore the performance of advanced ABR techniques(MPC,Pensieve)\nMPC-Fast/MPC-Robust -\u0026gt; predictive control Pensieve -\u0026gt; Deep Learning\nMethods (EXPERIMENTAL SETUP) computed 3 QoE metrics\naverage playback bitrate total rebuffering duration playback smoothness a linear representation q(Rn)= Rn,similar to [4], indicating that the playback quality increases linearly with the increase of playback bitrate.\nserver: lsquic,Go-QUIC. client: google chrome (we modify dash.js to add support for advance ABR algorithms like Pensieve and MPC)\nTo emulate realistic traffic behavior:\na Web-based javascript DASH player provided by DASH Industry Foundations (DASHIF) to stream the videos at the client side.\nMahimahi: benchmark traffic shaper(Accurate Record-and-Replay for HTTP)，用于記錄來自基于HTTP應用程序的流量，並針對模擬的網路環境進行模擬。 Public FCC dataset(模擬網路中dataset，compatible from Mahimahi): a broadband trace from FCC. Evaluation 比較QOE Average Video Bitrate: buffer-based ABR mechanisms aggressively use the highest quality levels. Playback Smoothness: A fluctuation in the quality level indicates less smoothness in the video playback, and, therefore, reduces the QoE. Advanced ABR techniques, such as MPC-Fast and Pensieve, provide better playback smoothness with DASH/QUIC, although the supported playback quality is lower compared to DASH/TCP. Rebuffering Time: based on which ABR technique is adopted. 比較應用場景 if the response latency is high, this segment may take longer time to reach the client, resulting in a rebuffering\nThe Throughput: the download time = the first and the last bytes received 兩者時間差. The Response Latency: initiation of the HTTP request and the time when the first byte of the response is received 兩者時間差 ( 收到的第一個ACK的時間 - 發起HTTP請求的時間 )\nwhy high response latency observed during the video streaming using QUIC? 在create two parallel HTTP requests情況下，generate two HTTP 相互依賴的stream，結果與上述差不多。 key:socket buffers between HTTP streams.在QUIC中，速度快的stream被比較慢的stream給queue住了。\nTCP creates two separate sockets for the two HTTP streams, each of the sockets maintains its own socket buffer. (Independent) QUIC multiplexes both the streams and uses a single UDP socket having a single socket buffer, the HTTP responses from both the streams interfere, and higher response rate at one stream affects the queuing delay for the response at the other stream (dependent) 在傳統DASH中，video的data rate比audio更高(video data \u0026gt; audio data)，TCP separate socket中的queue depending on their data generation rate.而對於QUIC來說，每個需要播放的segment都需要client發一個http request for video及一個http request for audio，由於video segemnt request先送，UDP socket buffer被video segement塞滿，audio segment必須等video segement在socket buffer中被釋放。\n(a) The audio data has to wait in the queue (the red timeline) before it gets served. (b) The audio streams at QUIC experiences a much higher latency compared to TCP. Audio Stream request時間較久 = response latency is higher\nConclusion The QUIC multiplexing of audio and video streams over a single UDP socket results in additional response latency for the audio segments, which are not captured during the calculation of channel throughput. As a consequence, the ABR algorithms take incorrect decisions during selecting the bitrates based on the calculated throughput over a QUIC connection Note ​\t#Anything additional that is not included in this outline\nReferences ​\t#Relevant articles that might be useful to look at and research on\n","permalink":"https://jonathan-tw.github.io/posts/2020-ieee-does-quic-suit-well-with-modern-adaptive-bitrate-streaming-techniques/","summary":"Summary 在QUIC中，速度快的video stream被比較慢的audio stream給queue住(sharing a single UDP socket buffer)，造成audio request response latency拉長。其上層的DASH計算throughput不考慮response latency，造成實際的throughput跟","title":"(2020 IEEE) Does QUIC Suit Well With Modern Adaptive Bitrate Streaming Techniques?"},{"content":"Ref 代碼隨想錄 TSMC \u0026amp; IC Design House(M,R,P)考古 Leetcode Blind 75 CodeTop Leetcode C++ Solutions Behavior questions 為什麼想來我們公司？ 你在xx有什麼特別的貢獻？ (xx = 實習公司) 在你過去工作經驗或求學過程中，你認為最大的困難/挑戰是?為什麼？ 在你過去工作經驗或求學過程中，你認為最大的成就/果是?為什麼？ 你人生遇到最挫折的事情？你怎麼克服？ 說說你的優點和缺點，並舉例說明。 未來的職業生涯目標是什麼？ 最看重工作的哪三個點？ 對於我們公司還有這個職缺你了解多少？ 為什麼我要錄取你？ 公司 廣達 - OpenBMC BU9 Team\n一面 D+0: 一對多面試，一開始為25分鐘C測驗含一題link list白板題。接下來自我介紹及論文介紹，目前openBMC蠻多大外商都在做，能練到的即戰力。 D+5 口頭offer get。 鴻海(鴻運科) - 通訊軟韌體研發工程師\n一面 D+0: 自我介紹及論文介紹，主管針對論文提出不少問題，也從問題中發現一些有趣的finding。像是不同transport protocol如何進行通訊，eg: server tcp, cliemt quic. D+3 口頭offer get。 台積 - IT BSID SCPM\n預面 3題hackerrank測驗1.5小 D+0: 難度EMM，一題bfs/dfs變形(1254. Number of Closed Islands)，一題弓箭射氣球變形(452. Minimum Number of Arrows to Burst Balloons)。最後一題有5個測資TLE，2.8題。 一面 D+0: 主管面試，自我介紹及論文介紹，主管針對部門的工作內容進行詳細的介紹。主管說這部門算是B部壓力比較大的，但學得到東西，不太需要oncall。 二面 D+7: 主管面試，技術面試(javascript, 基本coding)。這部分覺得當天答得不太好。 三面 D+14: 人資面試 到場測驗 D+15: 人格測驗 口頭offer get D+28: 核薪 offer get D+42: 因為簽約金的問題，所以多花了一些時間與HR商討(台積只有預聘有簽約金，it又有30w，可惜)。\n台達電 - 電動車充電樁\n一面 D+0: 與一位主管和一位同事進行面試，喜歡主管整體待人處事的氛圍。 D+7: HR核薪，原本以為會跟廣達差不多，結果比想像中高不少。台達電是系統場霸主應該是真的。 offer get D+7: 電動車市場現在很火，各個廠商磨拳擦掌在追spec，所以高工時高流動率在所難免。\nLenovo - CSP Firmware Engineer\n一面 D+0: 與兩位同事進行面試，前半自我介紹，後面考C，不難。 二面 D+5: 與兩位主管進行面試，一位來自「北京」的主管印象深刻(友善國人)，他比起台灣聯想的主管問了更多「技術」問題，特別是OS。由於一些技術名詞在兩岸用法上不太一樣，所以要再三確定問題。當天傍晚HR核薪。 offer get D+5: 台灣聯想招募效率很好，面試過程不拖泥帶水。主管說台灣聯想收購了之前的IBM，所以蠻多從IBM那跳來的人，整體公司管理風格與美商相近。因為算外商，一、二面試時都有叫我英文口說，當時沒準備，自己覺得講蠻爛的，但主管與同事都偏向正面回饋，不會刁難。\nMOXA(404) - WiFi application Engineer\n一面 D+0: 整體面試過程中主管態度友好，蠻香的系統廠。開發產品的週期較長，相對開發品質與效率也較好。一個禮拜有兩天可以WFH，「準時」下班，公司不鼓勵加班文化。 二面 coding測驗 D+4: 5題25分鐘，基本上都考C的觀念，問輸入輸出及一題recursive。 D+7: 收到實體面試通知，但已收到其他家的offer，故婉拒後續面試。 威旭資訊 - Infra Team Software Engineer\n預面 D+0: 一題程式題hw，難度easy-medium之間。 D+6: 收到技術實體面試通知，但已收到其他家的offer，故婉拒後續面試。 ","permalink":"https://jonathan-tw.github.io/posts/2023-%E8%BB%9F%E9%9F%8C%E6%96%B0%E9%AE%AE%E4%BA%BA-%E9%9D%A2%E8%A9%A6%E6%99%82%E7%A8%8B/","summary":"Ref 代碼隨想錄 TSMC \u0026amp; IC Design House(M,R,P)考古 Leetcode Blind 75 CodeTop Leetcode C++ Solutions Behavior questions 為什麼想來我們公司？ 你在xx有什麼特別的貢獻？ (xx = 實習公司) 在你過去工作經驗或求學過程中，你認為最大的困難/挑戰是?為什麼？ 在你過去工作經驗或求學過程中，你認為最大的成就/果是?為什麼？ 你人生遇到最挫折的事情？你怎","title":"2023 軟韌新鮮人 面試時程"},{"content":"CH1 - 基礎 1.1 1 2 int x, y z; y = z = 4 // y = 4 and z= 4 1.2 \u0026amp; 運算 =\u0026gt; 做二進位and運算 | 運算 =\u0026gt; 做二進位or運算 \u0026amp;\u0026amp; 和 || 皆為判斷式 =\u0026gt; 回傳1或者是0 1.3 1 x = (y == z) ? 4 : 5; y =4, z = 2, (問號前面的判斷式) false, x取後者, x = 5 if (true) ? x = 4 if (false) ? x = 5\n1.4 1 ::value = 2; 在C中無法編譯通過，C++可。(改變全局變數值)\n1.5 i++/++i 區別 1 2 3 int i = 8; printf(\u0026#34;%d\\n\u0026#34;, ++i); // 9 printf(\u0026#34;%d\\n\u0026#34;, i--); // 9, 先印9, i = 8 效率沒有區別, 自定義++i較佳\n1.7 不使用中間變量將a,b值進行交換 1 2 3 4 5 6 7 8 9 10 11 12 13 void swap2( int a, int b ) { a = a + b; b = a - b; a = a - b; } // 使用XOR完成交換 1xor1 = 0, 1xor0 = 1. void swap3( int a, int b ) { a = a ^ b; b = a ^ b; a = a ^ b; } 1.8 extern的使用 對於extern變量來說，僅僅是一個變量的聲明，其並不是在定義分配內存空間。如果該變量定義多次，會有連接錯誤。其聲明的函數或變量可以在其他module中使用。\n1.9 main()執行完還執行其他語句嗎 使用atexit(function_name), 註冊程式終止要被調用的函數。\nCH2 - macro, inline, const, static, sizeof 2.2 用#define定義最大和最小值 1 2 #define MAX(x,y) ( ((x) \u0026gt; (y)) ? (x) : (y) ) #define MIN(x,y) ( ((x) \u0026lt; (y)) ? (x) : (y) ) define: compile時代入 (沒數據類型) const: run-time時代入 (有數據類型，進行安全檢查)\nmacro 與 inline 相同之處：是把呼叫的位址換成函式主體，這樣執行速度較快，多半用於需要多次呼叫的程式。 相異之處：macro 是pre-compile，會在程式編譯成為機器語言之前先編譯完成，是一種文字替換的概念。而inline就像一般函式一樣，是compiler決定的。\n1 2 3 4 5 6 7 8 // macro example #define SIZE 1024 // 定義常數 #define SQUARE(x) ((x) * (x)) // 定義函式 //inline example inline int square(int x){ return x*x; } macro 與 inline 都是用空間換取時間\nfunction 則是用時間換取空間\n2.7 const的使用 #2-12\nconst定義常量 (只讀不寫)，具有不可變性。 對類型進行安全檢查。 不能引用(＆)const值。 傳遞參數到函式中，但又不希望額外新增參數。 void fun(A a); void fun(A const \u0026amp; a); 引用a但加const保持不變性 1 2 3 4 5 int b = 10; const int * a1 = \u0026amp;b; // 不允許修改值(修飾int)，但指向位置可以修改 int const * a1 = \u0026amp;b; int * const a2 = \u0026amp;b; // 不允許修改位置(修飾指標)，但值可以修改 const int * const a3 = \u0026amp;b; // 皆不能修改 2.10 static的使用 static在這function呼叫結束時，值維持不變(不會因為function結束而消失，具有繼承性)。 static在當全域變數時，可以被所有函數訪問，但不能被該檔案外的其他函數訪問，是一個本地的全局變量。 static function也只可被該檔案內的其他函數調用，不能被模塊外的其他函數訪問。 static變數在class中也具有共享性，即多個obj共享一個static參數。 static function在class中也具有共享性，即便我們沒有產生 instance 出來，我們也隨時可以取用這個 function。 1 2 3 static局部變數 vs 普通局部變數: static只初始化一次，下一次依據上一次結果值 static全局變數 vs 普通全局變數: static只初始化一次，防止在其他文件中被引用 static function vs 普通function: static function在內存中只有一份，普通function每個被調用都維持一份複製品 2.13 使用sizeof計算變數所占的bytes大小 char str[] = \u0026ldquo;hello\u0026rdquo;; char * p= str; int n = 10; sizeof(str) = 5 + 1 (\u0026rsquo;\\0\u0026rsquo;) = 6; sizeof(p) = 4; sizeof(n) = 4; 32位winnt平台下，指針和int都為4 bytes\nclass中的function不占內存，但virtual function佔用一個pointer的大小 = 4 bytes。 int和「指針」占4 bytes，char占1 bytes，double占8 bytes。 繼承的函數也要另外的空間。 static在function中，不算空間。 union的大小取決於他所有的成員中佔用空間最大一個成員的大小。同類型取最高，不同類型需對齊。 CH3 - pointer 1 2 \u0026amp; = get value address (call by reference) * = 取出pointer之value (取出address指向的value) 3.1 引用 \u0026amp; 1 2 3 4 5 int a = 10; int b = 20; int \u0026amp;rb = a; // 引用只能在聲明的時候賦值 rb = b; cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl; // a = 20 引用vs pointer: 引用初始完後不能被改變，pointer可以隨時指向別的對象。 3.10 用pointer賦值 1 2 3 4 5 6 char a[] = \u0026#34;hello world\u0026#34;; char * ptr = a; printf( \u0026#34;%c\\n\u0026#34;, *(ptr+4) ); // o printf( \u0026#34;%c\\n\u0026#34;, ptr[4] ); // o printf( \u0026#34;%c\\n\u0026#34;, a[4] ); // o printf( \u0026#34;%c\\n\u0026#34;, *(a+4) ); // o 3.12 Pointer比較 1 2 3 4 5 6 7 8 9 10 char str1[] = \u0026#34;abc\u0026#34;; // str1 != str2 記憶體位置不一樣 char str2[] = \u0026#34;abc\u0026#34;; char * str7 = \u0026#34;abc\u0026#34;; // str7 == str8 記憶體位置一樣 char * str8 = \u0026#34;abc\u0026#34;; char * str3 = \u0026#34;AAA\u0026#34;; str3[0] = \u0026#39;B\u0026#39;; // 不合法，str3指向常量，不能操錯。pointer不能直接賦值，要初始化。 int * a[10]; // 指針數組，數組中每一個元素都是指針 int * a = new int[10]; // 一個指針，指向一個數組。 3.21 指針數組找錯 1 2 3 4 5 6 char * str[] = { \u0026#34;Weclone\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;Fortemedia\u0026#34;, \u0026#34;Nanjing\u0026#34; }; // A,B,C,D char **p = str + 1; // p移動指向B str[0] = (*p++) + 2; // p移動指向C，str[0] = 空 str[1] = *(p+1); // p不移動，str[1] = D str[2] = p[1] + 3; // 指向D往後3個元素，Jing str[3] = p[0] + ( str[2] - str[1] ); 3.25 函數指針 1 2 3 4 5 6 7 int max( int x, int y ) { return x \u0026gt; y ? x : y; } int (*p) (int,int); int max( int, int ); p = \u0026amp;max; 1 2 3 4 5 6 7 8 9 int main() { int (*op[2]) (int a, int b); op[0] = add1; op[1] = add2; cout \u0026lt;\u0026lt; op[0]( 0, 0 ) \u0026lt;\u0026lt; op[1]( 0, 0 ); } int add1( int a1, int b1 ) { return a1+b1 }; int add2( int a2, int b2 ) { return a2+b2 }; 3.30 有了malloc/free，還需要new/delete malloc/free是標準庫函數，無法執行constructor與destructor\n1 2 3 4 5 6 char * name = \u0026#34;test\u0026#34;; char * coptyName() { char * newname = new char[strlen(name) + 1]; // 字符串已0作為結束符。 strcpy( newname, name ); return newname; } 3.35 棧內存 (Stack) ＆ 堆內存(Heap)\nstack: 呼叫函數的局部變數在此儲存，函數結束時自動釋放。 heap: new/malloc在此的動態內存分配。 CH4 - String [[8 - String to Integer (atoi)]]\n4-5 strcpy實作 1 2 3 4 5 6 7 8 9 10 11 12 13 char * DIY_strcpy( char * strDest, char * strSrc ) { if ( strDest == NULL || strSrc == NULL ) return NULL; char * strDestStart = strDest; while ( *strSrc != \u0026#39;\\0\u0026#39; ) { *strDest = *strSrc; strSrc++; strDest++; } *strDest = \u0026#39;\\0\u0026#39;; return strDestStart; } 4-6 memset vs memcpy vs memcmp memset 複製字符 c（一個無符號字符）到參數 str 所指向的字符串的前 n 個字符。 1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main () { char str[50]; strcpy(str,\u0026#34;This is string.h library function\u0026#34;); puts(str); memset(str,\u0026#39;$\u0026#39;,7); puts(str); return(0); } This is string.h library function qqqqqq string.h library function\nmemcpy vs strcpy memcpy可複製其他類型的數據，strcpy則只用來複製字符串(\u0026rsquo;\\0\u0026rsquo;)而結束。\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; void main() { char *s=\u0026#34;pannyloveworld\u0026#34;; char s1[10]; s1[9]=\u0026#39;\\0\u0026#39;; memcpy(s1,s+5,9); // (dest, start_index, 長度) printf(\u0026#34;%s\u0026#34;,s1); } Output: loveworld\nmemcmp vs strcmp 皆為比較字串的大小，memcmp為比較內存塊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main () { char str1[15]; char str2[15]; int ret; memcpy(str1, \u0026#34;abcdef\u0026#34;, 6); memcpy(str2, \u0026#34;ABCDEF\u0026#34;, 6); ret = memcmp(str1, str2, 5); // 比較str1和str2前n個字節AIIC碼值的大小； ret \u0026gt; 0 = str1 \u0026gt; str2, ret \u0026lt; 0 = str1 \u0026lt; str2 // 大寫ascii值較小 if (ret \u0026gt; 0) printf(\u0026#34;str1 is big than str2\u0026#34;); else if (ret \u0026lt; 0) printf(\u0026#34;str2 is big than str1\u0026#34;); else printf(\u0026#34;str1 is equal to str2\u0026#34;); return(0); } Output: str1 is less than str2\n4-11 計算字串長度 1 2 3 4 5 int strlen2( char * src ) { char * temp = src; while ( *src++ != \u0026#39;\\0\u0026#39; ); return ( src - temp - 1 ); } 4-18 aaabb = aaa3b2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;string.h\u0026gt; void transformation() { char str[200]; scanf( \u0026#34;%s\u0026#34;, str ); char * p = str; char * q = str + 1; int count = 1; int len = strlen(str); char * buf = (char*)malloc(len+1); while ( *q != \u0026#39;\\0\u0026#39; ) { if ( *q == *p ) { q++; p++; count++; } else { sprintf( buf, \u0026#34;%d\u0026#34;, count ); strcat( buf, q ); *q = \u0026#39;\\0\u0026#39;; strcat( str, buf ); q++; p = q; q = p + 1; count = 1; } } sprintf( buf, \u0026#34;%d\u0026#34;, count ); strcat( str, buf ); printf( \u0026#34;%s\\n\u0026#34;, str ); } 字串記得初始化\n1 2 3 char * str3 = NULL; str3 = new char[strlen(str1)+strlen(str2)+1] str3[0] = \u0026#39;\\0\u0026#39;; // 記得new完的字串要初始化。 4-19 實作strcat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; #include\u0026lt;string.h\u0026gt; // strcat(原本，追加) char * mystrcat( char * str1, char * str2 ) { char * dest = str1; while ( *str1 != \u0026#39;\\0\u0026#39; ) str1++; while ( *str2 != \u0026#39;\\0\u0026#39; ) { *str1 = *str2; str1++; str2++; } *str1 = \u0026#39;\\0\u0026#39;; return dest; } char * mystrcat2( char str1[], char str2[] ) { char * dest = str1; int i = 0; while ( str1[i] != \u0026#39;\\0\u0026#39; ) i++; int j = 0; while ( str2[j] != \u0026#39;\\0\u0026#39; ) { str1[i] = str2[j]; i++; j++; } str1[i] = \u0026#39;\\0\u0026#39;; return dest; } int main() { char * dest = NULL; char str1[40] = \u0026#34;Hello \u0026#34;; char str2[40] = \u0026#34;World!\u0026#34;; char * str3 = \u0026#34;Hello \u0026#34;; char * str4 = \u0026#34;World!\u0026#34;; dest = (char*)malloc(256); *dest = \u0026#39;\\0\u0026#39;; // dest = mystrcat2( str1, str2 ); dest = mystrcat( mystrcat( dest, str3 ) , str4 ); printf( \u0026#34;dest: %s\\n\u0026#34;, dest ); } CH5 - Bit operation 5-3 set/remove 特定bit bit 3 = 2的3次方 = 從右數來第四位。\n1 2 3 4 5 int main() { int i = 20, j = 28; i = i | ( 1 \u0026lt;\u0026lt; 3 ); // i = 28 (3 = n) j = j \u0026amp; ~( 1 \u0026lt;\u0026lt; 3 ); // j = 20 (3 = n) } 5-4 計算一個char有多少bit被設置1 1 2 3 4 5 6 7 8 9 int main( ) { int a = 63, count = 0; while ( a ) { a = a \u0026amp; a - 1; count++; } printf( \u0026#34;%d\\n\u0026#34;, count ); } 5-8 用define聲明一個常數 1 #define SECONDS_PER_YEAR ( 60 * 60 * 24 \u0026amp; 365) UL(無號常整數) ","permalink":"https://jonathan-tw.github.io/posts/c_c++-%E9%9D%A2%E8%A9%A6%E7%A7%98%E7%AC%88/","summary":"CH1 - 基礎 1.1 1 2 int x, y z; y = z = 4 // y = 4 and z= 4 1.2 \u0026amp; 運算 =\u0026gt; 做二進位and運算 | 運算 =\u0026gt; 做二進位or運算 \u0026amp;\u0026amp; 和 || 皆為判斷式 =\u0026gt; 回傳1或者是0 1.3 1 x = (y == z) ? 4 : 5; y =4, z = 2, (問號前面的判斷式) false, x取後者, x = 5 if (true) ? x = 4 if (false) ? x = 5 1.4 1 ::value = 2; 在C中無法編譯通過，C++可。(改變全局變數值","title":"C/C++ 面試題"},{"content":"Lecture 2 - Asynchronous I/O Programming Models\nConcurrency (Asynchronous I/O = cooperative multitasking) Multiple tasks have the ability to run in an overlapping manner Concurrency does not imply parallelism! Multiprocessing CPU-bounded tasks Multithreading I/O bound tasks It uses preemtive multitasking Promise object:\nan async object that be returned by the async function. An object representing completion or failure of an asynchronous operation. Await function:\nmakes program to wait until the promise is resolved or rejected can only be usded inside async function Lecture 3 - Cloud Architectures Cloud Computing Concepts On-demand and self-service (當需要資源時才被提供,自動化) Resources are provisioned as they are requested and when they are required ‒ No human interaction, automatic No human interaction, automatic Board network access (資源可被網路取得) Capabilities are available over the network Resource pooling (資源根據需求再提供,硬體配置被mutiple tenant共用) Resourcces are dynamically assigned/re-assigned according to demand Provider\u0026rsquo;s computing resources reused by multiple tenants Computing resources: CPU, memory, storage, network Scalability and elasticity (資源根據需求擴大或縮減) Infrastructure may grow and shrink according to needs Automatic or manual Measured service (資源可被控管) Resource usage can be monitored, controlled and reported Pay-per-use (當消費者使用資源時才付費) Consumers only pay for resources when they use them Multitenancy: Architectural approach where resources are shared between multiple tenants or consumers Infrastructure as Code: Version control, team development, scripting, etc.\nTerraform:\nHigher-level abstraction of the datacenter and associated services Supports many service providers: Google, Microsoft, Oracle, AWS VCN = a private network in a single region in which your instances reside\nLecture 4 - Cloud Native and Microservices Microservices: applications as independenly deployable services\nLoosely coupled: Integrated using well-defined interfaces Technology-agnostic protocols: HTTP, they use REST architecture Independently deployable and easy to replace: A change in small part requires to redeploy only that part Implemented using different technologies: PL, databases Container Dockerfile is a script that creates a new image\nA line in the Dockerfile will create an intermediary layer\ndocker build -t tomvit/httpd:v1 If processing fails at some step, all preceeding steps will be loaded from the cache on the next run.\nNetworking and Linking:\nbridge – container can access host\u0026rsquo;s network (default) host – all host\u0026rsquo;s network interfaces will be available in the container none – container will be placed on its own network and no network interfaces will be configured. Data Volume:\nA directory that bypass the union file system Data volumes can be shared and reused among containers Data volume persists even if the container is deleted It is possible to mount a shared sotrage volume as a data volue by using a volume plugin to mount e.g. NFS Lecture 5 - Browser Networking Network security\nConnection limits Request formatting and response processing TLS negotiation Same-origin policy XMLHttpRequest (XHR) basis for AJAX → Asynchronous JavaScript and XML Security Scripting Attacks CSRF(Cross-site request forgery) 利用Session未過期的特性，冒沖使用者身份來做惡意攻擊\nCSRF的攻擊流程是利用使用者剛使用完某服務後不久Session未過期的時間內，誘導使用者點擊惡意連結來冒充使用者的身份發送非本意的請求，例如使用者剛登入網銀沒多久後收到一個連結，點開來後才發現是一個轉帳的api將使用者的戶頭的錢轉到指定戶頭去。舉例來說，誘導使用者拜訪惡意網站，然後在網站內的img src塞有CSRF 漏洞的api，只要Session沒過期，就可以成功攻擊，當然那是api 是GET請求狀況。 預防方法:\n檢查Referer: Header的referer欄位記錄著來源的domain，只要是不同domain就擋，簡單直接，但可惜的是有些瀏覽器不帶referer。 XSS (Cross-Site-Scripting) 將惡意程式碼植入網站內，讓網站去執行惡意程式碼來達到獲取敏感資料的目的\nXSS: 將惡意程式碼植入網站內，讓網站去執行惡意程式碼來達到獲取敏感資料的目的 儲存型XSS: XSS攻擊常見於網站有讓使用者輸入的部分(如表單，回覆留言等等)，攻擊者輸入惡意程式碼後送出，來注入惡意程式到Database，來達到攻擊的目的，因為攻擊成功後只有在Database看得出來，且每個使用者對會受到影響，因此殺傷力最大。\nEx. 假如社群網站回覆留言時輸入 且網站沒有做任何XSS防禦就顯示user的留言，則網站就會顯示出惡意的通知。\r預防方法:\n最重要的邏輯是不能相信使用者的輸入，在輸入時檢查或是輸出時做檢查，例如將使用者輸入的值轉成純文字再輸出，在前端框架下(ex. Vue.js)，即會對所有的data做XSS的檢查將其轉換為純文字。 Cross-origin Resource Sharing Protocol (CORS) Allow for cross-site HTTP requests HTTP requests for resources from a different domain than the domain of the resource making the request. JSON and JSONP ajax請求受同源策略影響，不允許進行跨域請求，而script標籤src屬性中的鏈接卻可以訪問跨域的js腳本，利用這個特性，服務端不再返回JSON格式的數據，而是返回一段調用某個函數的js代碼，在src中進行了調用，這樣實現了跨域。\nService that supports JSONP allows to specify a query string parameter for a wrapper function to load the data in JavaScript code otherwise the data cannot be used in JavaScript they\u0026rsquo;re loaded into the memory but assigned to nothing A kind of workaround for the same origin policy Resource Format GET DELETE http://company.at/customers XML AJAX (1) AJAX (2) http://company.at/suppliers JSON AJAX, JSONP (3) AJAX (4) http://weather.at/innsbruck XML AJAX-CORS (5) AJAX-CORS (6) http://people.at/students JSON AJAX-CORS, JSONP (7) AJAX-CORS (8) http://people.at/{dob}/contact VCARD AJAX-CORS (9) AJAX-CORS (10) res.writeHead(200, {\r'Content-Type': 'Application/json',\r'Access-Control-Allow-Origin': '*'\r});\rLecture 6 - Security 驗證（Authentication）是證明身分（identity）的機制，例如: authenticate(name, passwd)方法定義了如何使用name與passwd進行驗證。此外，驗證方式不僅是基於名稱及密碼，也有可能基於憑證（Certificate）之類的機制。一旦caterpillar通過驗證，就可以看到訊息，也就是說，另外有個機制決定訊息資源可否授權觀看，就像授權（Authorization）定義了身分與資源之間的存取控制規則，例如，if(authorized()) { show(\u0026ldquo;message\u0026rdquo;); }這個流程，定義了\u0026quot;message\u0026quot;是否可以顯示。\nStandard: HTTP authentication\nHTTP defines two options Basic Access Authentication Digest Access Authentication Basic authentication HTTP Basic Authentication為一簡單的HTTP請求認證方法，用來保護server端的資源。當client端對server發起請求的同時必須提供帳號(user-id)及密碼(password)讓server端驗證，只有通過驗證才能取得資源。\nClient提供Basic Authentication請求的帳號密碼的方式為， 在HTTP Request Headers加入key=Authorization，value=Basic \u0026lt;basic-credentials\u0026gt;。Basic為Basic Authentication規範的名稱，固定加在前頭。\u0026lt;basic-credentials\u0026gt;為Basic Authentication的憑證，其為以Base64 encode對user-id:password的編碼。例如帳號為john，密碼為abc，則\u0026lt;basic-credentials\u0026gt;為以Base 64 encode對john:abc的編碼，也就是am9objphYmM=。\n因此對server資源發出請求時，在Request Headers加入以下欄位。\n1 Authentication: Basic am9objphYmM= A client may associate a valid credentials with realms such that it copies authorization information in requests for which server requires authentication (by WWW-Authenticate header) Credentials: credentials are base64 encoded (the format is: username:password) Digest Access Authentication No password between a client and a server but a hash value.\nTLS TLS provives message framing mechanism Every message is signed with Message Authentication Code (MAC) MAC hashes data in a message and combines the resulting hash with a key (negotiated during the TLS handshake) The result is a message authentication code sent with the message TLS and Proxy Servers:\nTLS Offloading: Inbound TLS connection, plain outbound connection ‒ Proxy can inspect messages\nTLS Bridging: Inbound TLS connection, new outbound TLS connection ‒ Proxy can inspect messages\nEnd-to-End TLS (TLS pass-through): TLS connection is passed-through the proxy - Proxy cannot inspect messages\nLoad balancer: Can use TLS offloading or TLS bridging ‒ Can use TLS pass-through with help of Server Name Indication (SNI)\nJSON Web Token (JWT) After user logs in, following requests contain JWT token.\n授權(Authorization)：這是很常見 JWT 的使用方式，例如使用者從 Client 端登入後，該使用者再次對 Server 端發送請求的時候，會夾帶著 JWT，允許使用者存取該 token 有權限的資源。單一登錄(Single Sign On)是當今廣泛使用 JWT 的功能之一，因為它的成本較小並且可以在不同的網域(domain)中輕鬆使用。 訊息交換(Information Exchange)：JWT 可以透過公鑰/私鑰來做簽章，讓我們可以知道是誰發送這個 JWT，此外，由於簽章是使用 header 和 payload 計算的，因此還可以驗證內容是否遭到篡改。 Open standard Mechanism to securely transmit information between parties as a JSON object Can be verified and trusted as it is digitally signed Basic concepts\nCompact → has a small size → can be transmitted via a URL, POST, HTTP header. Self-contained → payload contains all required user information. Oauth 2.0 OAuth 1.0 – first standard, security problems, quite complex OAuth 2.0 – new version, not backward compatibile with 1.0 request: client_id + and client_secret, and redirect_link. grant: code + client_id + client_secret + redirect_link + grant_type: \u0026ldquo;authorization_code\u0026rdquo; access token: JWT token jwt.decode(token) to get the data.\nResponse:\nSuccess – 200 OK. Error – 401 Unauthorized when token expires or the client hasn\u0026rsquo;t performed the authorization request. Refreshing a token:\nPOST request to the token endpoint with grant_type=refresh_token and the previously obtained value of refresh_token OpenID Protocol OpenID絕妙地解決了多個帳號同步問題 XRDS：一種基於XML的XRI資源描述符。它被設計用來提供關於XRI的可用的、描述性信息。在OpenID應用場合中，XRDS用來描述OpenID服務器，並且使用「priority」參數標識了用戶對服務器的優選順序。在下面的示例中，http://www.livejournal.com/users/frank具有最高的優先權（最低的數值）：\nLecture 7 - Protocols for the Realtime Web server is able to send pieces of response w/o terminating the conn.\nusing transfer-encoding header in HTTP 1.1 (Transfer-Encoding: chunked) Each chunk starts with hexadecimal value for length End of response is marked with the chunk length of 0 using End of File in HTTP 1.0 (server omits content-lenght in the response) Pushing – updates from the server (also called COMET)\nlong polling – server holds the request for some time streaming – server sends updates without closing the socket Server-Sent Events API to handle HTTP streaming in browsers by using DOM events transparent to underlying HTTP streaming mechanism Format response\u0026rsquo;s content-type must be text/event-stream ‒ every line starts with data:, event message terminates with 2 \\n chars. every message may have associated id (is optional) - When a connection is dropped EventSource will automatically reconnect: It may advertise the last seen message ID The client appends Last-Event-ID header in the reconnect request: The stream can be resumed and lost messages can be retransmitted. Cross-document messaging Lecture 8 - HTTP/2 communication is multiplexed within a single TCP connection Multiple requests and responses can be delivered in parallel, deliver lower page load times. Browser Request Prioritization Flow control Header compression Binary Framing Layer PUSH_PROMISE frames\nA singnal that the server intents to push resources to the client The client needs to know which resources the server intends to push to avoid creating duplicate requests for these resources. pushed resources must obey the same-origin policy Ref [1] CORS/CSRF/XSS 介紹與防禦方法 https://medium.com/%E7%A2%BC%E8%BE%B2%E8%83%8C%E5%8C%85%E5%AE%A2/cors-csrf-xss-%E4%BB%8B%E7%B4%B9%E8%88%87%E9%98%B2%E7%A6%A6%E6%96%B9%E6%B3%95-a1f5c55d96a1\n[2] js 跨域问题 https://zhuanlan.zhihu.com/p/583595367\n[3] 驗證與授權 https://www.ithome.com.tw/voice/134389\n[4] HTTP Basic Authentication https://matthung0807.blogspot.com/2020/04/http-basic-authentication.html\n[5] 是誰在敲打我窗？什麼是 JWT ？ https://5xruby.tw/posts/what-is-jwt\n","permalink":"https://jonathan-tw.github.io/posts/middleware-architectures-2-review/","summary":"Lecture 2 - Asynchronous I/O Programming Models Concurrency (Asynchronous I/O = cooperative multitasking) Multiple tasks have the ability to run in an overlapping manner Concurrency does not imply parallelism! Multiprocessing CPU-bounded tasks Multithreading I/O bound tasks It uses preemtive multitasking Promise object: an async object that be returned by the async function. An object representing completion or failure of an asynchronous operation. Await function: makes program to wait until the promise is resolved or rejected can only be usded inside async function Lecture 3 - Cloud Architectures Cloud Computing Concepts On-demand and self-service (當需要資源時才被提供,自動化) Resources are provisioned as they are requested and when they are required ‒ No human interaction, automatic No human interaction, automatic Board network access (資源可被網路","title":"(課程筆記)(CTU in Prague) Middleware architectures 2 Review"},{"content":"AJAX / XHR states / CORS / Data access Tasks Create a simple HTML page with an info text field and a single button Implement a JavaScript function which is triggered when the button is clicked The function should fetch relatively large file (e.g. 100-200MB) in the text field show following states: loading - when the open method was called loaded - when the send method was called downloading - while the data is being downloaded finished downloading - when the data has beeen downloaded you can use Promise, async/await Description AJAX overview:\nAsynchronous JavaScript and XML technique for creating better, faster, and more interactive web applications relies on XML, JSON, HTML, CSS and JavaScript AJAX is not a programming language Running this demo by using the jquery module to achieve the ajax request. Following the xhr state in https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/readyState. The browser console would show the current XMLHttpRequest state when starting the server.\nDeal with the problem of CORS To deal with the CORS problem in the ajax request, I upload the large file to the Amazon S3 bucket and set the following json code to allow the local server accessing the remote resources.\nData access A company needs to design an AJAX aplication that will access various resources on the Web by using JavaScript code running in a browser. This application is not public and only internal employees of the company can use it. This application will be available at the address http://company.at. Following table shows a list of URL addresses of resources, their formats and HTTP methods that the application will use to access these resources. Add all possible access technologies to the table for methods GET and DELETE. Note: parameter {dob} means date of birth.\nResource Format GET DELETE http://company.at/customers XML AJAX (1) AJAX (2) http://company.at/suppliers JSON AJAX, JSONP (3) AJAX (4) http://weather.at/innsbruck XML AJAX-CORS (5) AJAX-CORS (6) http://people.at/students JSON AJAX-CORS, JSONP (7) AJAX-CORS (8) http://people.at/{dob}/contact VCARD AJAX-CORS (9) AJAX-CORS (10) (1) (2): Follow the Same Origin Policy (3) (4): JSONP only works on GET method (5) (6) (9) (10): Using CORS to deal with problem of different domain (7) (8): JSONP only works on GET method AJAX-CORS (add http header in server) 1 2 3 4 res.writeHead(200, { \u0026#39;Content-Type\u0026#39;: \u0026#39;Application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }); AJAX-CORS (Using CORS Anywhere which adds CORS headers to the proxied request) Code (index.html) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt; HW2 - AJAX and XHR states \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;Download File\u0026#34; onclick=\u0026#34;DownloadFile()\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; value=\u0026#34;Ready to Download\u0026#34; id=\u0026#34;state\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;progressCounter\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://code.jquery.com/jquery-3.6.0.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var progressElem = $(\u0026#39;#progressCounter\u0026#39;); function DownloadFile() { var url = \u0026#34;https://mytest1.s3.amazonaws.com/file.bz2\u0026#34;; progressElem.text(url); $.ajax({ url: url, cache: false, xhr: function() { var xhr = new XMLHttpRequest(); xhr.onreadystatechange = function() { if ( xhr.readyState == 1 ) { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;loading\u0026#39;); console.log(\u0026#34;Loading - when the open method was called\u0026#34;); } // if else if (xhr.readyState == 2) { if ( xhr.status == 200 ) { xhr.responseType = \u0026#34;blob\u0026#34;; } // if document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;loaded\u0026#39;); console.log(\u0026#34;Loaded - when the send method was called\u0026#34;); } // else if else if ( xhr.readyState == 3 ) { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;downloading\u0026#39;); console.log(\u0026#34;downloading - while the data is being downloaded\u0026#34;); } // else if else { document.getElementById(\u0026#39;state\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, \u0026#39;finished downloading\u0026#39;); console.log(\u0026#34;finished downloading - when the data has beeen downloade\u0026#34;); } // else }; // xhr.onready xhr.addEventListener(\u0026#34;progress\u0026#34;, function (evt) { console.log(evt.lengthComputable); if (evt.lengthComputable) { var percentComplete = evt.loaded / evt.total; progressElem.html(Math.round(percentComplete * 100) + \u0026#34;%\u0026#34;); } }, false); return xhr; }, success: function (data) { console.log(\u0026#34;nono\u0026#34;); // Convert the Byte Data to BLOB object. var blob = new Blob([data], { type: \u0026#34;application/octetstream\u0026#34; }); var link = window.URL.createObjectURL(blob); var a = $(\u0026#34;\u0026lt;a /\u0026gt;\u0026#34;); a.attr(\u0026#34;download\u0026#34;, \u0026#34;file.bz2\u0026#34;); // attributes,value a.attr(\u0026#34;href\u0026#34;, link); console.log(link); $(\u0026#34;body\u0026#34;).append(a); a[0].click(); $(\u0026#34;body\u0026#34;).remove(a); } }); }; // DownloadFile() \u0026lt;/script\u0026gt; \u0026lt;br\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Node.js server in Docker Tasks Implement a simple HTTP server in Node.js implement a \u0026ldquo;hello NAME\u0026rdquo; service endpoint request: \u0026ldquo;http://localhost:8080/John\u0026rdquo;, response \u0026ldquo;Hello John\u0026rdquo; Define an image Dockerfile, with the following specifications: build on a chosen node.js image the container should listen on port 8888 load the server implementation from a local directory run the server Create a docker image from the Dockerfile Create a container for the image tip: use the -p parameter to map public port to a private port inside the container (e.g. -p 8080:8888) Check if the container is running (docker container ls) Send several requests to the server running within the container you can use ping, curl or telnet for testing Stop the container Description Docker Overview:\npackage an application and its dependencies in a virtual container build, commit and share images based and primarily for Linux allows operating-system-level virtualization run isolated packages, also known, as containers Define an image Dockerfile and create a docker image 1 2 3 4 FROM --platform=linux/amd64 node:10-alpine COPY . /app WORKDIR /app CMD node /app/server.js 1 docker build -t hw3-docker . Create a container and run the curl. -p (Port mapping for the container and the host machine) 1 docker run -p 8080:8888 hw3-docker Docker advanced (Redis) Tasks Task 1: Start a docker container for a redis server: build on a chosen redis image redis run the server Task 2: Start a docker container for a redis client: build on a chosen redis image redis with the client insert some user info where key is the person name, and value is the address Task 3: Implement a simple HTTP server in Node.js implement a \u0026ldquo;http://localhost:8080/person/{person_name}/address\u0026rdquo; API, which returns the address of a person request: GET \u0026ldquo;http://localhost:8080/person/John/address\u0026rdquo;, response \u0026ldquo;Thakurova 9, 160 00, Prague\u0026rdquo; the server should fetch the data from a Redis server. Redis runs in a separate container than the node.js server! (see above). Task 4: Define an image Dockerfile (for the node.js server), with the following specifications: build on a chosen node.js image load the server implementation from a local directory run the server Create a docker image from the Dockerfile Create and run a container Test the server - it shoudl return the address for a person retrieved from the linked redis server container Description Task1 - Run the Redis Server in a container Redis-server stores the data from Redis-client, and can handle the request from the Node.js\n1 sudo docker run -p 6379:6379 --name redis-server -d redis Task 2 - Run the Redis client in a container Redis-client is linked to the Redis-server\n1 sudo docker run -it --link redis-server:redis --name redis-client -d redis Redis-client let user insert the data {kay,value} Task3 - Implement a simple HTTP server and run it on the host, and Node.js fetches the data from the Redis-server\nFailture\r2022/04/03 - I faced the problem in the recent version of Redis, it seems that the problems are MAC docker or Redis version. https://stackoverflow.com/questions/71529419/redis-overiding-remote-host-in-nodejs I implemented the methods of docker-compose or link-container, but still encountered the connection refused problem.\rSuccess\rMust use redis version 3.0.2!!!!!!!!!\n1 npm install redis@3.0.2 Task4 - Docker-compose file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # version of docker-compose version: \u0026#39;3.9\u0026#39; # \u0026#39;services\u0026#39; are equivalent to \u0026#39;containers\u0026#39; services: redis: image: \u0026#39;redis\u0026#39; ports: - \u0026#39;6379:6379\u0026#39; tnode11: image: \u0026#39;hw4-docker\u0026#39; # Specify an array of ports to map ports: - \u0026#39;8080:8888\u0026#39; Code (Redis server) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 const http = require(\u0026#39;http\u0026#39;); const redis = require(\u0026#39;redis\u0026#39;) var url = require(\u0026#34;url\u0026#34;); const server = http.createServer((req,res) =\u0026gt; { res.setHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/plain\u0026#39;); get_api_string = req.url.split(\u0026#39;/\u0026#39;); const client = redis.createClient({ host: \u0026#39;redis\u0026#39;, port: 6379 }) // get request from if ( get_api_string[1] == \u0026#34;person\u0026#34; \u0026amp;\u0026amp; get_api_string[3] == \u0026#34;address\u0026#34; ) { client.get(get_api_string[2], function (error, result) { console.log(\u0026#39;GET result -\u0026gt;\u0026#39; + result); if ( result == null ) res.write(\u0026#34;You must input the data to the redis-server from the redis client\u0026#34;); else res.write(\u0026#34;Hello \u0026#34; + result ); res.end(); }); } else { var echo_name = req.url.substring(1); res.write(\u0026#34;Hello \u0026#34; + echo_name); res.end(); } }); server.listen(8888, \u0026#39;0.0.0.0\u0026#39;, () =\u0026gt; { console.log(\u0026#34;listening for 8080\u0026#34;); }); HTTP/2 Push mechanism Tasks Implement an HTTP/2 push mechanism Implement a simple HTML page consisting of at least 3 additional resources (e.g. css, js, image, ..) Implement a simple http server serving the page via HTTP/2 When the html page is requested, push all those files together with the requested page Use http2 module Description HTTP/2 with the push method HTTP/2 without the push method The above results indicate that HTTP/2 using the push method is faster than the method without the push method. The key reason is the concurrency in the different types of files in one connection (Extend this concept to the QUIC protocol, fixing the HoL blocking problem).\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const sendFile = (stream, fileName) =\u0026gt; { const fd = fs.openSync(fileName, \u0026#34;r\u0026#34;); const stat = fs.fstatSync(fd); const headers = { \u0026#34;content-length\u0026#34;: stat.size, \u0026#34;last-modified\u0026#34;: stat.mtime.toUTCString(), \u0026#34;content-type\u0026#34;: mime.getType(fileName) }; stream.respondWithFD(fd, headers); // key point }; const pushFile = (stream, path, fileName) =\u0026gt; { stream.pushStream({ \u0026#34;:path\u0026#34;: path }, (err, pushStream) =\u0026gt; { if (err) { throw err; } sendFile(pushStream, fileName); }); }; Server-Sent Events Task Design and implement simple SSE server in Node.js and simple client in HTML/JavaScript.\nThe client connects to server and displays all incoming messages. The server sends every 2 seconds one message - e.g. one line of text file. Description Server-sent events are functions extended on top of HTTP. The server can actively send data to the client based on the new MIME type( text/event-stream ). Client: The URL is passed to EventSource() and creates a new EventSource object. onmessage() is triggered by receiving the data.\nServer: new MIME type(text/event-stream).\nCode 1 2 3 4 const message = `retry: ${refreshRate}\\nid:${id}\\ndata: ${data}\\n\\n`; res.write(message); }, refreshRate); retry: Specify the time interval for the browser to re-initiate the connection\nid: ID for each event-stream\ndata: the event data\n\u0026lsquo;\\n\u0026rsquo; gaps the indicator. \u0026lsquo;\\n\\n\u0026rsquo; switches the lines\nOAuth-Browser-Based-App Task Design and implement a simple OAuth - Browser-Based App. Browser-based apps run entirely in the browser after loading the source code from a web page.\nUse a simple server (https) for serving static content (html, js, …​). Configure any OAuth application of your choice. You can use any OAuth solution as authorization and resource server: Google, GitHub, …​ The app in browser connects to the authorization server and allows access to the resources. The app collects an presents in the browser any resource from the resource server using the provided code/token (e.g. list of contacts, files, messages, repositories, …​) Do not use any OAuth library (e.g. GoogleAuth, …​) Description Workflow:\nLog in to the Google cloud console to register a new project. Get the client_id and client_secret and set the redirect_link\nClient calls getGoogleAuthURL() to get the code from https://accounts.google.com/o/oauth2/v2/auth\nScope type: userinfo is the public data. If we want to leverage the common services, just like Google drive or photo\u0026hellip;, we need to add the test users until publishing the project.\nThe server posts the code with Axios to get the token from the https://oauth2.googleapis.com/token\nCall jwt.decode(token) to get the data\nCreate a new project in the Google cloud console\rEnter index.html to log in through ouath2 After logging, Authorization: OK. Print the callback data\rCode (OAuth server implement) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 const HTTP2_PORT = 3000; const http2 = require(\u0026#39;http2\u0026#39;); const mime = require(\u0026#39;mime\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const url = require(\u0026#34;url\u0026#34;); const qs = require(\u0026#39;querystring\u0026#39;); const axios = require(\u0026#39;axios\u0026#39;); const jwt = require(\u0026#39;jsonwebtoken\u0026#39;); const serverOptions = { key: fs.readFileSync(\u0026#39;localhost-privkey.pem\u0026#39;), cert: fs.readFileSync(\u0026#39;localhost-cert.pem\u0026#39;) }; // fill your info var ClientID = \u0026#34;\u0026#34;; var ClientSercet = \u0026#34;\u0026#34;; var RedirectLink = \u0026#34;\u0026#34;; async function getTokenFromGoogle(get_token_url, value) { try { var res = await axios.post(get_token_url, qs.stringify(value), { headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/x-www-form-urlencoded\u0026#39;, }, }); } catch(err) { console.log(err); } return res.data; } // read and send file content in the stream const sendFile = (stream, fileName) =\u0026gt; { const fd = fs.openSync(fileName, \u0026#34;r\u0026#34;); const stat = fs.fstatSync(fd); const headers = { \u0026#34;content-length\u0026#34;: stat.size, \u0026#34;last-modified\u0026#34;: stat.mtime.toUTCString(), \u0026#34;content-type\u0026#34;: mime.getType(fileName) }; stream.respondWithFD(fd, headers); // key point }; // handle requests const http2Handlers = (req, res) =\u0026gt; { // send empty response for favicon.ico if (req.url === \u0026#34;/favicon.ico\u0026#34;) { res.stream.respond({ \u0026#34;:status\u0026#34;: 200 }); res.stream.end(); return; } if (req.url === \u0026#34;/\u0026#34; || req.url === \u0026#34;/index.html\u0026#34; ) { req.url = \u0026#34;/index.html\u0026#34;; const fileName = __dirname + req.url; sendFile(res.stream, fileName); } // if else if (req.url.includes( \u0026#34;/api/oauth/google\u0026#34; ) ) { var parsed = url.parse(req.url); var query = qs.parse(parsed.query); var code = query.code; // get the toake with code var get_token_url = \u0026#34;https://oauth2.googleapis.com/token\u0026#34;; const value = { code, client_id: ClientID, client_secret: ClientSercet, redirect_uri: RedirectLink, grant_type: \u0026#34;authorization_code\u0026#34;, }; getTokenFromGoogle(get_token_url, value).then(function(result) { var googleUser = jwt.decode(result.id_token); res.write( \u0026#39;\u0026lt;h1\u0026gt; Hello \u0026#39; + googleUser.name + \u0026#39;\u0026lt;/h1\u0026gt;\u0026#39;); res.write(\u0026#39;Your email address: \u0026#39; + googleUser.email + \u0026#39;\u0026lt;br\u0026gt;\u0026#39;); res.write(\u0026#39;Locale: \u0026#39; + googleUser.locale + \u0026#39;\u0026lt;br\u0026gt;\u0026#39;); res.end(\u0026#39;\u0026lt;img src=\u0026#39; + googleUser.picture + \u0026#39; referrerpolicy=\u0026#34;no-referrer\u0026#34;/\u0026gt;\u0026#39;) console.log(googleUser); }); } }; http2 .createSecureServer(serverOptions, http2Handlers) .listen(HTTP2_PORT, () =\u0026gt; { console.log(\u0026#34;http2 server started on port\u0026#34;, HTTP2_PORT); }); ","permalink":"https://jonathan-tw.github.io/posts/middleware-architectures-2-hw/","summary":"AJAX / XHR states / CORS / Data access Tasks Create a simple HTML page with an info text field and a single button Implement a JavaScript function which is triggered when the button is clicked The function should fetch relatively large file (e.g. 100-200MB) in the text field show following states: loading - when the open method was called loaded - when the send method was called downloading - while the data is being downloaded finished downloading - when the data has beeen downloaded you can use Promise, async/await Description AJAX overview: Asynchronous JavaScript and XML technique for creating better, faster, and more interactive web applications relies on XML, JSON, HTML, CSS and JavaScript AJAX is not a programming language Running this demo by using the jquery module to achieve the ajax request. Following the xhr state in https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/readyState.","title":"(課程筆記)(CTU in Prague) Middleware architectures 2 HW"},{"content":" OS:Ubuntu: 16.04 / 18.04\nOpencCV: 4.2\nCuda: 8.0 / 9.0\nNivida Driver: 418.56 (GTX 1050ti) / 435 (GTX 1060)\nCPU: i3-8300 3.70Ghz\nffmpeg: 4.2 / 4.2.4\nnv_codec_headers: 8.2 / 9.0\nInstall Nvidia Driver list available Nvidia Driver\n1 ubuntu-drivers devices add repoitory\n1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update instll Nvidia Driver\n1 sudo apt install nvidia-xxx Install CUDA Check CUDA and Nvidia driver compatibility https://docs.nvidia.com/deploy/cuda-compatibility/index.html\nDownload Cuda runfile which determines by your os https://developer.nvidia.com/cuda-downloads\nplease choose not to install nvidia graphics driver, or the driver will be updated**\ninstall cuda (ex: cuda 8.0 for ubuntnu 16.04 )\n1 sudo sh cuda_8.0.61_375.26_linux.run Add environment variables to ~/.bashrc\n1 2 3 4 sudo nano ~/.bashrc export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc Install NV_codec (optional for gpu ) download nv_codec_headers 8.2: https://github.com/FFmpeg/nv-codec-headers/tree/sdk/8.2\ninstall nv_codec_headers 8.2\n1 sudo make install nv_codec_headers 9.0需要cuda 9.0以上\nInstall FFmpeg 4.2.4 install necessary tool\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 sudo apt-get update -qq \u0026amp;\u0026amp; sudo apt-get -y install \\ autoconf \\ automake \\ build-essential \\ cmake \\ git-core \\ libass-dev \\ libfreetype6-dev \\ libgnutls28-dev \\ libsdl2-dev \\ libtool \\ libva-dev \\ libvdpau-dev \\ libvorbis-dev \\ libxcb1-dev \\ libxcb-shm0-dev \\ libxcb-xfixes0-dev \\ pkg-config \\ texinfo \\ wget \\ yasm \\ zlib1g-dev install libx264:\n1 2 3 4 git clone https://code.videolan.org/videolan/x264.git cd x264 sudo ./configure --enable-shared --disable-asm sudo make \u0026amp;\u0026amp; sudo make install installl ffplay\n1 sudo apt-get install libsdl2-dev install libfdk_aac (optional)\nhttps://sourceforge.net/projects/opencore-amr/files/fdk-aac/\n1 2 ./configure make \u0026amp;\u0026amp; sudo make install 1 2 3 vi /etc/ld/so.conf.d/aac.conf /usr/local/lib sudo ldconfig install mp3lame (optional)\n1 sudo apt-get install yasm libmp3lame-dev download ffmpeg:\n1 git clone https://git.ffmpeg.org/ffmpeg.git configure command (if you need cuvid)\n1 sudo ./configure --enable-libmp3lame --enable-libfdk_aac --enable-libx264 --enable-gpl --enable-cuda --enable-cuvid --enable-nvenc --enable-nonfree --enable-pic --enable-rpath --enable-shared --extra-cflags=-I/usr/local/cuda/include --extra-ldflags=-L/usr/local/cuda/lib64 --enable-libnpp --enable-ffplay configure command (if you don\u0026rsquo;t need cuvid and sound but want cuda )\n1 sudo ./configure --enable-libx264 --enable-gpl --enable-cuda --enable-nvenc --enable-nonfree --enable-rpath --enable-pic --enable-shared --extra-cflags=-I/usr/local/cuda/include --extra-ldflags=-L/usr/local/cuda/lib64 --enable-ffplay compile ffmpeg\n1 sudo make \u0026amp;\u0026amp; sudo make install add to envirnment\n1 2 3 4 5 6 7 sudo vi ~/.bashrc export PATH=/usr/local/bin:$PATH source ~/.bashrc sudo nano /etc/ld.so.conf # dertermine on your ffmpeg install location /usr/local/ffmpeg/lib sudo ldconfig for more build options https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu\nTest ffmpeg Hardware acceleration methods:\n1 ffmpeg -hwaccels cpu:\n1 time ffmpeg -rtsp_transport tcp -i rtsp://admin:ai123456@192.168.0.111 -r 30 -t 100 -c:v h264 -b:v 2048k -vf scale=1280:-1 -y -c:v libx264 tcp_cpu_output.mp4 gpu:\n1 time ffmpeg -rtsp_transport tcp -hwaccel cuvid -c:v h264_cuvid -i rtsp://admin:ai123456@192.168.0.111 -r 30 -t 100 -b:v 2048k -vf scale_npp=1280:-1 -y -c:v h264_nvenc tcp_gpu_output.mp4 默認下ffmpeg抓rtsp使用UDP,這會lose大量packet，使用TCP避免。\nVideo.mp4 CPU GPU Real Time 16s 0.5s Bit rate 1497kbps 1479kbps rtsp://admin:ai123456@192.168.0.111 (30幀) CPU GPU Real Time 1m49s 1m41s Bit rate 1911kbps 2067kbps Install Opencv install required and optional package\n1 sudo add-apt-repository -y \u0026#34;deb http://security.ubuntu.com/ubuntu xenial-security main\u0026#34; 1 sudo apt-get install -y libjpeg8-dev libjasper-dev libpng12-dev 1 sudo apt install build-essential cmake git pkg-config libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev 1 sudo apt install libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394-22-dev download opencv 4.2 and extra library\n1 2 3 4 5 6 7 mkdir opencv_base cd opencv_base git clone https://github.com/opencv/opencv.git --branch=4.2.0 git clone https://github.com/opencv/opencv_contrib.git --branch=4.2.0\tcd opencv_base/opencv/ mkdir build cd build with cuda and opencv_world check your cuda_arch_bin:https://developer.nvidia.com/cuda-gpus\n1 2 3 4 5 6 7 8 9 10 11 sudo cmake -D CMAKE_BUILD_TYPE=Release \\ -D OPENCV_GENERATE_PKGCONFIG=YES \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_JAVA=YES \\ -D WITH_CUDA=ON \\ -D BUILD_opencv_world=ON \\ -D WITH_FFMPEG=ON \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/usr/local/opencv_base/opencv_contrib/modules \\ -D BUILD_opencv_python3=yes \\ -D CUDA_ARCH_BIN=6.1 .. without cuda (Simple) 1 2 3 4 5 6 7 8 9 10 11 sudo cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D OPENCV_GENERATE_PKGCONFIG=YES \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_JAVA=YES \\ -D BUILD_opencv_world=ON \\ -D WITH_FFMPEG=ON \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/usr/local/opencv_base/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON .. Compile cuda \u0026amp;\u0026amp; ffmpeg with Opencv 開發指令 include及lib都為相對路徑 (if you want to pack code )\nTwo .so files need to be at system location libx264.so\n1 sudo cp lib/libx264.so.160 /lib/x86_64-linux-gnu/ libswresample.so\n1 sudo cp lib/libswresample.so.3 /usr/local/lib/ CUDA compiles to shared file (.so): (NV12toBGR) 1 nvcc -shared cuda_convert.cu -o ../lib/libcuda_convert.so --compiler-options \u0026#39;-fPIC\u0026#39; FFmpeg gpu server compiles to shared file (.so) 1 g++ -shared -fPIC ffmpeg_gpu_server.cpp -o ../lib/libffmpeg_gpu_server.so -I../include/ -L ../lib -Wl,--rpath=\u0026#39;../lib\u0026#39; -lavformat -lavcodec -lavutil -lswscale -lswresample -lcuda_convert -lx264 FFmpeg gpu client compiles 1 g++ ffmpeg_gpu_client.cpp -o ../bin/ffmpeg_gpu_client -I../include/ -Wl,--rpath=\u0026#39;../lib\u0026#39; -L -lffmpeg_gpu_server -lopencv_world -ldl -lm -lz -lX11 -std=gnu++11 ","permalink":"https://jonathan-tw.github.io/posts/ffmpeg_note_1/","summary":"OS:Ubuntu: 16.04 / 18.04 OpencCV: 4.2 Cuda: 8.0 / 9.0 Nivida Driver: 418.56 (GTX 1050ti) / 435 (GTX 1060) CPU: i3-8300 3.70Ghz ffmpeg: 4.2 / 4.2.4 nv_codec_headers: 8.2 / 9.0 Install Nvidia Driver list available Nvidia Driver 1 ubuntu-drivers devices add repoitory 1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update instll Nvidia Driver 1 sudo apt install nvidia-xxx Install CUDA Check CUDA and Nvidia driver compatibility https://docs.nvidia.com/deploy/cuda-compatibility/index.html Download Cuda runfile which determines by your os https://developer.nvidia.com/cuda-downloads please choose not to install nvidia graphics driver, or the driver will be updated** install cuda (ex: cuda 8.0 for ubuntnu 16.04 ) 1 sudo sh cuda_8.0.61_375.26_linux.run Add environment variables to ~/.bashrc 1 2 3 4 sudo nano ~/.bashrc export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc Install NV_codec (optional for gpu ) download nv_codec_headers 8.2: https://github.com/FFmpeg/nv-codec-headers/tree/sdk/8.2 install nv_codec_headers 8.2 1 sudo make install nv_codec_headers 9","title":"FFmpeg開發系列1 - FFmpeg、OpenCV、CUDA、NV_Codec GPU加速環境搭建"}]